{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9a: PCA for Face Recognition\n",
    "    \n",
    "Following the demo for this unit, we will explore further the use of PCA for feature dimension reduction for classification. We will use a 2-layer neural net on the PCA coefficients. We will practice optimizing the classificaiton parameters (the number of PCA components and the number of hidden nodes in the NN classifier). We will furthermore compare this approach with using convolutional neural net on raw images.\n",
    "\n",
    "Through the lab, you will learn to:\n",
    "\n",
    "* Perform PCA on the a face dataset to find the PC components\n",
    "* Evaluate the effect of using different nubmer of principle components for data representation and classification.\n",
    "* Optimize the number of PC coefficients and classifier parameters together to maximize classification accuracy.\n",
    "* Understand the impact of training data size on the feature and classification method selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading LFW metadata: https://ndownloader.figshare.com/files/5976012\n",
      "Downloading LFW metadata: https://ndownloader.figshare.com/files/5976009\n",
      "Downloading LFW metadata: https://ndownloader.figshare.com/files/5976006\n",
      "Downloading LFW data (~200MB): https://ndownloader.figshare.com/files/5976015\n"
     ]
    }
   ],
   "source": [
    "# Import the flw_people dataset. \n",
    "# Select only those people with at least 100 instances \n",
    "# Reduce the face image size by 0.4\n",
    "\n",
    "# TO DO\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=100, resize=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size = 50 x 37 = 1850 pixels\n",
      "Number faces = 1140\n",
      "Number classes = 5\n"
     ]
    }
   ],
   "source": [
    "# Save the face images in a datamatrix X and the labels and corresponding names in a datamatrix y and target_names\n",
    "# Furthermore, determine the number of samples and the image size \n",
    "# Determine the number of different faces (number of classes)\n",
    "\n",
    "# TO DO\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "npix = h*w\n",
    "# Data in 2D form\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "# Labels of images\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "print(\"Image size = {0:d} x {1:d} = {2:d} pixels\".format(h,w,npix)) \n",
    "print(\"Number faces = {0:d}\".format(n_samples))\n",
    "print(\"Number classes = {0:d}\".format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0d0bf240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot some sample images to make sure your data load is correct\n",
    "def plt_face(x):\n",
    "    h = 50\n",
    "    w = 37\n",
    "    plt.imshow(x.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "I = np.random.permutation(n_samples) \n",
    "plt.figure(figsize=(10,20))\n",
    "nplt = 4;\n",
    "for i in range(nplt):\n",
    "    ind = I[i]\n",
    "    plt.subplot(1,nplt,i+1)\n",
    "    plt_face(X[ind])\n",
    "    plt.title(target_names[y[ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number faces in training data = 570\n"
     ]
    }
   ],
   "source": [
    "# Split the data into a training set and test set with 50% data for training. \n",
    "# Use \"stratify\" option to make sure the training data and test data have same \n",
    "# proportion of images from different faces\n",
    "# print the number of samples in the training data\n",
    "\n",
    "# TO DO \n",
    "from sklearn.model_selection import train_test_split \n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y,test_size=0.5)\n",
    "\n",
    "n_samples_train = X_train.shape[0]\n",
    "print(\"Number faces in training data = {0:d}\".format(n_samples_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of PCs is 570.\n"
     ]
    }
   ],
   "source": [
    "# Perfom PCA on the training data to derive the principle components (PCs) and the PCA coefficients \n",
    "# You can directly use the PCA class in PCA package or use SVD.\n",
    "# Remember that you need to remove the mean from the data first\n",
    "# Also you should rescale the PCs so that the PCA coefficients all have unit variance\n",
    "# Determine the total number of PCs\n",
    "\n",
    "# TO DO \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "n_samples, _ = X_train.shape\n",
    "Xtr_mean = np.mean(X_train,0)\n",
    "Xtr = X_train - Xtr_mean[None,:]\n",
    "Utr,Str,Vtr = np.linalg.svd(Xtr, full_matrices=False)\n",
    "print(\"The total number of PCs is %d.\" % Vtr.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First let us construct a 2-layer neural net classifier that uses npc= 100 PCA coefficients to classify the faces.  Set up your training and testing data to contain npc PCA coefficients using the previously determined principle components. You should directly use matrix multiplication (i.e. projecting original data to the first 100 principle components you found previously) to find the coefficients rather then using the pca.transform( ) method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "npc = 100\n",
    "eigenface = Vtr[:npc,:]\n",
    "Xtr_pca = Xtr.dot(eigenface.T)\n",
    "Xtr_pca_s = Xtr_pca / Str[None,:npc] * np.sqrt(n_samples) \n",
    "Xts = X_test - Xtr_mean[None,:]\n",
    "Xts_pca = Xts.dot(eigenface.T)\n",
    "Xts_pca_s = Xts_pca / Str[None,:npc] * np.sqrt(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up and compile a NN model with number of hidden nodes nnode=100 and a output layer, and then fit the model to the training data. Use 'relu' for the activation for the hidden layer and use 'softmax' for the output layer. Using `sparse_categorical_crossentropy` for the loss. Use `accuracy` as the metrics. You can choose to do a small number of epochs (=10) with batch size =100.  Determine the accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 10,605\n",
      "Trainable params: 10,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TO DO\n",
    "import keras\n",
    "from keras.models import Model, Sequential \n",
    "from keras.layers import Dense, Activation \n",
    "from keras.layers import Dropout, Flatten \n",
    "from keras.layers import Conv2D, MaxPooling2D \n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "nin = Xtr_pca.shape[1] # dimension of input data \n",
    "nh = 100 # number of hidden units\n",
    "nout = int(np.max(y_train)+1) # number of outputs \n",
    "model = Sequential()\n",
    "model.add(Dense(nh, input_shape=(nin,), activation='relu', name='hidden'))\n",
    "model.add(Dense(nout, activation='softmax', name='output'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to identify the best number of PCs and the best number of hidden nodes in the NN classifer that can achieve the highest validation accuracy. \n",
    "You can set the range of PCs and nubmer of hidden nodes as below.\n",
    "\n",
    "nnodes = [50,100,150,200, 250],\n",
    "npcs = [50,100,150,200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 376us/step - loss: 0.0086 - acc: 0.9982\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 35us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 35us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 38us/step - loss: 8.5248e-04 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 34us/step - loss: 4.0785e-04 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 30us/step - loss: 2.4188e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 30us/step - loss: 1.5982e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 36us/step - loss: 1.1410e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 26us/step - loss: 8.8744e-05 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 34us/step - loss: 7.2105e-05 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Set up an array to store accuracy for different nnode and npcs\n",
    "# TO DO\n",
    "from keras import optimizers\n",
    "opt = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) \n",
    "model.compile(optimizer=opt,\n",
    "                       loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "hist = model.fit(Xtr_pca_s, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "nnodes = [50,100,150,200,250]\n",
    "npcs = [50,100,150,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 363us/step - loss: 1.5764 - acc: 0.4175 - val_loss: 1.4764 - val_acc: 0.4123\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 36us/step - loss: 1.0512 - acc: 0.6263 - val_loss: 1.2468 - val_acc: 0.5421\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 44us/step - loss: 0.7170 - acc: 0.7877 - val_loss: 1.0887 - val_acc: 0.6140\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 45us/step - loss: 0.5308 - acc: 0.8491 - val_loss: 0.9990 - val_acc: 0.6316\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 43us/step - loss: 0.3848 - acc: 0.8877 - val_loss: 0.8993 - val_acc: 0.6842\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 34us/step - loss: 0.3033 - acc: 0.9158 - val_loss: 0.8786 - val_acc: 0.6895\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 39us/step - loss: 0.2402 - acc: 0.9368 - val_loss: 0.8422 - val_acc: 0.7123\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 38us/step - loss: 0.2031 - acc: 0.9474 - val_loss: 0.8453 - val_acc: 0.7088\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 40us/step - loss: 0.1678 - acc: 0.9614 - val_loss: 0.8573 - val_acc: 0.7158\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 36us/step - loss: 0.1440 - acc: 0.9702 - val_loss: 0.8378 - val_acc: 0.7228\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 376us/step - loss: 1.5896 - acc: 0.4053 - val_loss: 1.3334 - val_acc: 0.4737\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 38us/step - loss: 0.8556 - acc: 0.6702 - val_loss: 1.0258 - val_acc: 0.6491\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 53us/step - loss: 0.5079 - acc: 0.8737 - val_loss: 0.8853 - val_acc: 0.6947\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 50us/step - loss: 0.3494 - acc: 0.9035 - val_loss: 0.7995 - val_acc: 0.7421\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 50us/step - loss: 0.2530 - acc: 0.9281 - val_loss: 0.7781 - val_acc: 0.7263\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 43us/step - loss: 0.1989 - acc: 0.9456 - val_loss: 0.7361 - val_acc: 0.7386\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 0.1576 - acc: 0.9632 - val_loss: 0.7362 - val_acc: 0.7509\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 39us/step - loss: 0.1217 - acc: 0.9789 - val_loss: 0.7306 - val_acc: 0.7368\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 43us/step - loss: 0.0939 - acc: 0.9895 - val_loss: 0.7336 - val_acc: 0.7439\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 0.0751 - acc: 0.9912 - val_loss: 0.7363 - val_acc: 0.7386\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 375us/step - loss: 1.6693 - acc: 0.4368 - val_loss: 1.1651 - val_acc: 0.5947\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 41us/step - loss: 0.8089 - acc: 0.7228 - val_loss: 0.9108 - val_acc: 0.7035\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 50us/step - loss: 0.5032 - acc: 0.8456 - val_loss: 0.7556 - val_acc: 0.7561\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 44us/step - loss: 0.3448 - acc: 0.8895 - val_loss: 0.7389 - val_acc: 0.7456\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 49us/step - loss: 0.2658 - acc: 0.9228 - val_loss: 0.6983 - val_acc: 0.7684\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 0.2091 - acc: 0.9421 - val_loss: 0.6845 - val_acc: 0.7614\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 109us/step - loss: 0.1496 - acc: 0.9737 - val_loss: 0.7254 - val_acc: 0.7614\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 90us/step - loss: 0.1149 - acc: 0.9807 - val_loss: 0.7052 - val_acc: 0.7702\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 56us/step - loss: 0.0932 - acc: 0.9895 - val_loss: 0.7424 - val_acc: 0.7632\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 58us/step - loss: 0.0725 - acc: 0.9860 - val_loss: 0.7649 - val_acc: 0.7544\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 497us/step - loss: 1.8334 - acc: 0.3947 - val_loss: 1.0999 - val_acc: 0.6333\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 53us/step - loss: 0.9750 - acc: 0.6632 - val_loss: 0.9434 - val_acc: 0.6930\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 44us/step - loss: 0.5965 - acc: 0.8053 - val_loss: 0.7846 - val_acc: 0.7333\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 64us/step - loss: 0.3893 - acc: 0.8772 - val_loss: 0.6896 - val_acc: 0.7684\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 51us/step - loss: 0.2602 - acc: 0.9246 - val_loss: 0.6953 - val_acc: 0.7632\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 57us/step - loss: 0.2054 - acc: 0.9474 - val_loss: 0.6573 - val_acc: 0.7737\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 0.1513 - acc: 0.9596 - val_loss: 0.6332 - val_acc: 0.7860\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 59us/step - loss: 0.1092 - acc: 0.9825 - val_loss: 0.6524 - val_acc: 0.7719\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 47us/step - loss: 0.0822 - acc: 0.9895 - val_loss: 0.6612 - val_acc: 0.7719\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 57us/step - loss: 0.0610 - acc: 0.9912 - val_loss: 0.6721 - val_acc: 0.7737\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 485us/step - loss: 1.4528 - acc: 0.4772 - val_loss: 0.9997 - val_acc: 0.6982\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 54us/step - loss: 0.7021 - acc: 0.7421 - val_loss: 0.8022 - val_acc: 0.7351\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 46us/step - loss: 0.4844 - acc: 0.8544 - val_loss: 0.6714 - val_acc: 0.7754\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 49us/step - loss: 0.2950 - acc: 0.9035 - val_loss: 0.6453 - val_acc: 0.7789\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 49us/step - loss: 0.2129 - acc: 0.9404 - val_loss: 0.6420 - val_acc: 0.7649\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 45us/step - loss: 0.1460 - acc: 0.9719 - val_loss: 0.6268 - val_acc: 0.7842\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 46us/step - loss: 0.1151 - acc: 0.9807 - val_loss: 0.6277 - val_acc: 0.7912\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 49us/step - loss: 0.0766 - acc: 0.9895 - val_loss: 0.6506 - val_acc: 0.7860\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 54us/step - loss: 0.0563 - acc: 0.9947 - val_loss: 0.6385 - val_acc: 0.7947\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 91us/step - loss: 0.0409 - acc: 0.9965 - val_loss: 0.6533 - val_acc: 0.7930\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 420us/step - loss: 1.6058 - acc: 0.3544 - val_loss: 1.4671 - val_acc: 0.3702\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 62us/step - loss: 1.1019 - acc: 0.6175 - val_loss: 1.2648 - val_acc: 0.4825\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 50us/step - loss: 0.7890 - acc: 0.7246 - val_loss: 1.1189 - val_acc: 0.5649\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 49us/step - loss: 0.4885 - acc: 0.8649 - val_loss: 1.0264 - val_acc: 0.6000\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 51us/step - loss: 0.3013 - acc: 0.9333 - val_loss: 0.9970 - val_acc: 0.6070\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 57us/step - loss: 0.2145 - acc: 0.9474 - val_loss: 0.9312 - val_acc: 0.6526\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 56us/step - loss: 0.1478 - acc: 0.9789 - val_loss: 0.9424 - val_acc: 0.6474\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 47us/step - loss: 0.0956 - acc: 0.9842 - val_loss: 0.9176 - val_acc: 0.6667\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 37us/step - loss: 0.0646 - acc: 0.9947 - val_loss: 0.9431 - val_acc: 0.6702\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570/570 [==============================] - 0s 44us/step - loss: 0.0440 - acc: 0.9947 - val_loss: 0.9487 - val_acc: 0.6877\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 364us/step - loss: 1.5875 - acc: 0.4667 - val_loss: 1.1933 - val_acc: 0.6053\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 38us/step - loss: 0.7907 - acc: 0.7544 - val_loss: 0.9521 - val_acc: 0.7018\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 50us/step - loss: 0.3971 - acc: 0.8982 - val_loss: 0.8306 - val_acc: 0.7281\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 55us/step - loss: 0.2126 - acc: 0.9596 - val_loss: 0.7588 - val_acc: 0.7456\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 75us/step - loss: 0.1167 - acc: 0.9877 - val_loss: 0.7298 - val_acc: 0.7561\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 63us/step - loss: 0.0722 - acc: 0.9912 - val_loss: 0.7262 - val_acc: 0.7719\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 0.0412 - acc: 0.9965 - val_loss: 0.7356 - val_acc: 0.7596\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 79us/step - loss: 0.0254 - acc: 1.0000 - val_loss: 0.7495 - val_acc: 0.7632\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 55us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.7622 - val_acc: 0.7614\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 55us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.7658 - val_acc: 0.7632\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 432us/step - loss: 1.7754 - acc: 0.4246 - val_loss: 1.1200 - val_acc: 0.6211\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 0.8183 - acc: 0.7140 - val_loss: 0.8544 - val_acc: 0.7333\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 56us/step - loss: 0.4437 - acc: 0.9105 - val_loss: 0.7478 - val_acc: 0.7456\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 59us/step - loss: 0.2544 - acc: 0.9316 - val_loss: 0.6776 - val_acc: 0.7702\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 59us/step - loss: 0.1400 - acc: 0.9719 - val_loss: 0.6626 - val_acc: 0.7877\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 62us/step - loss: 0.0783 - acc: 0.9877 - val_loss: 0.6804 - val_acc: 0.7789\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 51us/step - loss: 0.0427 - acc: 0.9965 - val_loss: 0.6768 - val_acc: 0.7842\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 47us/step - loss: 0.0278 - acc: 1.0000 - val_loss: 0.6857 - val_acc: 0.7877\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 47us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.7096 - val_acc: 0.7719\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 51us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.7026 - val_acc: 0.7772\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 398us/step - loss: 1.8298 - acc: 0.4368 - val_loss: 1.0430 - val_acc: 0.6456\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 39us/step - loss: 0.8003 - acc: 0.8053 - val_loss: 0.8020 - val_acc: 0.7404\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 51us/step - loss: 0.4108 - acc: 0.9018 - val_loss: 0.6977 - val_acc: 0.7632\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 102us/step - loss: 0.2311 - acc: 0.9298 - val_loss: 0.6334 - val_acc: 0.7754\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 71us/step - loss: 0.1173 - acc: 0.9772 - val_loss: 0.6751 - val_acc: 0.7807\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 64us/step - loss: 0.0643 - acc: 0.9930 - val_loss: 0.6768 - val_acc: 0.7877\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 54us/step - loss: 0.0347 - acc: 1.0000 - val_loss: 0.6902 - val_acc: 0.7912\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 57us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 0.7209 - val_acc: 0.7930\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 51us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.7472 - val_acc: 0.7895\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 62us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.7588 - val_acc: 0.7860\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 483us/step - loss: 2.2324 - acc: 0.3561 - val_loss: 1.2062 - val_acc: 0.5175\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 46us/step - loss: 0.8449 - acc: 0.6772 - val_loss: 0.8731 - val_acc: 0.6789\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 53us/step - loss: 0.4990 - acc: 0.8421 - val_loss: 0.8505 - val_acc: 0.7000\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 0.2864 - acc: 0.9439 - val_loss: 0.7829 - val_acc: 0.7281\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 52us/step - loss: 0.1511 - acc: 0.9632 - val_loss: 0.8305 - val_acc: 0.7211\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 59us/step - loss: 0.0868 - acc: 0.9860 - val_loss: 0.8247 - val_acc: 0.7298\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 51us/step - loss: 0.0480 - acc: 0.9965 - val_loss: 0.8693 - val_acc: 0.7351\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 62us/step - loss: 0.0246 - acc: 1.0000 - val_loss: 0.9087 - val_acc: 0.7211\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 58us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.9223 - val_acc: 0.7263\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 82us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.9167 - val_acc: 0.7351\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 508us/step - loss: 1.9450 - acc: 0.3421 - val_loss: 1.7318 - val_acc: 0.3316\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 46us/step - loss: 1.1963 - acc: 0.5702 - val_loss: 1.5324 - val_acc: 0.3895\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 51us/step - loss: 0.9007 - acc: 0.7070 - val_loss: 1.3448 - val_acc: 0.4877\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 35us/step - loss: 0.5927 - acc: 0.8158 - val_loss: 1.2444 - val_acc: 0.5368\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 42us/step - loss: 0.3642 - acc: 0.9316 - val_loss: 1.1061 - val_acc: 0.5895\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 40us/step - loss: 0.2084 - acc: 0.9596 - val_loss: 1.0514 - val_acc: 0.6263\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 35us/step - loss: 0.1134 - acc: 0.9895 - val_loss: 0.9989 - val_acc: 0.6614\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 38us/step - loss: 0.0670 - acc: 0.9947 - val_loss: 0.9784 - val_acc: 0.6912\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 37us/step - loss: 0.0398 - acc: 1.0000 - val_loss: 0.9710 - val_acc: 0.6982\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 46us/step - loss: 0.0255 - acc: 1.0000 - val_loss: 0.9677 - val_acc: 0.7035\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 365us/step - loss: 1.9493 - acc: 0.3807 - val_loss: 1.2582 - val_acc: 0.5158\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 39us/step - loss: 0.9423 - acc: 0.6825 - val_loss: 1.0057 - val_acc: 0.6421\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 43us/step - loss: 0.5678 - acc: 0.8421 - val_loss: 0.9069 - val_acc: 0.6807\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 41us/step - loss: 0.2989 - acc: 0.9246 - val_loss: 0.8692 - val_acc: 0.7035\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 38us/step - loss: 0.1483 - acc: 0.9807 - val_loss: 0.8578 - val_acc: 0.7140\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 40us/step - loss: 0.0759 - acc: 0.9930 - val_loss: 0.8456 - val_acc: 0.7246\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 36us/step - loss: 0.0407 - acc: 0.9982 - val_loss: 0.8467 - val_acc: 0.7263\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 45us/step - loss: 0.0248 - acc: 1.0000 - val_loss: 0.8365 - val_acc: 0.7351\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570/570 [==============================] - 0s 44us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.8427 - val_acc: 0.7298\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 45us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.8470 - val_acc: 0.7421\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 387us/step - loss: 2.4614 - acc: 0.3965 - val_loss: 1.4814 - val_acc: 0.4123\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 41us/step - loss: 1.0770 - acc: 0.6895 - val_loss: 1.1959 - val_acc: 0.5649\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 49us/step - loss: 0.6413 - acc: 0.8351 - val_loss: 1.0752 - val_acc: 0.6140\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 49us/step - loss: 0.4009 - acc: 0.9070 - val_loss: 0.9634 - val_acc: 0.6754\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 0.2585 - acc: 0.9193 - val_loss: 0.9708 - val_acc: 0.6737\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 49us/step - loss: 0.1391 - acc: 0.9737 - val_loss: 0.9895 - val_acc: 0.6807\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 0.0753 - acc: 0.9947 - val_loss: 0.9621 - val_acc: 0.7053\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 56us/step - loss: 0.0434 - acc: 0.9947 - val_loss: 0.9823 - val_acc: 0.7158\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 52us/step - loss: 0.0240 - acc: 1.0000 - val_loss: 0.9943 - val_acc: 0.7105\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 49us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.9966 - val_acc: 0.7140\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 387us/step - loss: 2.7022 - acc: 0.3509 - val_loss: 1.3713 - val_acc: 0.4632\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 52us/step - loss: 1.0771 - acc: 0.6158 - val_loss: 1.2398 - val_acc: 0.5421\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 54us/step - loss: 0.6682 - acc: 0.7912 - val_loss: 1.1104 - val_acc: 0.6123\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 49us/step - loss: 0.4051 - acc: 0.8895 - val_loss: 1.0834 - val_acc: 0.6439\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 56us/step - loss: 0.2041 - acc: 0.9667 - val_loss: 1.1229 - val_acc: 0.6614\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 54us/step - loss: 0.1056 - acc: 0.9860 - val_loss: 1.1471 - val_acc: 0.6684\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 55us/step - loss: 0.0492 - acc: 0.9982 - val_loss: 1.1697 - val_acc: 0.6702\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 50us/step - loss: 0.0290 - acc: 0.9982 - val_loss: 1.2178 - val_acc: 0.6667\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 55us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 1.2537 - val_acc: 0.6649\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 57us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 1.2517 - val_acc: 0.6684\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 446us/step - loss: 2.8959 - acc: 0.3018 - val_loss: 1.3449 - val_acc: 0.4719\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 59us/step - loss: 1.0104 - acc: 0.6053 - val_loss: 1.1852 - val_acc: 0.5789\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 56us/step - loss: 0.7345 - acc: 0.7351 - val_loss: 1.2627 - val_acc: 0.5614\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 52us/step - loss: 0.4059 - acc: 0.9333 - val_loss: 1.2439 - val_acc: 0.6000\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 50us/step - loss: 0.1962 - acc: 0.9614 - val_loss: 1.2936 - val_acc: 0.5982\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 54us/step - loss: 0.0926 - acc: 0.9860 - val_loss: 1.3380 - val_acc: 0.6035\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 55us/step - loss: 0.0443 - acc: 0.9930 - val_loss: 1.4190 - val_acc: 0.6018\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 59us/step - loss: 0.0213 - acc: 1.0000 - val_loss: 1.4494 - val_acc: 0.6123\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 63us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 1.5153 - val_acc: 0.6035\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 58us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 1.5307 - val_acc: 0.6158\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 368us/step - loss: 1.8770 - acc: 0.3649 - val_loss: 2.1561 - val_acc: 0.2018\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 35us/step - loss: 1.0651 - acc: 0.5737 - val_loss: 1.9608 - val_acc: 0.3035\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 38us/step - loss: 0.6934 - acc: 0.8070 - val_loss: 1.8269 - val_acc: 0.3596\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 46us/step - loss: 0.4012 - acc: 0.9000 - val_loss: 1.7630 - val_acc: 0.4035\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 47us/step - loss: 0.2100 - acc: 0.9684 - val_loss: 1.6654 - val_acc: 0.4667\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 39us/step - loss: 0.1101 - acc: 0.9895 - val_loss: 1.6493 - val_acc: 0.4912\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 44us/step - loss: 0.0539 - acc: 0.9982 - val_loss: 1.5785 - val_acc: 0.5333\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 45us/step - loss: 0.0286 - acc: 1.0000 - val_loss: 1.5687 - val_acc: 0.5351\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 37us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 1.5664 - val_acc: 0.5439\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 46us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 1.5834 - val_acc: 0.5456\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 638us/step - loss: 2.1762 - acc: 0.3158 - val_loss: 1.3310 - val_acc: 0.4772\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 46us/step - loss: 1.0503 - acc: 0.6070 - val_loss: 1.0908 - val_acc: 0.6105\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 0.6806 - acc: 0.8088 - val_loss: 1.0266 - val_acc: 0.6351\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 50us/step - loss: 0.3719 - acc: 0.9088 - val_loss: 0.9945 - val_acc: 0.6544\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 47us/step - loss: 0.1836 - acc: 0.9719 - val_loss: 0.9782 - val_acc: 0.6754\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 46us/step - loss: 0.0773 - acc: 0.9947 - val_loss: 1.0222 - val_acc: 0.6579\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 43us/step - loss: 0.0377 - acc: 1.0000 - val_loss: 1.0402 - val_acc: 0.6667\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 41us/step - loss: 0.0203 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.6684\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 44us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 1.0993 - val_acc: 0.6684\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 46us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 1.1152 - val_acc: 0.6649\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 432us/step - loss: 1.9143 - acc: 0.3789 - val_loss: 1.1167 - val_acc: 0.5754\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 60us/step - loss: 0.8178 - acc: 0.7175 - val_loss: 0.9301 - val_acc: 0.6807\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 54us/step - loss: 0.4119 - acc: 0.9351 - val_loss: 0.8151 - val_acc: 0.7281\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 54us/step - loss: 0.1854 - acc: 0.9561 - val_loss: 0.8858 - val_acc: 0.7035\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 41us/step - loss: 0.0694 - acc: 0.9930 - val_loss: 0.8320 - val_acc: 0.7421\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 47us/step - loss: 0.0399 - acc: 0.9930 - val_loss: 0.9149 - val_acc: 0.7088\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 47us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.9219 - val_acc: 0.7193\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570/570 [==============================] - 0s 42us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.9255 - val_acc: 0.7175\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 147us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.9596 - val_acc: 0.7105\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 53us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.9670 - val_acc: 0.7211\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 477us/step - loss: 2.5762 - acc: 0.3632 - val_loss: 1.6685 - val_acc: 0.3228\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 46us/step - loss: 1.0014 - acc: 0.6456 - val_loss: 1.5409 - val_acc: 0.3842\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 58us/step - loss: 0.6294 - acc: 0.8123 - val_loss: 1.5791 - val_acc: 0.4386\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 54us/step - loss: 0.3459 - acc: 0.9035 - val_loss: 1.4684 - val_acc: 0.5246\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 56us/step - loss: 0.1493 - acc: 0.9807 - val_loss: 1.4700 - val_acc: 0.5561\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 55us/step - loss: 0.0600 - acc: 0.9965 - val_loss: 1.4481 - val_acc: 0.5825\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 55us/step - loss: 0.0258 - acc: 1.0000 - val_loss: 1.4447 - val_acc: 0.6018\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 53us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 1.4679 - val_acc: 0.6140\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 1.4793 - val_acc: 0.6123\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 50us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 1.4877 - val_acc: 0.6140\n",
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/10\n",
      "570/570 [==============================] - 0s 393us/step - loss: 2.7537 - acc: 0.3596 - val_loss: 1.3580 - val_acc: 0.4596\n",
      "Epoch 2/10\n",
      "570/570 [==============================] - 0s 48us/step - loss: 1.0996 - acc: 0.6246 - val_loss: 1.2328 - val_acc: 0.5386\n",
      "Epoch 3/10\n",
      "570/570 [==============================] - 0s 53us/step - loss: 0.6900 - acc: 0.7719 - val_loss: 1.2491 - val_acc: 0.5526\n",
      "Epoch 4/10\n",
      "570/570 [==============================] - 0s 57us/step - loss: 0.3710 - acc: 0.8930 - val_loss: 1.2873 - val_acc: 0.5684\n",
      "Epoch 5/10\n",
      "570/570 [==============================] - 0s 58us/step - loss: 0.1553 - acc: 0.9737 - val_loss: 1.3395 - val_acc: 0.5877\n",
      "Epoch 6/10\n",
      "570/570 [==============================] - 0s 54us/step - loss: 0.0640 - acc: 0.9912 - val_loss: 1.4080 - val_acc: 0.6053\n",
      "Epoch 7/10\n",
      "570/570 [==============================] - 0s 60us/step - loss: 0.0219 - acc: 1.0000 - val_loss: 1.4398 - val_acc: 0.6140\n",
      "Epoch 8/10\n",
      "570/570 [==============================] - 0s 63us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 1.5059 - val_acc: 0.6140\n",
      "Epoch 9/10\n",
      "570/570 [==============================] - 0s 60us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 1.5633 - val_acc: 0.6000\n",
      "Epoch 10/10\n",
      "570/570 [==============================] - 0s 57us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 1.5749 - val_acc: 0.6070\n"
     ]
    }
   ],
   "source": [
    "# Loop through the combinations to find the accuracy for each combination\n",
    "# For each possible combination of `nnode` and `npc`, set up and fit the model \n",
    "# using features containing only coefficents corresponding to npc coefficients.\n",
    "\n",
    "# TO DO \n",
    "result = np.zeros((len(npcs),len(nnodes)))\n",
    "loss_hist = []\n",
    "train_acc_hist = []\n",
    "val_acc_hist = []\n",
    "for i,npc in enumerate(npcs):\n",
    "    for j,nnode in enumerate(nnodes):\n",
    "        K.clear_session()\n",
    "        eigenface = Vtr[:npc,:]\n",
    "        Xtr_pca = X_train.dot(eigenface.T)\n",
    "        Xtr_pca_s = Xtr_pca / Str[None,:npc] * np.sqrt(n_samples) \n",
    "        Xts = X_test - Xtr_mean[None,:]\n",
    "        Xts_pca = Xts.dot(eigenface.T)\n",
    "        Xts_pca_s = Xts_pca / Str[None,:npc] * np.sqrt(n_samples)\n",
    "        nin = Xtr_pca.shape[1] # dimension of input data\n",
    "        nh = nnode # number of hidden units\n",
    "        nout = int(np.max(y_train)+1)\n",
    "        model = Sequential()\n",
    "        model.add(Dense(nh, input_shape=(nin,), activation='relu', name='hidden')) \n",
    "        model.add(Dense(nout, activation='softmax', name='output'))\n",
    "        opt = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08,decay=0.0) \n",
    "        model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "        hist = model.fit(Xtr_pca_s, y_train, epochs=10, batch_size=100,\n",
    "                         validation_data=(Xts_pca_s, y_test),shuffle=True) \n",
    "        result[i][j] = hist.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best npc is 50, and the best nnode is 250.\n",
      "The best validation accuracy is 0.792982.\n"
     ]
    }
   ],
   "source": [
    "# Determine the npc and nnode that provides the highest validation accuracy \n",
    "# TO DO \n",
    "highest_accuracy = result[0][0] \n",
    "opt_npc_index = 0\n",
    "opt_nnode_index = 0\n",
    "for i in range(0,len(npcs)):\n",
    "    for j in range(0,len(nnodes)):\n",
    "        if result[i][j] > highest_accuracy:\n",
    "            highest_accuracy = result[i][j]\n",
    "            opt_npc_index = i\n",
    "            opt_nnode_index = j\n",
    "print(\"The best npc is %d, and the best nnode is %d.\" % (npcs[opt_npc_index],nnodes[opt_nnode_index]))\n",
    "print(\"The best validation accuracy is %f.\" % highest_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'npcs')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2a63cb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Produce a contour plot of the accuracy using different nnode and npc combincations\n",
    "# TO DO\n",
    "\n",
    "# plt.contourf ...\n",
    "grid_x, grid_y = np.mgrid[50:250:50, 50:300:50] \n",
    "plt.contourf(grid_y,grid_x,result)\n",
    "plt.xlabel(\"nnodes\")\n",
    "plt.ylabel(\"npcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let us compare the PCA+NN with applying a CNN on the raw image data only. \n",
    "\n",
    "Note that you should scale your image data to between 0 and 1. And you should reshape your training and testing data according to image width and height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for input to CNN\n",
    "# TO DO\n",
    "Xtr_cnn = X_train.astype(\"float32\")/255\n",
    "Xts_cnn = X_test.astype(\"float32\")/255\n",
    "Xtr_cnn = np.reshape(Xtr_cnn, (len(Xtr_cnn),h,w,1))\n",
    "Xts_cnn = np.reshape(Xts_cnn, (len(X)-len(Xtr_cnn),h,w,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 46, 33, 16)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 19, 12, 16)        6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 864)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               173000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1005      \n",
      "=================================================================\n",
      "Total params: 180,837\n",
      "Trainable params: 180,837\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Set up a CNN model\n",
    "# You can use 2 conv2D layer, each with kernel size of 5x5, each followed by a pooling layer with strides of 2\n",
    "# For this part, let both conv2D layer generate 16 channels. \n",
    "# The Conv layer should be followed by a flatten layer and two dense layers. \n",
    "# The first dense layer should produce 200 outputs. \n",
    "# The last dense layer is the output layer with n_classes output using 'softmax' activation.\n",
    "# Print model summary to verify it follows the desired structure and compile the model\n",
    "\n",
    "# TO DO \n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (5, 5),\n",
    "                 padding='valid',\n",
    "                 input_shape=Xtr_cnn.shape[1:],\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(16, (5, 5), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation='relu')) \n",
    "model.add(Dense(nout, activation='softmax')) # TO DO\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 570 samples, validate on 570 samples\n",
      "Epoch 1/40\n",
      "570/570 [==============================] - 2s 3ms/step - loss: 1.4842 - acc: 0.3877 - val_loss: 1.3951 - val_acc: 0.4649\n",
      "Epoch 2/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 1.3941 - acc: 0.4649 - val_loss: 1.3807 - val_acc: 0.4649\n",
      "Epoch 3/40\n",
      "570/570 [==============================] - 2s 3ms/step - loss: 1.3714 - acc: 0.4649 - val_loss: 1.3579 - val_acc: 0.4649\n",
      "Epoch 4/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 1.3443 - acc: 0.4649 - val_loss: 1.3325 - val_acc: 0.4649\n",
      "Epoch 5/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 1.3016 - acc: 0.4912 - val_loss: 1.2861 - val_acc: 0.4684\n",
      "Epoch 6/40\n",
      "570/570 [==============================] - 2s 3ms/step - loss: 1.2390 - acc: 0.5000 - val_loss: 1.2291 - val_acc: 0.5404\n",
      "Epoch 7/40\n",
      "570/570 [==============================] - 2s 3ms/step - loss: 1.1645 - acc: 0.5579 - val_loss: 1.1554 - val_acc: 0.5579\n",
      "Epoch 8/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 1.0714 - acc: 0.6123 - val_loss: 1.0941 - val_acc: 0.6105\n",
      "Epoch 9/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 1.0132 - acc: 0.6404 - val_loss: 1.0870 - val_acc: 0.6246\n",
      "Epoch 10/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 0.9418 - acc: 0.6561 - val_loss: 0.9847 - val_acc: 0.6491\n",
      "Epoch 11/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.8758 - acc: 0.6842 - val_loss: 0.9528 - val_acc: 0.6509\n",
      "Epoch 12/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.8227 - acc: 0.7158 - val_loss: 0.9266 - val_acc: 0.6526\n",
      "Epoch 13/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.7655 - acc: 0.7386 - val_loss: 0.8758 - val_acc: 0.6737\n",
      "Epoch 14/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 0.6990 - acc: 0.7632 - val_loss: 0.8561 - val_acc: 0.6912\n",
      "Epoch 15/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.6613 - acc: 0.7860 - val_loss: 0.8041 - val_acc: 0.7140\n",
      "Epoch 16/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.6094 - acc: 0.8105 - val_loss: 0.8319 - val_acc: 0.7105\n",
      "Epoch 17/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.5817 - acc: 0.8263 - val_loss: 0.7599 - val_acc: 0.7228\n",
      "Epoch 18/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.5340 - acc: 0.8456 - val_loss: 0.7169 - val_acc: 0.7614\n",
      "Epoch 19/40\n",
      "570/570 [==============================] - 2s 3ms/step - loss: 0.4920 - acc: 0.8544 - val_loss: 0.6886 - val_acc: 0.7684\n",
      "Epoch 20/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.4560 - acc: 0.8632 - val_loss: 0.6635 - val_acc: 0.7825\n",
      "Epoch 21/40\n",
      "570/570 [==============================] - 2s 3ms/step - loss: 0.4495 - acc: 0.8579 - val_loss: 0.7289 - val_acc: 0.7456\n",
      "Epoch 22/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 0.4474 - acc: 0.8491 - val_loss: 0.7316 - val_acc: 0.7456\n",
      "Epoch 23/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 0.4532 - acc: 0.8544 - val_loss: 0.6669 - val_acc: 0.7614\n",
      "Epoch 24/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.4010 - acc: 0.8807 - val_loss: 0.6257 - val_acc: 0.7930\n",
      "Epoch 25/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.3669 - acc: 0.8965 - val_loss: 0.6173 - val_acc: 0.8000\n",
      "Epoch 26/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.3661 - acc: 0.8825 - val_loss: 0.6199 - val_acc: 0.8018\n",
      "Epoch 27/40\n",
      "570/570 [==============================] - 2s 3ms/step - loss: 0.3955 - acc: 0.8719 - val_loss: 0.7103 - val_acc: 0.7667\n",
      "Epoch 28/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 0.3784 - acc: 0.8754 - val_loss: 0.5923 - val_acc: 0.8211\n",
      "Epoch 29/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 0.3359 - acc: 0.8965 - val_loss: 0.6325 - val_acc: 0.8070\n",
      "Epoch 30/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.3253 - acc: 0.9105 - val_loss: 0.6034 - val_acc: 0.8158\n",
      "Epoch 31/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.3294 - acc: 0.9000 - val_loss: 0.7601 - val_acc: 0.7386\n",
      "Epoch 32/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.3716 - acc: 0.8772 - val_loss: 0.7715 - val_acc: 0.7421\n",
      "Epoch 33/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 0.3400 - acc: 0.8912 - val_loss: 0.6199 - val_acc: 0.8140\n",
      "Epoch 34/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.3221 - acc: 0.9035 - val_loss: 0.5758 - val_acc: 0.8211\n",
      "Epoch 35/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.3049 - acc: 0.9018 - val_loss: 0.6172 - val_acc: 0.8211\n",
      "Epoch 36/40\n",
      "570/570 [==============================] - 1s 3ms/step - loss: 0.2915 - acc: 0.9158 - val_loss: 0.5618 - val_acc: 0.8228\n",
      "Epoch 37/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.2561 - acc: 0.9316 - val_loss: 0.5798 - val_acc: 0.8333\n",
      "Epoch 38/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.2444 - acc: 0.9368 - val_loss: 0.6359 - val_acc: 0.8088\n",
      "Epoch 39/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.2327 - acc: 0.9421 - val_loss: 0.6145 - val_acc: 0.8088\n",
      "Epoch 40/40\n",
      "570/570 [==============================] - 1s 2ms/step - loss: 0.2227 - acc: 0.9404 - val_loss: 0.5723 - val_acc: 0.8351\n",
      "The accuracy on validation set is:\n",
      "[0.46491228070175439, 0.46491228070175439, 0.46491228070175439, 0.46491228070175439, 0.46842104928535327, 0.54035088315344693, 0.55789474855389509, 0.61052630972443966, 0.62456141124691877, 0.64912281015463047, 0.65087720176629849, 0.65263157769253377, 0.67368421951929724, 0.69122807393994246, 0.71403510633267853, 0.71052631474377814, 0.72280701419763393, 0.76140351044504262, 0.76842104761224039, 0.78245614286054643, 0.74561403613341481, 0.74561402985924163, 0.76140350521656508, 0.79298245697690728, 0.80000000460106024, 0.80175438366438212, 0.76666666959461416, 0.82105262969669546, 0.80701753863117154, 0.81578947368421051, 0.73859648746356632, 0.74210526441272939, 0.81403508207254238, 0.82105261610265368, 0.82105263178808652, 0.82280699830306203, 0.83333332705916019, 0.80877193233423061, 0.80877192292297095, 0.835087704031091]\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using batch size=100, epochs = 40\n",
    "# Print the accuracy on the validation set\n",
    "\n",
    "# TO DO \n",
    "opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) \n",
    "# Let's train the model using Adam\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                       optimizer=opt,metrics=['accuracy'])\n",
    "hist_basic = model.fit(Xtr_cnn, y_train,batch_size=100,epochs=40,\n",
    "                       validation_data=(Xts_cnn, y_test),shuffle=True) \n",
    "print(\"The accuracy on validation set is:\")\n",
    "print(hist_basic.history['val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How do the result compared with the PCA+NN method? (If you did right, they should be similar, with PCA+NN being slightly better. If you used more training data (e.g. 75%) and you trained the CNN with more epochs, CNN method may get better). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A: PCA+NN method is slightly better with validation accuracy around 0.83, CNN method is with validation accuracy around 0.79.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat the above using a small dataset\n",
    "\n",
    "Instead of using 50% of the total data for training, let us assume you have only 10% of the total data for training. Repeat both the PCA+NN and the CNN method, to see which one gives you better results. \n",
    "\n",
    "Note that with only 10% data for training, the range of the npc has to be set to be below the total number of training samples. \n",
    "\n",
    "For the CNN model, because you have small number of training samples, you cannot train a network with a large number of parameters reliably. Instead of producing 16 channels for each of the two conv2D layers, configure the model to produce only 8 channels each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.6904 - acc: 0.2982 - val_loss: 1.4509 - val_acc: 0.4172\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 198us/step - loss: 1.2262 - acc: 0.5088 - val_loss: 1.3349 - val_acc: 0.4951\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 205us/step - loss: 0.9618 - acc: 0.7018 - val_loss: 1.2358 - val_acc: 0.5585\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 151us/step - loss: 0.7583 - acc: 0.7982 - val_loss: 1.1481 - val_acc: 0.5926\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 205us/step - loss: 0.6010 - acc: 0.8684 - val_loss: 1.0799 - val_acc: 0.6140\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 179us/step - loss: 0.4785 - acc: 0.9211 - val_loss: 1.0265 - val_acc: 0.6287\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 158us/step - loss: 0.3843 - acc: 0.9298 - val_loss: 0.9814 - val_acc: 0.6472\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 180us/step - loss: 0.3067 - acc: 0.9386 - val_loss: 0.9418 - val_acc: 0.6628\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 173us/step - loss: 0.2408 - acc: 0.9737 - val_loss: 0.9070 - val_acc: 0.6784\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 156us/step - loss: 0.1897 - acc: 0.9737 - val_loss: 0.8809 - val_acc: 0.6969\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.6233 - acc: 0.2719 - val_loss: 1.2955 - val_acc: 0.5244\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 158us/step - loss: 0.9601 - acc: 0.7018 - val_loss: 1.1209 - val_acc: 0.6062\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 177us/step - loss: 0.6531 - acc: 0.8070 - val_loss: 1.0227 - val_acc: 0.6452\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 195us/step - loss: 0.4570 - acc: 0.9035 - val_loss: 0.9579 - val_acc: 0.6637\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 156us/step - loss: 0.3270 - acc: 0.9561 - val_loss: 0.9155 - val_acc: 0.6715\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 163us/step - loss: 0.2350 - acc: 0.9649 - val_loss: 0.8814 - val_acc: 0.6852\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 181us/step - loss: 0.1646 - acc: 0.9912 - val_loss: 0.8553 - val_acc: 0.6930\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 154us/step - loss: 0.1132 - acc: 1.0000 - val_loss: 0.8368 - val_acc: 0.7057\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 185us/step - loss: 0.0778 - acc: 1.0000 - val_loss: 0.8281 - val_acc: 0.7154\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 182us/step - loss: 0.0537 - acc: 1.0000 - val_loss: 0.8269 - val_acc: 0.7164\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.7323 - acc: 0.2368 - val_loss: 1.2536 - val_acc: 0.5292\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 161us/step - loss: 0.9330 - acc: 0.6579 - val_loss: 1.0939 - val_acc: 0.6072\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 205us/step - loss: 0.5822 - acc: 0.8509 - val_loss: 1.0068 - val_acc: 0.6491\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 191us/step - loss: 0.3773 - acc: 0.9474 - val_loss: 0.9522 - val_acc: 0.6686\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 190us/step - loss: 0.2443 - acc: 0.9912 - val_loss: 0.9220 - val_acc: 0.6725\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 387us/step - loss: 0.1591 - acc: 0.9912 - val_loss: 0.9080 - val_acc: 0.6842\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 321us/step - loss: 0.0999 - acc: 1.0000 - val_loss: 0.9073 - val_acc: 0.6862\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 271us/step - loss: 0.0641 - acc: 1.0000 - val_loss: 0.9189 - val_acc: 0.6901\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 251us/step - loss: 0.0425 - acc: 1.0000 - val_loss: 0.9387 - val_acc: 0.6949\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 214us/step - loss: 0.0293 - acc: 1.0000 - val_loss: 0.9608 - val_acc: 0.6940\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.6380 - acc: 0.2632 - val_loss: 1.1900 - val_acc: 0.5604\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 161us/step - loss: 0.7516 - acc: 0.7544 - val_loss: 1.0559 - val_acc: 0.6014\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 173us/step - loss: 0.4446 - acc: 0.8860 - val_loss: 0.9635 - val_acc: 0.6530\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 158us/step - loss: 0.2516 - acc: 0.9561 - val_loss: 0.9214 - val_acc: 0.6657\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 186us/step - loss: 0.1564 - acc: 0.9825 - val_loss: 0.9056 - val_acc: 0.6832\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 170us/step - loss: 0.1081 - acc: 0.9825 - val_loss: 0.9017 - val_acc: 0.6988\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 208us/step - loss: 0.0624 - acc: 0.9825 - val_loss: 0.9077 - val_acc: 0.7047\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 193us/step - loss: 0.0377 - acc: 0.9912 - val_loss: 0.9260 - val_acc: 0.7018\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 294us/step - loss: 0.0252 - acc: 1.0000 - val_loss: 0.9514 - val_acc: 0.7027\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 306us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.9800 - val_acc: 0.7076\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.6684 - acc: 0.2018 - val_loss: 1.1165 - val_acc: 0.5994\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 179us/step - loss: 0.7468 - acc: 0.7281 - val_loss: 0.9484 - val_acc: 0.6579\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 222us/step - loss: 0.4239 - acc: 0.8947 - val_loss: 0.8517 - val_acc: 0.7164\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 186us/step - loss: 0.2361 - acc: 0.9561 - val_loss: 0.8161 - val_acc: 0.7349\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 189us/step - loss: 0.1378 - acc: 0.9737 - val_loss: 0.8251 - val_acc: 0.7251\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 162us/step - loss: 0.0802 - acc: 1.0000 - val_loss: 0.8572 - val_acc: 0.7164\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 186us/step - loss: 0.0545 - acc: 1.0000 - val_loss: 0.8964 - val_acc: 0.7105\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 216us/step - loss: 0.0354 - acc: 1.0000 - val_loss: 0.9315 - val_acc: 0.7115\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 200us/step - loss: 0.0240 - acc: 1.0000 - val_loss: 0.9639 - val_acc: 0.7086\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 202us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.9942 - val_acc: 0.7125\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 2.0253 - acc: 0.2456 - val_loss: 1.6149 - val_acc: 0.3363\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 195us/step - loss: 1.2639 - acc: 0.4737 - val_loss: 1.3992 - val_acc: 0.4561\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 249us/step - loss: 0.8926 - acc: 0.6930 - val_loss: 1.2850 - val_acc: 0.5127\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 183us/step - loss: 0.6819 - acc: 0.7895 - val_loss: 1.2100 - val_acc: 0.5487\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 235us/step - loss: 0.5304 - acc: 0.8596 - val_loss: 1.1472 - val_acc: 0.5887\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 198us/step - loss: 0.4093 - acc: 0.8947 - val_loss: 1.0912 - val_acc: 0.6092\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 248us/step - loss: 0.3062 - acc: 0.9211 - val_loss: 1.0446 - val_acc: 0.6335\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 274us/step - loss: 0.2269 - acc: 0.9649 - val_loss: 1.0078 - val_acc: 0.6462\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 208us/step - loss: 0.1705 - acc: 0.9912 - val_loss: 0.9801 - val_acc: 0.6530\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 0s 157us/step - loss: 0.1264 - acc: 1.0000 - val_loss: 0.9598 - val_acc: 0.6715\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.9153 - acc: 0.1930 - val_loss: 1.3261 - val_acc: 0.5136\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 159us/step - loss: 1.0128 - acc: 0.6491 - val_loss: 1.1451 - val_acc: 0.5712\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 184us/step - loss: 0.6759 - acc: 0.7632 - val_loss: 1.0528 - val_acc: 0.6082\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 188us/step - loss: 0.4772 - acc: 0.8333 - val_loss: 0.9669 - val_acc: 0.6394\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 205us/step - loss: 0.3117 - acc: 0.9561 - val_loss: 0.8927 - val_acc: 0.6920\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 188us/step - loss: 0.1990 - acc: 0.9825 - val_loss: 0.8485 - val_acc: 0.6998\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 207us/step - loss: 0.1256 - acc: 0.9912 - val_loss: 0.8255 - val_acc: 0.7105\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 159us/step - loss: 0.0857 - acc: 0.9912 - val_loss: 0.8162 - val_acc: 0.7154\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 160us/step - loss: 0.0591 - acc: 1.0000 - val_loss: 0.8133 - val_acc: 0.7173\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 210us/step - loss: 0.0422 - acc: 1.0000 - val_loss: 0.8144 - val_acc: 0.7173\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 2.1994 - acc: 0.1053 - val_loss: 1.3580 - val_acc: 0.4873\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 160us/step - loss: 0.9814 - acc: 0.7807 - val_loss: 1.1429 - val_acc: 0.5780\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 210us/step - loss: 0.6070 - acc: 0.7895 - val_loss: 1.0959 - val_acc: 0.5799\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 246us/step - loss: 0.4192 - acc: 0.8333 - val_loss: 1.0050 - val_acc: 0.6296\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 224us/step - loss: 0.2615 - acc: 0.9386 - val_loss: 0.9206 - val_acc: 0.6657\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 384us/step - loss: 0.1548 - acc: 0.9737 - val_loss: 0.8641 - val_acc: 0.6969\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 269us/step - loss: 0.0985 - acc: 0.9912 - val_loss: 0.8358 - val_acc: 0.7105\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 278us/step - loss: 0.0662 - acc: 0.9912 - val_loss: 0.8281 - val_acc: 0.7222\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 271us/step - loss: 0.0482 - acc: 1.0000 - val_loss: 0.8319 - val_acc: 0.7222\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 312us/step - loss: 0.0354 - acc: 1.0000 - val_loss: 0.8424 - val_acc: 0.7193\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.6396 - acc: 0.3070 - val_loss: 1.2194 - val_acc: 0.5351\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 187us/step - loss: 0.7322 - acc: 0.7281 - val_loss: 1.0808 - val_acc: 0.6150\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 199us/step - loss: 0.4317 - acc: 0.8772 - val_loss: 0.9725 - val_acc: 0.6618\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 197us/step - loss: 0.2352 - acc: 0.9737 - val_loss: 0.8935 - val_acc: 0.6881\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 210us/step - loss: 0.1195 - acc: 0.9912 - val_loss: 0.8495 - val_acc: 0.6998\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 197us/step - loss: 0.0631 - acc: 1.0000 - val_loss: 0.8405 - val_acc: 0.7076\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 191us/step - loss: 0.0370 - acc: 1.0000 - val_loss: 0.8519 - val_acc: 0.7300\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 204us/step - loss: 0.0235 - acc: 1.0000 - val_loss: 0.8692 - val_acc: 0.7300\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 228us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.8879 - val_acc: 0.7329\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 231us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.9071 - val_acc: 0.7349\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.6465 - acc: 0.2982 - val_loss: 1.1589 - val_acc: 0.5517\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 268us/step - loss: 0.6836 - acc: 0.6930 - val_loss: 1.0296 - val_acc: 0.6238\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 245us/step - loss: 0.3843 - acc: 0.9035 - val_loss: 0.9563 - val_acc: 0.6618\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 297us/step - loss: 0.2022 - acc: 0.9737 - val_loss: 0.9034 - val_acc: 0.6735\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 253us/step - loss: 0.1072 - acc: 0.9912 - val_loss: 0.8723 - val_acc: 0.6930\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 218us/step - loss: 0.0609 - acc: 0.9912 - val_loss: 0.8594 - val_acc: 0.7076\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 210us/step - loss: 0.0347 - acc: 1.0000 - val_loss: 0.8594 - val_acc: 0.7144\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 254us/step - loss: 0.0212 - acc: 1.0000 - val_loss: 0.8673 - val_acc: 0.7212\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 260us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.8801 - val_acc: 0.7261\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 175us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.8953 - val_acc: 0.7173\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.7950 - acc: 0.2544 - val_loss: 1.4890 - val_acc: 0.4279\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 200us/step - loss: 1.1355 - acc: 0.5614 - val_loss: 1.3397 - val_acc: 0.5244\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 166us/step - loss: 0.8220 - acc: 0.7368 - val_loss: 1.2564 - val_acc: 0.5565\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 152us/step - loss: 0.6162 - acc: 0.8158 - val_loss: 1.1838 - val_acc: 0.5799\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 140us/step - loss: 0.4637 - acc: 0.8860 - val_loss: 1.1122 - val_acc: 0.6101\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 165us/step - loss: 0.3420 - acc: 0.9298 - val_loss: 1.0517 - val_acc: 0.6257\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 182us/step - loss: 0.2490 - acc: 0.9825 - val_loss: 1.0073 - val_acc: 0.6472\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 174us/step - loss: 0.1837 - acc: 0.9912 - val_loss: 0.9736 - val_acc: 0.6589\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 151us/step - loss: 0.1387 - acc: 1.0000 - val_loss: 0.9480 - val_acc: 0.6725\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 224us/step - loss: 0.1055 - acc: 1.0000 - val_loss: 0.9305 - val_acc: 0.6823\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 1.6452 - acc: 0.2719 - val_loss: 1.3059 - val_acc: 0.4971\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 180us/step - loss: 0.8722 - acc: 0.6667 - val_loss: 1.1665 - val_acc: 0.5536\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 244us/step - loss: 0.5582 - acc: 0.8158 - val_loss: 1.0902 - val_acc: 0.5653\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 200us/step - loss: 0.3641 - acc: 0.8772 - val_loss: 1.0301 - val_acc: 0.6033\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 191us/step - loss: 0.2249 - acc: 0.9825 - val_loss: 0.9836 - val_acc: 0.6218\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 196us/step - loss: 0.1385 - acc: 1.0000 - val_loss: 0.9520 - val_acc: 0.6355\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 169us/step - loss: 0.0810 - acc: 1.0000 - val_loss: 0.9328 - val_acc: 0.6530\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 196us/step - loss: 0.0498 - acc: 1.0000 - val_loss: 0.9210 - val_acc: 0.6667\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 0s 143us/step - loss: 0.0324 - acc: 1.0000 - val_loss: 0.9172 - val_acc: 0.6764\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 185us/step - loss: 0.0220 - acc: 1.0000 - val_loss: 0.9207 - val_acc: 0.6842\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.8350 - acc: 0.1930 - val_loss: 1.2804 - val_acc: 0.5107\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 170us/step - loss: 0.8478 - acc: 0.7105 - val_loss: 1.1431 - val_acc: 0.5673\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 179us/step - loss: 0.5193 - acc: 0.8509 - val_loss: 1.0249 - val_acc: 0.6238\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 183us/step - loss: 0.3003 - acc: 0.9298 - val_loss: 0.9452 - val_acc: 0.6589\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 182us/step - loss: 0.1767 - acc: 0.9825 - val_loss: 0.8972 - val_acc: 0.6832\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 220us/step - loss: 0.1097 - acc: 0.9912 - val_loss: 0.8763 - val_acc: 0.7057\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 205us/step - loss: 0.0717 - acc: 0.9912 - val_loss: 0.8719 - val_acc: 0.7057\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 203us/step - loss: 0.0486 - acc: 1.0000 - val_loss: 0.8771 - val_acc: 0.7105\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 247us/step - loss: 0.0339 - acc: 1.0000 - val_loss: 0.8855 - val_acc: 0.7086\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 262us/step - loss: 0.0245 - acc: 1.0000 - val_loss: 0.8965 - val_acc: 0.7066\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.8324 - acc: 0.1667 - val_loss: 1.2364 - val_acc: 0.5253\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 231us/step - loss: 0.7643 - acc: 0.6842 - val_loss: 1.1102 - val_acc: 0.5789\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 209us/step - loss: 0.4110 - acc: 0.8772 - val_loss: 0.9704 - val_acc: 0.6550\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 239us/step - loss: 0.2029 - acc: 0.9825 - val_loss: 0.9132 - val_acc: 0.6657\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 236us/step - loss: 0.1121 - acc: 1.0000 - val_loss: 0.9140 - val_acc: 0.6637\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 349us/step - loss: 0.0677 - acc: 1.0000 - val_loss: 0.9335 - val_acc: 0.6696\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 261us/step - loss: 0.0426 - acc: 1.0000 - val_loss: 0.9497 - val_acc: 0.6774\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 286us/step - loss: 0.0270 - acc: 1.0000 - val_loss: 0.9610 - val_acc: 0.6842\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 285us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.9686 - val_acc: 0.6862\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 214us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.9758 - val_acc: 0.6881\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.7138 - acc: 0.2982 - val_loss: 1.1933 - val_acc: 0.5351\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 167us/step - loss: 0.6811 - acc: 0.7105 - val_loss: 1.0699 - val_acc: 0.5809\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 194us/step - loss: 0.3587 - acc: 0.8947 - val_loss: 0.9634 - val_acc: 0.6462\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 219us/step - loss: 0.1739 - acc: 0.9825 - val_loss: 0.8989 - val_acc: 0.6930\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 219us/step - loss: 0.0853 - acc: 1.0000 - val_loss: 0.8943 - val_acc: 0.6949\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 239us/step - loss: 0.0494 - acc: 1.0000 - val_loss: 0.9155 - val_acc: 0.6901\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 193us/step - loss: 0.0303 - acc: 1.0000 - val_loss: 0.9455 - val_acc: 0.6871\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 243us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.9802 - val_acc: 0.6852\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 194us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 1.0159 - val_acc: 0.6891\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 254us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 1.0509 - val_acc: 0.6930\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 1.8343 - acc: 0.1842 - val_loss: 1.4286 - val_acc: 0.4103\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 181us/step - loss: 1.1308 - acc: 0.6053 - val_loss: 1.2563 - val_acc: 0.5185\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 169us/step - loss: 0.8028 - acc: 0.7632 - val_loss: 1.1614 - val_acc: 0.5565\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 170us/step - loss: 0.5921 - acc: 0.8596 - val_loss: 1.0916 - val_acc: 0.5838\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 186us/step - loss: 0.4357 - acc: 0.9211 - val_loss: 1.0385 - val_acc: 0.6179\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 143us/step - loss: 0.3247 - acc: 0.9737 - val_loss: 0.9959 - val_acc: 0.6316\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 186us/step - loss: 0.2376 - acc: 0.9825 - val_loss: 0.9617 - val_acc: 0.6481\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 165us/step - loss: 0.1746 - acc: 1.0000 - val_loss: 0.9325 - val_acc: 0.6589\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 194us/step - loss: 0.1266 - acc: 1.0000 - val_loss: 0.9079 - val_acc: 0.6715\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 190us/step - loss: 0.0930 - acc: 1.0000 - val_loss: 0.8898 - val_acc: 0.6803\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 2.5671 - acc: 0.1140 - val_loss: 1.5736 - val_acc: 0.3285\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 250us/step - loss: 1.1793 - acc: 0.5702 - val_loss: 1.2532 - val_acc: 0.5322\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 206us/step - loss: 0.6385 - acc: 0.8333 - val_loss: 1.1498 - val_acc: 0.5770\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 205us/step - loss: 0.4226 - acc: 0.9123 - val_loss: 1.1177 - val_acc: 0.5945\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 237us/step - loss: 0.3066 - acc: 0.9298 - val_loss: 1.0831 - val_acc: 0.6150\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 264us/step - loss: 0.2083 - acc: 0.9561 - val_loss: 1.0436 - val_acc: 0.6326\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 258us/step - loss: 0.1336 - acc: 0.9912 - val_loss: 1.0027 - val_acc: 0.6520\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 217us/step - loss: 0.0833 - acc: 1.0000 - val_loss: 0.9665 - val_acc: 0.6589\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 268us/step - loss: 0.0501 - acc: 1.0000 - val_loss: 0.9429 - val_acc: 0.6686\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 207us/step - loss: 0.0330 - acc: 1.0000 - val_loss: 0.9287 - val_acc: 0.6696\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.6092 - acc: 0.4298 - val_loss: 1.1824 - val_acc: 0.5497\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 224us/step - loss: 0.6742 - acc: 0.8246 - val_loss: 1.0293 - val_acc: 0.6316\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 181us/step - loss: 0.3472 - acc: 0.9737 - val_loss: 0.9505 - val_acc: 0.6745\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 172us/step - loss: 0.1884 - acc: 1.0000 - val_loss: 0.9156 - val_acc: 0.6881\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 197us/step - loss: 0.1000 - acc: 1.0000 - val_loss: 0.9052 - val_acc: 0.6930\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 199us/step - loss: 0.0549 - acc: 1.0000 - val_loss: 0.9086 - val_acc: 0.7008\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 208us/step - loss: 0.0310 - acc: 1.0000 - val_loss: 0.9195 - val_acc: 0.6979\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 0s 179us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.9351 - val_acc: 0.7076\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 185us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.9532 - val_acc: 0.7086\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 183us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.9717 - val_acc: 0.7144\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 2.0338 - acc: 0.1491 - val_loss: 1.2148 - val_acc: 0.5497\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 187us/step - loss: 0.7785 - acc: 0.7368 - val_loss: 1.1358 - val_acc: 0.5595\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 185us/step - loss: 0.4785 - acc: 0.7895 - val_loss: 1.0569 - val_acc: 0.6043\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 210us/step - loss: 0.2627 - acc: 0.9474 - val_loss: 0.9653 - val_acc: 0.6511\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 177us/step - loss: 0.1296 - acc: 1.0000 - val_loss: 0.8986 - val_acc: 0.6823\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 201us/step - loss: 0.0643 - acc: 1.0000 - val_loss: 0.8630 - val_acc: 0.6969\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 185us/step - loss: 0.0348 - acc: 1.0000 - val_loss: 0.8541 - val_acc: 0.6949\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 225us/step - loss: 0.0207 - acc: 1.0000 - val_loss: 0.8613 - val_acc: 0.7066\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 211us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.8777 - val_acc: 0.7144\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 211us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.8984 - val_acc: 0.7096\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.7110 - acc: 0.2281 - val_loss: 1.1830 - val_acc: 0.5419\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 176us/step - loss: 0.6111 - acc: 0.7456 - val_loss: 0.9948 - val_acc: 0.6160\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 167us/step - loss: 0.2616 - acc: 0.9825 - val_loss: 0.8634 - val_acc: 0.6979\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 176us/step - loss: 0.1098 - acc: 1.0000 - val_loss: 0.8188 - val_acc: 0.7193\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 200us/step - loss: 0.0555 - acc: 1.0000 - val_loss: 0.8156 - val_acc: 0.7251\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 188us/step - loss: 0.0305 - acc: 1.0000 - val_loss: 0.8282 - val_acc: 0.7242\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 210us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.8457 - val_acc: 0.7261\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 240us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.8645 - val_acc: 0.7290\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 227us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.8836 - val_acc: 0.7310\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 229us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.9023 - val_acc: 0.7300\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 2.5833 - acc: 0.1316 - val_loss: 1.9606 - val_acc: 0.1784\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 239us/step - loss: 1.6094 - acc: 0.3509 - val_loss: 1.5727 - val_acc: 0.3148\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 250us/step - loss: 1.0194 - acc: 0.6842 - val_loss: 1.3291 - val_acc: 0.4825\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 246us/step - loss: 0.6793 - acc: 0.8684 - val_loss: 1.1994 - val_acc: 0.5448\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 332us/step - loss: 0.4856 - acc: 0.9211 - val_loss: 1.1469 - val_acc: 0.5721\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 277us/step - loss: 0.3735 - acc: 0.9298 - val_loss: 1.1264 - val_acc: 0.5897\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 216us/step - loss: 0.2945 - acc: 0.9298 - val_loss: 1.1092 - val_acc: 0.6014\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 254us/step - loss: 0.2224 - acc: 0.9561 - val_loss: 1.0811 - val_acc: 0.6092\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 247us/step - loss: 0.1610 - acc: 0.9825 - val_loss: 1.0483 - val_acc: 0.6257\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 223us/step - loss: 0.1120 - acc: 0.9912 - val_loss: 1.0170 - val_acc: 0.6394\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 2.1399 - acc: 0.2105 - val_loss: 1.3974 - val_acc: 0.4522\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 187us/step - loss: 0.9121 - acc: 0.7018 - val_loss: 1.1930 - val_acc: 0.5546\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 255us/step - loss: 0.5221 - acc: 0.8246 - val_loss: 1.0947 - val_acc: 0.5936\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 279us/step - loss: 0.3185 - acc: 0.9386 - val_loss: 1.0228 - val_acc: 0.6306\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 243us/step - loss: 0.1886 - acc: 0.9912 - val_loss: 0.9677 - val_acc: 0.6647\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 229us/step - loss: 0.1138 - acc: 1.0000 - val_loss: 0.9347 - val_acc: 0.6842\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 241us/step - loss: 0.0701 - acc: 1.0000 - val_loss: 0.9179 - val_acc: 0.6949\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 248us/step - loss: 0.0452 - acc: 1.0000 - val_loss: 0.9104 - val_acc: 0.7027\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 165us/step - loss: 0.0299 - acc: 1.0000 - val_loss: 0.9091 - val_acc: 0.6979\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 271us/step - loss: 0.0214 - acc: 1.0000 - val_loss: 0.9113 - val_acc: 0.6979\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.8167 - acc: 0.1842 - val_loss: 1.2762 - val_acc: 0.5185\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 170us/step - loss: 0.7433 - acc: 0.7982 - val_loss: 1.1586 - val_acc: 0.5692\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 181us/step - loss: 0.3647 - acc: 0.9474 - val_loss: 1.0575 - val_acc: 0.6199\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 178us/step - loss: 0.1727 - acc: 0.9912 - val_loss: 0.9952 - val_acc: 0.6394\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 173us/step - loss: 0.0875 - acc: 1.0000 - val_loss: 0.9626 - val_acc: 0.6550\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 182us/step - loss: 0.0484 - acc: 1.0000 - val_loss: 0.9541 - val_acc: 0.6647\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 177us/step - loss: 0.0295 - acc: 1.0000 - val_loss: 0.9583 - val_acc: 0.6715\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 178us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.9685 - val_acc: 0.6784\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 192us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.9810 - val_acc: 0.6862\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 197us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.9944 - val_acc: 0.6871\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 1.6185 - acc: 0.2895 - val_loss: 1.1999 - val_acc: 0.5614\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 246us/step - loss: 0.5094 - acc: 0.9211 - val_loss: 1.0243 - val_acc: 0.6267\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 273us/step - loss: 0.2204 - acc: 0.9649 - val_loss: 0.9304 - val_acc: 0.6589\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 289us/step - loss: 0.1051 - acc: 1.0000 - val_loss: 0.8880 - val_acc: 0.6891\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 355us/step - loss: 0.0529 - acc: 1.0000 - val_loss: 0.8779 - val_acc: 0.6969\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 317us/step - loss: 0.0289 - acc: 1.0000 - val_loss: 0.8821 - val_acc: 0.6949\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 0s 253us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.8944 - val_acc: 0.7008\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 208us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.9111 - val_acc: 0.6969\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 263us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.9294 - val_acc: 0.6979\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 230us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.9475 - val_acc: 0.6998\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.8944 - acc: 0.1404 - val_loss: 1.1934 - val_acc: 0.5643\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 197us/step - loss: 0.5178 - acc: 0.8947 - val_loss: 1.0422 - val_acc: 0.6150\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 257us/step - loss: 0.2081 - acc: 0.9912 - val_loss: 0.9549 - val_acc: 0.6579\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 273us/step - loss: 0.0828 - acc: 1.0000 - val_loss: 0.9164 - val_acc: 0.6784\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 252us/step - loss: 0.0370 - acc: 1.0000 - val_loss: 0.9139 - val_acc: 0.6881\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 218us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.9293 - val_acc: 0.6940\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 190us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.9522 - val_acc: 0.6998\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 300us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.9771 - val_acc: 0.7018\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 244us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 1.0014 - val_acc: 0.7027\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 232us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 1.0239 - val_acc: 0.7057\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 2.0177 - acc: 0.1930 - val_loss: 1.5553 - val_acc: 0.3470\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 170us/step - loss: 1.1667 - acc: 0.5088 - val_loss: 1.3504 - val_acc: 0.4786\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 203us/step - loss: 0.7713 - acc: 0.7807 - val_loss: 1.2528 - val_acc: 0.5214\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 225us/step - loss: 0.5577 - acc: 0.8772 - val_loss: 1.1951 - val_acc: 0.5497\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 224us/step - loss: 0.4066 - acc: 0.9123 - val_loss: 1.1443 - val_acc: 0.5770\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 226us/step - loss: 0.2954 - acc: 0.9474 - val_loss: 1.1073 - val_acc: 0.6043\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 201us/step - loss: 0.2181 - acc: 0.9649 - val_loss: 1.0798 - val_acc: 0.6199\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 205us/step - loss: 0.1617 - acc: 0.9737 - val_loss: 1.0582 - val_acc: 0.6248\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 202us/step - loss: 0.1192 - acc: 0.9912 - val_loss: 1.0415 - val_acc: 0.6296\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 217us/step - loss: 0.0872 - acc: 1.0000 - val_loss: 1.0294 - val_acc: 0.6306\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 2.0224 - acc: 0.1316 - val_loss: 1.3642 - val_acc: 0.4542\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 147us/step - loss: 0.8256 - acc: 0.7456 - val_loss: 1.2136 - val_acc: 0.5439\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 182us/step - loss: 0.4746 - acc: 0.8684 - val_loss: 1.1552 - val_acc: 0.5604\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 189us/step - loss: 0.2973 - acc: 0.9211 - val_loss: 1.0992 - val_acc: 0.5984\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 210us/step - loss: 0.1755 - acc: 0.9737 - val_loss: 1.0541 - val_acc: 0.6111\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 215us/step - loss: 0.1007 - acc: 1.0000 - val_loss: 1.0268 - val_acc: 0.6316\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 184us/step - loss: 0.0576 - acc: 1.0000 - val_loss: 1.0113 - val_acc: 0.6404\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 220us/step - loss: 0.0365 - acc: 1.0000 - val_loss: 1.0032 - val_acc: 0.6511\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 278us/step - loss: 0.0236 - acc: 1.0000 - val_loss: 1.0014 - val_acc: 0.6540\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 231us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 1.0039 - val_acc: 0.6569\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.7516 - acc: 0.2105 - val_loss: 1.2205 - val_acc: 0.5595\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 235us/step - loss: 0.6624 - acc: 0.7632 - val_loss: 1.1005 - val_acc: 0.6023\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 268us/step - loss: 0.3393 - acc: 0.9211 - val_loss: 1.0219 - val_acc: 0.6345\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 236us/step - loss: 0.1607 - acc: 1.0000 - val_loss: 0.9702 - val_acc: 0.6589\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 184us/step - loss: 0.0775 - acc: 1.0000 - val_loss: 0.9439 - val_acc: 0.6589\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 249us/step - loss: 0.0434 - acc: 1.0000 - val_loss: 0.9420 - val_acc: 0.6618\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 265us/step - loss: 0.0279 - acc: 1.0000 - val_loss: 0.9506 - val_acc: 0.6657\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 271us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.9618 - val_acc: 0.6676\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 216us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.9724 - val_acc: 0.6754\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 330us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.9817 - val_acc: 0.6754\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.6753 - acc: 0.2368 - val_loss: 1.2299 - val_acc: 0.5166\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 204us/step - loss: 0.5428 - acc: 0.7807 - val_loss: 1.1372 - val_acc: 0.5575\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 213us/step - loss: 0.2207 - acc: 0.9912 - val_loss: 1.0167 - val_acc: 0.6462\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 174us/step - loss: 0.0867 - acc: 1.0000 - val_loss: 0.9589 - val_acc: 0.6598\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 214us/step - loss: 0.0419 - acc: 1.0000 - val_loss: 0.9501 - val_acc: 0.6598\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 0s 195us/step - loss: 0.0247 - acc: 1.0000 - val_loss: 0.9628 - val_acc: 0.6589\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 197us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.9834 - val_acc: 0.6647\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 214us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 1.0057 - val_acc: 0.6667\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 213us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 1.0271 - val_acc: 0.6686\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 736us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 1.0470 - val_acc: 0.6715\n",
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 1.8982 - acc: 0.1491 - val_loss: 1.2603 - val_acc: 0.5039\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 0s 216us/step - loss: 0.6363 - acc: 0.7895 - val_loss: 1.2042 - val_acc: 0.5263\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 0s 162us/step - loss: 0.3003 - acc: 0.9123 - val_loss: 1.0721 - val_acc: 0.5936\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 0s 205us/step - loss: 0.1106 - acc: 0.9912 - val_loss: 0.9913 - val_acc: 0.6608\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 0s 161us/step - loss: 0.0424 - acc: 1.0000 - val_loss: 0.9927 - val_acc: 0.6667\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 0s 199us/step - loss: 0.0234 - acc: 1.0000 - val_loss: 1.0277 - val_acc: 0.6608\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 0s 195us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 1.0692 - val_acc: 0.6676\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 0s 203us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 1.1070 - val_acc: 0.6657\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 0s 239us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 1.1411 - val_acc: 0.6725\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 0s 239us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 1.1711 - val_acc: 0.6696\n",
      "The best npc is 60, and the best nnode is 200.\n",
      "The best validation accuracy is 0.734893.\n"
     ]
    }
   ],
   "source": [
    "## TO DO\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y,test_size=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y,test_size=0.9) \n",
    "n_samples, _ = X_train.shape\n",
    "Xtr_mean = np.mean(X_train,0)\n",
    "Xtr = X_train - Xtr_mean[None,:]\n",
    "Utr,Str,Vtr = np.linalg.svd(Xtr, full_matrices=False)\n",
    "nnodes = [50,100,150,200,250]\n",
    "npcs = [50,60,70,80,90,100]\n",
    "result = np.zeros((len(npcs),len(nnodes)))\n",
    "loss_hist = []\n",
    "train_acc_hist = []\n",
    "val_acc_hist = []\n",
    "for i,npc in enumerate(npcs):\n",
    "    for j,nnode in enumerate(nnodes):\n",
    "        K.clear_session()\n",
    "        eigenface = Vtr[:npc,:]\n",
    "        Xtr_pca = Xtr.dot(eigenface.T)\n",
    "        Xtr_pca_s = Xtr_pca / Str[None,:npc] * np.sqrt(n_samples) \n",
    "        Xts = X_test - Xtr_mean[None,:]\n",
    "        Xts_pca = Xts.dot(eigenface.T)\n",
    "        Xts_pca_s = Xts_pca / Str[None,:npc] * np.sqrt(n_samples)\n",
    "        nin = Xtr_pca.shape[1] # dimension of input data\n",
    "        nh = nnode # number of hidden units\n",
    "        nout = int(np.max(y_train)+1)\n",
    "        model = Sequential()\n",
    "        model.add(Dense(nh, input_shape=(nin,), activation='relu', name='hidden')) \n",
    "        model.add(Dense(nout, activation='softmax', name='output'))\n",
    "        opt = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        hist = model.fit(Xtr_pca_s, y_train, epochs=10, batch_size=100,\n",
    "                         validation_data=(Xts_pca_s, y_test))\n",
    "        result[i][j] = hist.history['val_acc'][-1]\n",
    "highest_accuracy = result[0][0] \n",
    "opt_npc_index = 0\n",
    "opt_nnode_index = 0\n",
    "for i in range(0,len(npcs)):\n",
    "    for j in range(0,len(nnodes)):\n",
    "        if result[i][j] > highest_accuracy:\n",
    "            highest_accuracy = result[i][j]\n",
    "            opt_npc_index = i\n",
    "            opt_nnode_index = j\n",
    "print(\"The best npc is %d, and the best nnode is %d.\" % (npcs[opt_npc_index],nnodes[opt_nnode_index]))\n",
    "print(\"The best validation accuracy is %f.\" % highest_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114 samples, validate on 1026 samples\n",
      "Epoch 1/40\n",
      "114/114 [==============================] - 1s 10ms/step - loss: 1.5895 - acc: 0.1579 - val_loss: 1.4190 - val_acc: 0.4649\n",
      "Epoch 2/40\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 1.4490 - acc: 0.4649 - val_loss: 1.4260 - val_acc: 0.4649\n",
      "Epoch 3/40\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 1.4300 - acc: 0.4649 - val_loss: 1.4114 - val_acc: 0.4649\n",
      "Epoch 4/40\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 1.4118 - acc: 0.4649 - val_loss: 1.4127 - val_acc: 0.4649\n",
      "Epoch 5/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.4072 - acc: 0.4649 - val_loss: 1.4078 - val_acc: 0.4649\n",
      "Epoch 6/40\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 1.3997 - acc: 0.4649 - val_loss: 1.4003 - val_acc: 0.4649\n",
      "Epoch 7/40\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 1.3937 - acc: 0.4649 - val_loss: 1.3972 - val_acc: 0.4649\n",
      "Epoch 8/40\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 1.3846 - acc: 0.4649 - val_loss: 1.3971 - val_acc: 0.4649\n",
      "Epoch 9/40\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 1.3803 - acc: 0.4649 - val_loss: 1.3972 - val_acc: 0.4649\n",
      "Epoch 10/40\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 1.3776 - acc: 0.4649 - val_loss: 1.4105 - val_acc: 0.4649\n",
      "Epoch 11/40\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 1.3877 - acc: 0.4649 - val_loss: 1.4149 - val_acc: 0.4649\n",
      "Epoch 12/40\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 1.3900 - acc: 0.4649 - val_loss: 1.3890 - val_acc: 0.4649\n",
      "Epoch 13/40\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 1.3607 - acc: 0.4649 - val_loss: 1.3822 - val_acc: 0.4649\n",
      "Epoch 14/40\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 1.3624 - acc: 0.4649 - val_loss: 1.3970 - val_acc: 0.4649\n",
      "Epoch 15/40\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 1.3707 - acc: 0.4649 - val_loss: 1.3962 - val_acc: 0.4649\n",
      "Epoch 16/40\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 1.3646 - acc: 0.4737 - val_loss: 1.3887 - val_acc: 0.4649\n",
      "Epoch 17/40\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 1.3531 - acc: 0.4649 - val_loss: 1.3804 - val_acc: 0.4649\n",
      "Epoch 18/40\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 1.3442 - acc: 0.4649 - val_loss: 1.3738 - val_acc: 0.4649\n",
      "Epoch 19/40\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 1.3362 - acc: 0.4649 - val_loss: 1.3636 - val_acc: 0.4649\n",
      "Epoch 20/40\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 1.3246 - acc: 0.4649 - val_loss: 1.3577 - val_acc: 0.4649\n",
      "Epoch 21/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.3189 - acc: 0.4649 - val_loss: 1.3567 - val_acc: 0.4649\n",
      "Epoch 22/40\n",
      "114/114 [==============================] - 1s 8ms/step - loss: 1.3157 - acc: 0.4649 - val_loss: 1.3523 - val_acc: 0.4649\n",
      "Epoch 23/40\n",
      "114/114 [==============================] - 1s 8ms/step - loss: 1.3067 - acc: 0.4649 - val_loss: 1.3450 - val_acc: 0.4649\n",
      "Epoch 24/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.2926 - acc: 0.4649 - val_loss: 1.3357 - val_acc: 0.4649\n",
      "Epoch 25/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.2768 - acc: 0.4649 - val_loss: 1.3431 - val_acc: 0.4698\n",
      "Epoch 26/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.2780 - acc: 0.4912 - val_loss: 1.3382 - val_acc: 0.4912\n",
      "Epoch 27/40\n",
      "114/114 [==============================] - 1s 9ms/step - loss: 1.2625 - acc: 0.5439 - val_loss: 1.3160 - val_acc: 0.4669\n",
      "Epoch 28/40\n",
      "114/114 [==============================] - 1s 8ms/step - loss: 1.2444 - acc: 0.4825 - val_loss: 1.3101 - val_acc: 0.4649\n",
      "Epoch 29/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.2335 - acc: 0.4649 - val_loss: 1.3097 - val_acc: 0.4649\n",
      "Epoch 30/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.2311 - acc: 0.4649 - val_loss: 1.3079 - val_acc: 0.4649\n",
      "Epoch 31/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.2195 - acc: 0.4649 - val_loss: 1.3035 - val_acc: 0.4649\n",
      "Epoch 32/40\n",
      "114/114 [==============================] - 1s 8ms/step - loss: 1.2071 - acc: 0.4649 - val_loss: 1.2769 - val_acc: 0.4688\n",
      "Epoch 33/40\n",
      "114/114 [==============================] - 1s 8ms/step - loss: 1.1675 - acc: 0.4737 - val_loss: 1.2695 - val_acc: 0.5088\n",
      "Epoch 34/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.1577 - acc: 0.5789 - val_loss: 1.2805 - val_acc: 0.5302\n",
      "Epoch 35/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.1555 - acc: 0.5877 - val_loss: 1.2569 - val_acc: 0.5331\n",
      "Epoch 36/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.1208 - acc: 0.5877 - val_loss: 1.2354 - val_acc: 0.5029\n",
      "Epoch 37/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.0941 - acc: 0.5439 - val_loss: 1.2267 - val_acc: 0.4854\n",
      "Epoch 38/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.0740 - acc: 0.5439 - val_loss: 1.2122 - val_acc: 0.5029\n",
      "Epoch 39/40\n",
      "114/114 [==============================] - 1s 7ms/step - loss: 1.0460 - acc: 0.5789 - val_loss: 1.2136 - val_acc: 0.5536\n",
      "Epoch 40/40\n",
      "114/114 [==============================] - 1s 8ms/step - loss: 1.0384 - acc: 0.6404 - val_loss: 1.2029 - val_acc: 0.5497\n",
      "The accuracy on validation set is:\n",
      "[0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.469785576849653, 0.4912280727315832, 0.4668616001252775, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.46881091600505465, 0.508771924363707, 0.53021442227893401, 0.53313839191581769, 0.50292396801024619, 0.48538011550670945, 0.50292396801024619, 0.55360623316922852, 0.54970759722689211]\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "Xtr_cnn = X_train.astype(\"float32\")/255\n",
    "Xts_cnn = X_test.astype(\"float32\")/255\n",
    "Xtr_cnn = np.reshape(Xtr_cnn, (len(Xtr_cnn),h,w,1)) \n",
    "Xts_cnn = np.reshape(Xts_cnn, (len(X)-len(Xtr_cnn),h,w,1))\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, (5, 5),\n",
    "                 padding='valid',\n",
    "                 input_shape=Xtr_cnn.shape[1:],\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(8, (5, 5), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(nout, activation='softmax'))\n",
    "opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# Let's train the model using Adam\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "hist_basic = model.fit(Xtr_cnn, y_train,batch_size=100,epochs=40,\n",
    "                       validation_data=(Xts_cnn, y_test),shuffle=True) \n",
    "print(\"The accuracy on validation set is:\")\n",
    "print(hist_basic.history['val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How does CNN compare with PCA+NN with the small training set? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A:The validation accuracy CNN gets is much smaller than that of PCA+NN. It probably because of its smaller training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
