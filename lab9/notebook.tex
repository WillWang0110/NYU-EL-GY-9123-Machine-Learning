
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Lab09a\_pca\_NN\_CNN\_partial}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Lab 9a: PCA for Face
Recognition}\label{lab-9a-pca-for-face-recognition}

Following the demo for this unit, we will explore further the use of PCA
for feature dimension reduction for classification. We will use a
2-layer neural net on the PCA coefficients. We will practice optimizing
the classificaiton parameters (the number of PCA components and the
number of hidden nodes in the NN classifier). We will furthermore
compare this approach with using convolutional neural net on raw images.

Through the lab, you will learn to:

\begin{itemize}
\tightlist
\item
  Perform PCA on the a face dataset to find the PC components
\item
  Evaluate the effect of using different nubmer of principle components
  for data representation and classification.
\item
  Optimize the number of PC coefficients and classifier parameters
  together to maximize classification accuracy.
\item
  Understand the impact of training data size on the feature and
  classification method selection.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} Import the flw\PYZus{}people dataset. }
        \PY{c+c1}{\PYZsh{} Select only those people with at least 100 instances }
        \PY{c+c1}{\PYZsh{} Reduce the face image size by 0.4}
        
        \PY{c+c1}{\PYZsh{} TO DO}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{import} \PY{n}{fetch\PYZus{}lfw\PYZus{}people}
        \PY{n}{lfw\PYZus{}people} \PY{o}{=} \PY{n}{fetch\PYZus{}lfw\PYZus{}people}\PY{p}{(}\PY{n}{min\PYZus{}faces\PYZus{}per\PYZus{}person}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{resize}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Downloading LFW metadata: https://ndownloader.figshare.com/files/5976012
Downloading LFW metadata: https://ndownloader.figshare.com/files/5976009
Downloading LFW metadata: https://ndownloader.figshare.com/files/5976006
Downloading LFW data (\textasciitilde{}200MB): https://ndownloader.figshare.com/files/5976015

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} Save the face images in a datamatrix X and the labels and corresponding names in a datamatrix y and target\PYZus{}names}
        \PY{c+c1}{\PYZsh{} Furthermore, determine the number of samples and the image size }
        \PY{c+c1}{\PYZsh{} Determine the number of different faces (number of classes)}
        
        \PY{c+c1}{\PYZsh{} TO DO}
        \PY{n}{n\PYZus{}samples}\PY{p}{,} \PY{n}{h}\PY{p}{,} \PY{n}{w} \PY{o}{=} \PY{n}{lfw\PYZus{}people}\PY{o}{.}\PY{n}{images}\PY{o}{.}\PY{n}{shape}
        \PY{n}{npix} \PY{o}{=} \PY{n}{h}\PY{o}{*}\PY{n}{w}
        \PY{c+c1}{\PYZsh{} Data in 2D form}
        \PY{n}{X} \PY{o}{=} \PY{n}{lfw\PYZus{}people}\PY{o}{.}\PY{n}{data}
        \PY{n}{n\PYZus{}features} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} Labels of images}
        \PY{n}{y} \PY{o}{=} \PY{n}{lfw\PYZus{}people}\PY{o}{.}\PY{n}{target}
        \PY{n}{target\PYZus{}names} \PY{o}{=} \PY{n}{lfw\PYZus{}people}\PY{o}{.}\PY{n}{target\PYZus{}names}
        \PY{n}{n\PYZus{}classes} \PY{o}{=} \PY{n}{target\PYZus{}names}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Image size = }\PY{l+s+si}{\PYZob{}0:d\PYZcb{}}\PY{l+s+s2}{ x }\PY{l+s+si}{\PYZob{}1:d\PYZcb{}}\PY{l+s+s2}{ = }\PY{l+s+si}{\PYZob{}2:d\PYZcb{}}\PY{l+s+s2}{ pixels}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{h}\PY{p}{,}\PY{n}{w}\PY{p}{,}\PY{n}{npix}\PY{p}{)}\PY{p}{)} 
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number faces = }\PY{l+s+si}{\PYZob{}0:d\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number classes = }\PY{l+s+si}{\PYZob{}0:d\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{n\PYZus{}classes}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Image size = 50 x 37 = 1850 pixels
Number faces = 1140
Number classes = 5

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} Plot some sample images to make sure your data load is correct}
        \PY{k}{def} \PY{n+nf}{plt\PYZus{}face}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
            \PY{n}{h} \PY{o}{=} \PY{l+m+mi}{50}
            \PY{n}{w} \PY{o}{=} \PY{l+m+mi}{37}
            \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{h}\PY{p}{,} \PY{n}{w}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{cm}\PY{o}{.}\PY{n}{gray}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{)}
        \PY{n}{I} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{permutation}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{p}{)} 
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{)}
        \PY{n}{nplt} \PY{o}{=} \PY{l+m+mi}{4}\PY{p}{;}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{nplt}\PY{p}{)}\PY{p}{:}
            \PY{n}{ind} \PY{o}{=} \PY{n}{I}\PY{p}{[}\PY{n}{i}\PY{p}{]}
            \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{nplt}\PY{p}{,}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{plt\PYZus{}face}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{ind}\PY{p}{]}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{target\PYZus{}names}\PY{p}{[}\PY{n}{y}\PY{p}{[}\PY{n}{ind}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_4_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} Split the data into a training set and test set with 50\PYZpc{} data for training. }
        \PY{c+c1}{\PYZsh{} Use \PYZdq{}stratify\PYZdq{} option to make sure the training data and test data have same }
        \PY{c+c1}{\PYZsh{} proportion of images from different faces}
        \PY{c+c1}{\PYZsh{} print the number of samples in the training data}
        
        \PY{c+c1}{\PYZsh{} TO DO }
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split} 
        \PY{c+c1}{\PYZsh{} split into a training and testing set}
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
            \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y}\PY{p}{,}\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
        
        \PY{n}{n\PYZus{}samples\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number faces in training data = }\PY{l+s+si}{\PYZob{}0:d\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{n\PYZus{}samples\PYZus{}train}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number faces in training data = 570

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{} Perfom PCA on the training data to derive the principle components (PCs) and the PCA coefficients }
        \PY{c+c1}{\PYZsh{} You can directly use the PCA class in PCA package or use SVD.}
        \PY{c+c1}{\PYZsh{} Remember that you need to remove the mean from the data first}
        \PY{c+c1}{\PYZsh{} Also you should rescale the PCs so that the PCA coefficients all have unit variance}
        \PY{c+c1}{\PYZsh{} Determine the total number of PCs}
        
        \PY{c+c1}{\PYZsh{} TO DO }
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVC}
        \PY{n}{n\PYZus{}samples}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}
        \PY{n}{Xtr\PYZus{}mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{Xtr} \PY{o}{=} \PY{n}{X\PYZus{}train} \PY{o}{\PYZhy{}} \PY{n}{Xtr\PYZus{}mean}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{p}{]}
        \PY{n}{Utr}\PY{p}{,}\PY{n}{Str}\PY{p}{,}\PY{n}{Vtr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{svd}\PY{p}{(}\PY{n}{Xtr}\PY{p}{,} \PY{n}{full\PYZus{}matrices}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The total number of PCs is }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{Vtr}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The total number of PCs is 570.

    \end{Verbatim}

    First let us construct a 2-layer neural net classifier that uses npc=
100 PCA coefficients to classify the faces. Set up your training and
testing data to contain npc PCA coefficients using the previously
determined principle components. You should directly use matrix
multiplication (i.e. projecting original data to the first 100 principle
components you found previously) to find the coefficients rather then
using the pca.transform( ) method.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} TO DO}
         \PY{n}{npc} \PY{o}{=} \PY{l+m+mi}{100}
         \PY{n}{eigenface} \PY{o}{=} \PY{n}{Vtr}\PY{p}{[}\PY{p}{:}\PY{n}{npc}\PY{p}{,}\PY{p}{:}\PY{p}{]}
         \PY{n}{Xtr\PYZus{}pca} \PY{o}{=} \PY{n}{Xtr}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{eigenface}\PY{o}{.}\PY{n}{T}\PY{p}{)}
         \PY{n}{Xtr\PYZus{}pca\PYZus{}s} \PY{o}{=} \PY{n}{Xtr\PYZus{}pca} \PY{o}{/} \PY{n}{Str}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{n}{npc}\PY{p}{]} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{p}{)} 
         \PY{n}{Xts} \PY{o}{=} \PY{n}{X\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{Xtr\PYZus{}mean}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{p}{]}
         \PY{n}{Xts\PYZus{}pca} \PY{o}{=} \PY{n}{Xts}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{eigenface}\PY{o}{.}\PY{n}{T}\PY{p}{)}
         \PY{n}{Xts\PYZus{}pca\PYZus{}s} \PY{o}{=} \PY{n}{Xts\PYZus{}pca} \PY{o}{/} \PY{n}{Str}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{n}{npc}\PY{p}{]} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{p}{)}
\end{Verbatim}


    Now set up and compile a NN model with number of hidden nodes nnode=100
and a output layer, and then fit the model to the training data. Use
'relu' for the activation for the hidden layer and use 'softmax' for the
output layer. Using \texttt{sparse\_categorical\_crossentropy} for the
loss. Use \texttt{accuracy} as the metrics. You can choose to do a small
number of epochs (=10) with batch size =100. Determine the accuracy on
the validation set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{} TO DO}
         \PY{k+kn}{import} \PY{n+nn}{keras}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Model}\PY{p}{,} \PY{n}{Sequential} 
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Activation} 
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dropout}\PY{p}{,} \PY{n}{Flatten} 
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Conv2D}\PY{p}{,} \PY{n}{MaxPooling2D} 
         \PY{k+kn}{import} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{backend} \PY{k}{as} \PY{n+nn}{K}
         \PY{n}{K}\PY{o}{.}\PY{n}{clear\PYZus{}session}\PY{p}{(}\PY{p}{)}
         \PY{n}{nin} \PY{o}{=} \PY{n}{Xtr\PYZus{}pca}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{} dimension of input data }
         \PY{n}{nh} \PY{o}{=} \PY{l+m+mi}{100} \PY{c+c1}{\PYZsh{} number of hidden units}
         \PY{n}{nout} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{} number of outputs }
         \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nh}\PY{p}{,} \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{n}{nin}\PY{p}{,}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hidden}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nout}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{output}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
hidden (Dense)               (None, 100)               10100     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
output (Dense)               (None, 5)                 505       
=================================================================
Total params: 10,605
Trainable params: 10,605
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    Now try to identify the best number of PCs and the best number of hidden
nodes in the NN classifer that can achieve the highest validation
accuracy. You can set the range of PCs and nubmer of hidden nodes as
below.

nnodes = {[}50,100,150,200, 250{]}, npcs = {[}50,100,150,200{]}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{c+c1}{\PYZsh{} Set up an array to store accuracy for different nnode and npcs}
         \PY{c+c1}{\PYZsh{} TO DO}
         \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{optimizers}
         \PY{n}{opt} \PY{o}{=} \PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{beta\PYZus{}1}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{,} \PY{n}{beta\PYZus{}2}\PY{o}{=}\PY{l+m+mf}{0.999}\PY{p}{,} \PY{n}{epsilon}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}08}\PY{p}{,} \PY{n}{decay}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{)} 
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{n}{opt}\PY{p}{,}
                                \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{hist} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtr\PYZus{}pca\PYZus{}s}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
         
         \PY{n}{nnodes} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{150}\PY{p}{,}\PY{l+m+mi}{200}\PY{p}{,}\PY{l+m+mi}{250}\PY{p}{]}
         \PY{n}{npcs} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{150}\PY{p}{,}\PY{l+m+mi}{200}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/10
570/570 [==============================] - 0s 376us/step - loss: 0.0086 - acc: 0.9982
Epoch 2/10
570/570 [==============================] - 0s 35us/step - loss: 0.0031 - acc: 1.0000
Epoch 3/10
570/570 [==============================] - 0s 35us/step - loss: 0.0014 - acc: 1.0000
Epoch 4/10
570/570 [==============================] - 0s 38us/step - loss: 8.5248e-04 - acc: 1.0000
Epoch 5/10
570/570 [==============================] - 0s 34us/step - loss: 4.0785e-04 - acc: 1.0000
Epoch 6/10
570/570 [==============================] - 0s 30us/step - loss: 2.4188e-04 - acc: 1.0000
Epoch 7/10
570/570 [==============================] - 0s 30us/step - loss: 1.5982e-04 - acc: 1.0000
Epoch 8/10
570/570 [==============================] - 0s 36us/step - loss: 1.1410e-04 - acc: 1.0000
Epoch 9/10
570/570 [==============================] - 0s 26us/step - loss: 8.8744e-05 - acc: 1.0000
Epoch 10/10
570/570 [==============================] - 0s 34us/step - loss: 7.2105e-05 - acc: 1.0000

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} Loop through the combinations to find the accuracy for each combination}
         \PY{c+c1}{\PYZsh{} For each possible combination of `nnode` and `npc`, set up and fit the model }
         \PY{c+c1}{\PYZsh{} using features containing only coefficents corresponding to npc coefficients.}
         
         \PY{c+c1}{\PYZsh{} TO DO }
         \PY{n}{result} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{npcs}\PY{p}{)}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{nnodes}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{loss\PYZus{}hist} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{train\PYZus{}acc\PYZus{}hist} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{val\PYZus{}acc\PYZus{}hist} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{npc} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{npcs}\PY{p}{)}\PY{p}{:}
             \PY{k}{for} \PY{n}{j}\PY{p}{,}\PY{n}{nnode} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{nnodes}\PY{p}{)}\PY{p}{:}
                 \PY{n}{K}\PY{o}{.}\PY{n}{clear\PYZus{}session}\PY{p}{(}\PY{p}{)}
                 \PY{n}{eigenface} \PY{o}{=} \PY{n}{Vtr}\PY{p}{[}\PY{p}{:}\PY{n}{npc}\PY{p}{,}\PY{p}{:}\PY{p}{]}
                 \PY{n}{Xtr\PYZus{}pca} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{eigenface}\PY{o}{.}\PY{n}{T}\PY{p}{)}
                 \PY{n}{Xtr\PYZus{}pca\PYZus{}s} \PY{o}{=} \PY{n}{Xtr\PYZus{}pca} \PY{o}{/} \PY{n}{Str}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{n}{npc}\PY{p}{]} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{p}{)} 
                 \PY{n}{Xts} \PY{o}{=} \PY{n}{X\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{Xtr\PYZus{}mean}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{p}{]}
                 \PY{n}{Xts\PYZus{}pca} \PY{o}{=} \PY{n}{Xts}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{eigenface}\PY{o}{.}\PY{n}{T}\PY{p}{)}
                 \PY{n}{Xts\PYZus{}pca\PYZus{}s} \PY{o}{=} \PY{n}{Xts\PYZus{}pca} \PY{o}{/} \PY{n}{Str}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{n}{npc}\PY{p}{]} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{p}{)}
                 \PY{n}{nin} \PY{o}{=} \PY{n}{Xtr\PYZus{}pca}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{} dimension of input data}
                 \PY{n}{nh} \PY{o}{=} \PY{n}{nnode} \PY{c+c1}{\PYZsh{} number of hidden units}
                 \PY{n}{nout} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
                 \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nh}\PY{p}{,} \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{n}{nin}\PY{p}{,}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hidden}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} 
                 \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nout}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{output}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                 \PY{n}{opt} \PY{o}{=} \PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{beta\PYZus{}1}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{,} \PY{n}{beta\PYZus{}2}\PY{o}{=}\PY{l+m+mf}{0.999}\PY{p}{,} \PY{n}{epsilon}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}08}\PY{p}{,}\PY{n}{decay}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{)} 
                 \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{n}{opt}\PY{p}{,}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
                 \PY{n}{hist} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtr\PYZus{}pca\PYZus{}s}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
                                  \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{Xts\PYZus{}pca\PYZus{}s}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,}\PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} 
                 \PY{n}{result}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{hist}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 363us/step - loss: 1.5764 - acc: 0.4175 - val\_loss: 1.4764 - val\_acc: 0.4123
Epoch 2/10
570/570 [==============================] - 0s 36us/step - loss: 1.0512 - acc: 0.6263 - val\_loss: 1.2468 - val\_acc: 0.5421
Epoch 3/10
570/570 [==============================] - 0s 44us/step - loss: 0.7170 - acc: 0.7877 - val\_loss: 1.0887 - val\_acc: 0.6140
Epoch 4/10
570/570 [==============================] - 0s 45us/step - loss: 0.5308 - acc: 0.8491 - val\_loss: 0.9990 - val\_acc: 0.6316
Epoch 5/10
570/570 [==============================] - 0s 43us/step - loss: 0.3848 - acc: 0.8877 - val\_loss: 0.8993 - val\_acc: 0.6842
Epoch 6/10
570/570 [==============================] - 0s 34us/step - loss: 0.3033 - acc: 0.9158 - val\_loss: 0.8786 - val\_acc: 0.6895
Epoch 7/10
570/570 [==============================] - 0s 39us/step - loss: 0.2402 - acc: 0.9368 - val\_loss: 0.8422 - val\_acc: 0.7123
Epoch 8/10
570/570 [==============================] - 0s 38us/step - loss: 0.2031 - acc: 0.9474 - val\_loss: 0.8453 - val\_acc: 0.7088
Epoch 9/10
570/570 [==============================] - 0s 40us/step - loss: 0.1678 - acc: 0.9614 - val\_loss: 0.8573 - val\_acc: 0.7158
Epoch 10/10
570/570 [==============================] - 0s 36us/step - loss: 0.1440 - acc: 0.9702 - val\_loss: 0.8378 - val\_acc: 0.7228
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 376us/step - loss: 1.5896 - acc: 0.4053 - val\_loss: 1.3334 - val\_acc: 0.4737
Epoch 2/10
570/570 [==============================] - 0s 38us/step - loss: 0.8556 - acc: 0.6702 - val\_loss: 1.0258 - val\_acc: 0.6491
Epoch 3/10
570/570 [==============================] - 0s 53us/step - loss: 0.5079 - acc: 0.8737 - val\_loss: 0.8853 - val\_acc: 0.6947
Epoch 4/10
570/570 [==============================] - 0s 50us/step - loss: 0.3494 - acc: 0.9035 - val\_loss: 0.7995 - val\_acc: 0.7421
Epoch 5/10
570/570 [==============================] - 0s 50us/step - loss: 0.2530 - acc: 0.9281 - val\_loss: 0.7781 - val\_acc: 0.7263
Epoch 6/10
570/570 [==============================] - 0s 43us/step - loss: 0.1989 - acc: 0.9456 - val\_loss: 0.7361 - val\_acc: 0.7386
Epoch 7/10
570/570 [==============================] - 0s 48us/step - loss: 0.1576 - acc: 0.9632 - val\_loss: 0.7362 - val\_acc: 0.7509
Epoch 8/10
570/570 [==============================] - 0s 39us/step - loss: 0.1217 - acc: 0.9789 - val\_loss: 0.7306 - val\_acc: 0.7368
Epoch 9/10
570/570 [==============================] - 0s 43us/step - loss: 0.0939 - acc: 0.9895 - val\_loss: 0.7336 - val\_acc: 0.7439
Epoch 10/10
570/570 [==============================] - 0s 48us/step - loss: 0.0751 - acc: 0.9912 - val\_loss: 0.7363 - val\_acc: 0.7386
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 375us/step - loss: 1.6693 - acc: 0.4368 - val\_loss: 1.1651 - val\_acc: 0.5947
Epoch 2/10
570/570 [==============================] - 0s 41us/step - loss: 0.8089 - acc: 0.7228 - val\_loss: 0.9108 - val\_acc: 0.7035
Epoch 3/10
570/570 [==============================] - 0s 50us/step - loss: 0.5032 - acc: 0.8456 - val\_loss: 0.7556 - val\_acc: 0.7561
Epoch 4/10
570/570 [==============================] - 0s 44us/step - loss: 0.3448 - acc: 0.8895 - val\_loss: 0.7389 - val\_acc: 0.7456
Epoch 5/10
570/570 [==============================] - 0s 49us/step - loss: 0.2658 - acc: 0.9228 - val\_loss: 0.6983 - val\_acc: 0.7684
Epoch 6/10
570/570 [==============================] - 0s 48us/step - loss: 0.2091 - acc: 0.9421 - val\_loss: 0.6845 - val\_acc: 0.7614
Epoch 7/10
570/570 [==============================] - 0s 109us/step - loss: 0.1496 - acc: 0.9737 - val\_loss: 0.7254 - val\_acc: 0.7614
Epoch 8/10
570/570 [==============================] - 0s 90us/step - loss: 0.1149 - acc: 0.9807 - val\_loss: 0.7052 - val\_acc: 0.7702
Epoch 9/10
570/570 [==============================] - 0s 56us/step - loss: 0.0932 - acc: 0.9895 - val\_loss: 0.7424 - val\_acc: 0.7632
Epoch 10/10
570/570 [==============================] - 0s 58us/step - loss: 0.0725 - acc: 0.9860 - val\_loss: 0.7649 - val\_acc: 0.7544
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 497us/step - loss: 1.8334 - acc: 0.3947 - val\_loss: 1.0999 - val\_acc: 0.6333
Epoch 2/10
570/570 [==============================] - 0s 53us/step - loss: 0.9750 - acc: 0.6632 - val\_loss: 0.9434 - val\_acc: 0.6930
Epoch 3/10
570/570 [==============================] - 0s 44us/step - loss: 0.5965 - acc: 0.8053 - val\_loss: 0.7846 - val\_acc: 0.7333
Epoch 4/10
570/570 [==============================] - 0s 64us/step - loss: 0.3893 - acc: 0.8772 - val\_loss: 0.6896 - val\_acc: 0.7684
Epoch 5/10
570/570 [==============================] - 0s 51us/step - loss: 0.2602 - acc: 0.9246 - val\_loss: 0.6953 - val\_acc: 0.7632
Epoch 6/10
570/570 [==============================] - 0s 57us/step - loss: 0.2054 - acc: 0.9474 - val\_loss: 0.6573 - val\_acc: 0.7737
Epoch 7/10
570/570 [==============================] - 0s 48us/step - loss: 0.1513 - acc: 0.9596 - val\_loss: 0.6332 - val\_acc: 0.7860
Epoch 8/10
570/570 [==============================] - 0s 59us/step - loss: 0.1092 - acc: 0.9825 - val\_loss: 0.6524 - val\_acc: 0.7719
Epoch 9/10
570/570 [==============================] - 0s 47us/step - loss: 0.0822 - acc: 0.9895 - val\_loss: 0.6612 - val\_acc: 0.7719
Epoch 10/10
570/570 [==============================] - 0s 57us/step - loss: 0.0610 - acc: 0.9912 - val\_loss: 0.6721 - val\_acc: 0.7737
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 485us/step - loss: 1.4528 - acc: 0.4772 - val\_loss: 0.9997 - val\_acc: 0.6982
Epoch 2/10
570/570 [==============================] - 0s 54us/step - loss: 0.7021 - acc: 0.7421 - val\_loss: 0.8022 - val\_acc: 0.7351
Epoch 3/10
570/570 [==============================] - 0s 46us/step - loss: 0.4844 - acc: 0.8544 - val\_loss: 0.6714 - val\_acc: 0.7754
Epoch 4/10
570/570 [==============================] - 0s 49us/step - loss: 0.2950 - acc: 0.9035 - val\_loss: 0.6453 - val\_acc: 0.7789
Epoch 5/10
570/570 [==============================] - 0s 49us/step - loss: 0.2129 - acc: 0.9404 - val\_loss: 0.6420 - val\_acc: 0.7649
Epoch 6/10
570/570 [==============================] - 0s 45us/step - loss: 0.1460 - acc: 0.9719 - val\_loss: 0.6268 - val\_acc: 0.7842
Epoch 7/10
570/570 [==============================] - 0s 46us/step - loss: 0.1151 - acc: 0.9807 - val\_loss: 0.6277 - val\_acc: 0.7912
Epoch 8/10
570/570 [==============================] - 0s 49us/step - loss: 0.0766 - acc: 0.9895 - val\_loss: 0.6506 - val\_acc: 0.7860
Epoch 9/10
570/570 [==============================] - 0s 54us/step - loss: 0.0563 - acc: 0.9947 - val\_loss: 0.6385 - val\_acc: 0.7947
Epoch 10/10
570/570 [==============================] - 0s 91us/step - loss: 0.0409 - acc: 0.9965 - val\_loss: 0.6533 - val\_acc: 0.7930
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 420us/step - loss: 1.6058 - acc: 0.3544 - val\_loss: 1.4671 - val\_acc: 0.3702
Epoch 2/10
570/570 [==============================] - 0s 62us/step - loss: 1.1019 - acc: 0.6175 - val\_loss: 1.2648 - val\_acc: 0.4825
Epoch 3/10
570/570 [==============================] - 0s 50us/step - loss: 0.7890 - acc: 0.7246 - val\_loss: 1.1189 - val\_acc: 0.5649
Epoch 4/10
570/570 [==============================] - 0s 49us/step - loss: 0.4885 - acc: 0.8649 - val\_loss: 1.0264 - val\_acc: 0.6000
Epoch 5/10
570/570 [==============================] - 0s 51us/step - loss: 0.3013 - acc: 0.9333 - val\_loss: 0.9970 - val\_acc: 0.6070
Epoch 6/10
570/570 [==============================] - 0s 57us/step - loss: 0.2145 - acc: 0.9474 - val\_loss: 0.9312 - val\_acc: 0.6526
Epoch 7/10
570/570 [==============================] - 0s 56us/step - loss: 0.1478 - acc: 0.9789 - val\_loss: 0.9424 - val\_acc: 0.6474
Epoch 8/10
570/570 [==============================] - 0s 47us/step - loss: 0.0956 - acc: 0.9842 - val\_loss: 0.9176 - val\_acc: 0.6667
Epoch 9/10
570/570 [==============================] - 0s 37us/step - loss: 0.0646 - acc: 0.9947 - val\_loss: 0.9431 - val\_acc: 0.6702
Epoch 10/10
570/570 [==============================] - 0s 44us/step - loss: 0.0440 - acc: 0.9947 - val\_loss: 0.9487 - val\_acc: 0.6877
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 364us/step - loss: 1.5875 - acc: 0.4667 - val\_loss: 1.1933 - val\_acc: 0.6053
Epoch 2/10
570/570 [==============================] - 0s 38us/step - loss: 0.7907 - acc: 0.7544 - val\_loss: 0.9521 - val\_acc: 0.7018
Epoch 3/10
570/570 [==============================] - 0s 50us/step - loss: 0.3971 - acc: 0.8982 - val\_loss: 0.8306 - val\_acc: 0.7281
Epoch 4/10
570/570 [==============================] - 0s 55us/step - loss: 0.2126 - acc: 0.9596 - val\_loss: 0.7588 - val\_acc: 0.7456
Epoch 5/10
570/570 [==============================] - 0s 75us/step - loss: 0.1167 - acc: 0.9877 - val\_loss: 0.7298 - val\_acc: 0.7561
Epoch 6/10
570/570 [==============================] - 0s 63us/step - loss: 0.0722 - acc: 0.9912 - val\_loss: 0.7262 - val\_acc: 0.7719
Epoch 7/10
570/570 [==============================] - 0s 48us/step - loss: 0.0412 - acc: 0.9965 - val\_loss: 0.7356 - val\_acc: 0.7596
Epoch 8/10
570/570 [==============================] - 0s 79us/step - loss: 0.0254 - acc: 1.0000 - val\_loss: 0.7495 - val\_acc: 0.7632
Epoch 9/10
570/570 [==============================] - 0s 55us/step - loss: 0.0178 - acc: 1.0000 - val\_loss: 0.7622 - val\_acc: 0.7614
Epoch 10/10
570/570 [==============================] - 0s 55us/step - loss: 0.0126 - acc: 1.0000 - val\_loss: 0.7658 - val\_acc: 0.7632
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 432us/step - loss: 1.7754 - acc: 0.4246 - val\_loss: 1.1200 - val\_acc: 0.6211
Epoch 2/10
570/570 [==============================] - 0s 48us/step - loss: 0.8183 - acc: 0.7140 - val\_loss: 0.8544 - val\_acc: 0.7333
Epoch 3/10
570/570 [==============================] - 0s 56us/step - loss: 0.4437 - acc: 0.9105 - val\_loss: 0.7478 - val\_acc: 0.7456
Epoch 4/10
570/570 [==============================] - 0s 59us/step - loss: 0.2544 - acc: 0.9316 - val\_loss: 0.6776 - val\_acc: 0.7702
Epoch 5/10
570/570 [==============================] - 0s 59us/step - loss: 0.1400 - acc: 0.9719 - val\_loss: 0.6626 - val\_acc: 0.7877
Epoch 6/10
570/570 [==============================] - 0s 62us/step - loss: 0.0783 - acc: 0.9877 - val\_loss: 0.6804 - val\_acc: 0.7789
Epoch 7/10
570/570 [==============================] - 0s 51us/step - loss: 0.0427 - acc: 0.9965 - val\_loss: 0.6768 - val\_acc: 0.7842
Epoch 8/10
570/570 [==============================] - 0s 47us/step - loss: 0.0278 - acc: 1.0000 - val\_loss: 0.6857 - val\_acc: 0.7877
Epoch 9/10
570/570 [==============================] - 0s 47us/step - loss: 0.0180 - acc: 1.0000 - val\_loss: 0.7096 - val\_acc: 0.7719
Epoch 10/10
570/570 [==============================] - 0s 51us/step - loss: 0.0125 - acc: 1.0000 - val\_loss: 0.7026 - val\_acc: 0.7772
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 398us/step - loss: 1.8298 - acc: 0.4368 - val\_loss: 1.0430 - val\_acc: 0.6456
Epoch 2/10
570/570 [==============================] - 0s 39us/step - loss: 0.8003 - acc: 0.8053 - val\_loss: 0.8020 - val\_acc: 0.7404
Epoch 3/10
570/570 [==============================] - 0s 51us/step - loss: 0.4108 - acc: 0.9018 - val\_loss: 0.6977 - val\_acc: 0.7632
Epoch 4/10
570/570 [==============================] - 0s 102us/step - loss: 0.2311 - acc: 0.9298 - val\_loss: 0.6334 - val\_acc: 0.7754
Epoch 5/10
570/570 [==============================] - 0s 71us/step - loss: 0.1173 - acc: 0.9772 - val\_loss: 0.6751 - val\_acc: 0.7807
Epoch 6/10
570/570 [==============================] - 0s 64us/step - loss: 0.0643 - acc: 0.9930 - val\_loss: 0.6768 - val\_acc: 0.7877
Epoch 7/10
570/570 [==============================] - 0s 54us/step - loss: 0.0347 - acc: 1.0000 - val\_loss: 0.6902 - val\_acc: 0.7912
Epoch 8/10
570/570 [==============================] - 0s 57us/step - loss: 0.0206 - acc: 1.0000 - val\_loss: 0.7209 - val\_acc: 0.7930
Epoch 9/10
570/570 [==============================] - 0s 51us/step - loss: 0.0132 - acc: 1.0000 - val\_loss: 0.7472 - val\_acc: 0.7895
Epoch 10/10
570/570 [==============================] - 0s 62us/step - loss: 0.0095 - acc: 1.0000 - val\_loss: 0.7588 - val\_acc: 0.7860
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 483us/step - loss: 2.2324 - acc: 0.3561 - val\_loss: 1.2062 - val\_acc: 0.5175
Epoch 2/10
570/570 [==============================] - 0s 46us/step - loss: 0.8449 - acc: 0.6772 - val\_loss: 0.8731 - val\_acc: 0.6789
Epoch 3/10
570/570 [==============================] - 0s 53us/step - loss: 0.4990 - acc: 0.8421 - val\_loss: 0.8505 - val\_acc: 0.7000
Epoch 4/10
570/570 [==============================] - 0s 48us/step - loss: 0.2864 - acc: 0.9439 - val\_loss: 0.7829 - val\_acc: 0.7281
Epoch 5/10
570/570 [==============================] - 0s 52us/step - loss: 0.1511 - acc: 0.9632 - val\_loss: 0.8305 - val\_acc: 0.7211
Epoch 6/10
570/570 [==============================] - 0s 59us/step - loss: 0.0868 - acc: 0.9860 - val\_loss: 0.8247 - val\_acc: 0.7298
Epoch 7/10
570/570 [==============================] - 0s 51us/step - loss: 0.0480 - acc: 0.9965 - val\_loss: 0.8693 - val\_acc: 0.7351
Epoch 8/10
570/570 [==============================] - 0s 62us/step - loss: 0.0246 - acc: 1.0000 - val\_loss: 0.9087 - val\_acc: 0.7211
Epoch 9/10
570/570 [==============================] - 0s 58us/step - loss: 0.0163 - acc: 1.0000 - val\_loss: 0.9223 - val\_acc: 0.7263
Epoch 10/10
570/570 [==============================] - 0s 82us/step - loss: 0.0099 - acc: 1.0000 - val\_loss: 0.9167 - val\_acc: 0.7351
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 508us/step - loss: 1.9450 - acc: 0.3421 - val\_loss: 1.7318 - val\_acc: 0.3316
Epoch 2/10
570/570 [==============================] - 0s 46us/step - loss: 1.1963 - acc: 0.5702 - val\_loss: 1.5324 - val\_acc: 0.3895
Epoch 3/10
570/570 [==============================] - 0s 51us/step - loss: 0.9007 - acc: 0.7070 - val\_loss: 1.3448 - val\_acc: 0.4877
Epoch 4/10
570/570 [==============================] - 0s 35us/step - loss: 0.5927 - acc: 0.8158 - val\_loss: 1.2444 - val\_acc: 0.5368
Epoch 5/10
570/570 [==============================] - 0s 42us/step - loss: 0.3642 - acc: 0.9316 - val\_loss: 1.1061 - val\_acc: 0.5895
Epoch 6/10
570/570 [==============================] - 0s 40us/step - loss: 0.2084 - acc: 0.9596 - val\_loss: 1.0514 - val\_acc: 0.6263
Epoch 7/10
570/570 [==============================] - 0s 35us/step - loss: 0.1134 - acc: 0.9895 - val\_loss: 0.9989 - val\_acc: 0.6614
Epoch 8/10
570/570 [==============================] - 0s 38us/step - loss: 0.0670 - acc: 0.9947 - val\_loss: 0.9784 - val\_acc: 0.6912
Epoch 9/10
570/570 [==============================] - 0s 37us/step - loss: 0.0398 - acc: 1.0000 - val\_loss: 0.9710 - val\_acc: 0.6982
Epoch 10/10
570/570 [==============================] - 0s 46us/step - loss: 0.0255 - acc: 1.0000 - val\_loss: 0.9677 - val\_acc: 0.7035
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 365us/step - loss: 1.9493 - acc: 0.3807 - val\_loss: 1.2582 - val\_acc: 0.5158
Epoch 2/10
570/570 [==============================] - 0s 39us/step - loss: 0.9423 - acc: 0.6825 - val\_loss: 1.0057 - val\_acc: 0.6421
Epoch 3/10
570/570 [==============================] - 0s 43us/step - loss: 0.5678 - acc: 0.8421 - val\_loss: 0.9069 - val\_acc: 0.6807
Epoch 4/10
570/570 [==============================] - 0s 41us/step - loss: 0.2989 - acc: 0.9246 - val\_loss: 0.8692 - val\_acc: 0.7035
Epoch 5/10
570/570 [==============================] - 0s 38us/step - loss: 0.1483 - acc: 0.9807 - val\_loss: 0.8578 - val\_acc: 0.7140
Epoch 6/10
570/570 [==============================] - 0s 40us/step - loss: 0.0759 - acc: 0.9930 - val\_loss: 0.8456 - val\_acc: 0.7246
Epoch 7/10
570/570 [==============================] - 0s 36us/step - loss: 0.0407 - acc: 0.9982 - val\_loss: 0.8467 - val\_acc: 0.7263
Epoch 8/10
570/570 [==============================] - 0s 45us/step - loss: 0.0248 - acc: 1.0000 - val\_loss: 0.8365 - val\_acc: 0.7351
Epoch 9/10
570/570 [==============================] - 0s 44us/step - loss: 0.0159 - acc: 1.0000 - val\_loss: 0.8427 - val\_acc: 0.7298
Epoch 10/10
570/570 [==============================] - 0s 45us/step - loss: 0.0113 - acc: 1.0000 - val\_loss: 0.8470 - val\_acc: 0.7421
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 387us/step - loss: 2.4614 - acc: 0.3965 - val\_loss: 1.4814 - val\_acc: 0.4123
Epoch 2/10
570/570 [==============================] - 0s 41us/step - loss: 1.0770 - acc: 0.6895 - val\_loss: 1.1959 - val\_acc: 0.5649
Epoch 3/10
570/570 [==============================] - 0s 49us/step - loss: 0.6413 - acc: 0.8351 - val\_loss: 1.0752 - val\_acc: 0.6140
Epoch 4/10
570/570 [==============================] - 0s 49us/step - loss: 0.4009 - acc: 0.9070 - val\_loss: 0.9634 - val\_acc: 0.6754
Epoch 5/10
570/570 [==============================] - 0s 48us/step - loss: 0.2585 - acc: 0.9193 - val\_loss: 0.9708 - val\_acc: 0.6737
Epoch 6/10
570/570 [==============================] - 0s 49us/step - loss: 0.1391 - acc: 0.9737 - val\_loss: 0.9895 - val\_acc: 0.6807
Epoch 7/10
570/570 [==============================] - 0s 48us/step - loss: 0.0753 - acc: 0.9947 - val\_loss: 0.9621 - val\_acc: 0.7053
Epoch 8/10
570/570 [==============================] - 0s 56us/step - loss: 0.0434 - acc: 0.9947 - val\_loss: 0.9823 - val\_acc: 0.7158
Epoch 9/10
570/570 [==============================] - 0s 52us/step - loss: 0.0240 - acc: 1.0000 - val\_loss: 0.9943 - val\_acc: 0.7105
Epoch 10/10
570/570 [==============================] - 0s 49us/step - loss: 0.0157 - acc: 1.0000 - val\_loss: 0.9966 - val\_acc: 0.7140
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 387us/step - loss: 2.7022 - acc: 0.3509 - val\_loss: 1.3713 - val\_acc: 0.4632
Epoch 2/10
570/570 [==============================] - 0s 52us/step - loss: 1.0771 - acc: 0.6158 - val\_loss: 1.2398 - val\_acc: 0.5421
Epoch 3/10
570/570 [==============================] - 0s 54us/step - loss: 0.6682 - acc: 0.7912 - val\_loss: 1.1104 - val\_acc: 0.6123
Epoch 4/10
570/570 [==============================] - 0s 49us/step - loss: 0.4051 - acc: 0.8895 - val\_loss: 1.0834 - val\_acc: 0.6439
Epoch 5/10
570/570 [==============================] - 0s 56us/step - loss: 0.2041 - acc: 0.9667 - val\_loss: 1.1229 - val\_acc: 0.6614
Epoch 6/10
570/570 [==============================] - 0s 54us/step - loss: 0.1056 - acc: 0.9860 - val\_loss: 1.1471 - val\_acc: 0.6684
Epoch 7/10
570/570 [==============================] - 0s 55us/step - loss: 0.0492 - acc: 0.9982 - val\_loss: 1.1697 - val\_acc: 0.6702
Epoch 8/10
570/570 [==============================] - 0s 50us/step - loss: 0.0290 - acc: 0.9982 - val\_loss: 1.2178 - val\_acc: 0.6667
Epoch 9/10
570/570 [==============================] - 0s 55us/step - loss: 0.0159 - acc: 1.0000 - val\_loss: 1.2537 - val\_acc: 0.6649
Epoch 10/10
570/570 [==============================] - 0s 57us/step - loss: 0.0087 - acc: 1.0000 - val\_loss: 1.2517 - val\_acc: 0.6684
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 446us/step - loss: 2.8959 - acc: 0.3018 - val\_loss: 1.3449 - val\_acc: 0.4719
Epoch 2/10
570/570 [==============================] - 0s 59us/step - loss: 1.0104 - acc: 0.6053 - val\_loss: 1.1852 - val\_acc: 0.5789
Epoch 3/10
570/570 [==============================] - 0s 56us/step - loss: 0.7345 - acc: 0.7351 - val\_loss: 1.2627 - val\_acc: 0.5614
Epoch 4/10
570/570 [==============================] - 0s 52us/step - loss: 0.4059 - acc: 0.9333 - val\_loss: 1.2439 - val\_acc: 0.6000
Epoch 5/10
570/570 [==============================] - 0s 50us/step - loss: 0.1962 - acc: 0.9614 - val\_loss: 1.2936 - val\_acc: 0.5982
Epoch 6/10
570/570 [==============================] - 0s 54us/step - loss: 0.0926 - acc: 0.9860 - val\_loss: 1.3380 - val\_acc: 0.6035
Epoch 7/10
570/570 [==============================] - 0s 55us/step - loss: 0.0443 - acc: 0.9930 - val\_loss: 1.4190 - val\_acc: 0.6018
Epoch 8/10
570/570 [==============================] - 0s 59us/step - loss: 0.0213 - acc: 1.0000 - val\_loss: 1.4494 - val\_acc: 0.6123
Epoch 9/10
570/570 [==============================] - 0s 63us/step - loss: 0.0123 - acc: 1.0000 - val\_loss: 1.5153 - val\_acc: 0.6035
Epoch 10/10
570/570 [==============================] - 0s 58us/step - loss: 0.0077 - acc: 1.0000 - val\_loss: 1.5307 - val\_acc: 0.6158
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 368us/step - loss: 1.8770 - acc: 0.3649 - val\_loss: 2.1561 - val\_acc: 0.2018
Epoch 2/10
570/570 [==============================] - 0s 35us/step - loss: 1.0651 - acc: 0.5737 - val\_loss: 1.9608 - val\_acc: 0.3035
Epoch 3/10
570/570 [==============================] - 0s 38us/step - loss: 0.6934 - acc: 0.8070 - val\_loss: 1.8269 - val\_acc: 0.3596
Epoch 4/10
570/570 [==============================] - 0s 46us/step - loss: 0.4012 - acc: 0.9000 - val\_loss: 1.7630 - val\_acc: 0.4035
Epoch 5/10
570/570 [==============================] - 0s 47us/step - loss: 0.2100 - acc: 0.9684 - val\_loss: 1.6654 - val\_acc: 0.4667
Epoch 6/10
570/570 [==============================] - 0s 39us/step - loss: 0.1101 - acc: 0.9895 - val\_loss: 1.6493 - val\_acc: 0.4912
Epoch 7/10
570/570 [==============================] - 0s 44us/step - loss: 0.0539 - acc: 0.9982 - val\_loss: 1.5785 - val\_acc: 0.5333
Epoch 8/10
570/570 [==============================] - 0s 45us/step - loss: 0.0286 - acc: 1.0000 - val\_loss: 1.5687 - val\_acc: 0.5351
Epoch 9/10
570/570 [==============================] - 0s 37us/step - loss: 0.0177 - acc: 1.0000 - val\_loss: 1.5664 - val\_acc: 0.5439
Epoch 10/10
570/570 [==============================] - 0s 46us/step - loss: 0.0119 - acc: 1.0000 - val\_loss: 1.5834 - val\_acc: 0.5456
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 638us/step - loss: 2.1762 - acc: 0.3158 - val\_loss: 1.3310 - val\_acc: 0.4772
Epoch 2/10
570/570 [==============================] - 0s 46us/step - loss: 1.0503 - acc: 0.6070 - val\_loss: 1.0908 - val\_acc: 0.6105
Epoch 3/10
570/570 [==============================] - 0s 48us/step - loss: 0.6806 - acc: 0.8088 - val\_loss: 1.0266 - val\_acc: 0.6351
Epoch 4/10
570/570 [==============================] - 0s 50us/step - loss: 0.3719 - acc: 0.9088 - val\_loss: 0.9945 - val\_acc: 0.6544
Epoch 5/10
570/570 [==============================] - 0s 47us/step - loss: 0.1836 - acc: 0.9719 - val\_loss: 0.9782 - val\_acc: 0.6754
Epoch 6/10
570/570 [==============================] - 0s 46us/step - loss: 0.0773 - acc: 0.9947 - val\_loss: 1.0222 - val\_acc: 0.6579
Epoch 7/10
570/570 [==============================] - 0s 43us/step - loss: 0.0377 - acc: 1.0000 - val\_loss: 1.0402 - val\_acc: 0.6667
Epoch 8/10
570/570 [==============================] - 0s 41us/step - loss: 0.0203 - acc: 1.0000 - val\_loss: 1.0635 - val\_acc: 0.6684
Epoch 9/10
570/570 [==============================] - 0s 44us/step - loss: 0.0113 - acc: 1.0000 - val\_loss: 1.0993 - val\_acc: 0.6684
Epoch 10/10
570/570 [==============================] - 0s 46us/step - loss: 0.0085 - acc: 1.0000 - val\_loss: 1.1152 - val\_acc: 0.6649
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 432us/step - loss: 1.9143 - acc: 0.3789 - val\_loss: 1.1167 - val\_acc: 0.5754
Epoch 2/10
570/570 [==============================] - 0s 60us/step - loss: 0.8178 - acc: 0.7175 - val\_loss: 0.9301 - val\_acc: 0.6807
Epoch 3/10
570/570 [==============================] - 0s 54us/step - loss: 0.4119 - acc: 0.9351 - val\_loss: 0.8151 - val\_acc: 0.7281
Epoch 4/10
570/570 [==============================] - 0s 54us/step - loss: 0.1854 - acc: 0.9561 - val\_loss: 0.8858 - val\_acc: 0.7035
Epoch 5/10
570/570 [==============================] - 0s 41us/step - loss: 0.0694 - acc: 0.9930 - val\_loss: 0.8320 - val\_acc: 0.7421
Epoch 6/10
570/570 [==============================] - 0s 47us/step - loss: 0.0399 - acc: 0.9930 - val\_loss: 0.9149 - val\_acc: 0.7088
Epoch 7/10
570/570 [==============================] - 0s 47us/step - loss: 0.0180 - acc: 1.0000 - val\_loss: 0.9219 - val\_acc: 0.7193
Epoch 8/10
570/570 [==============================] - 0s 42us/step - loss: 0.0110 - acc: 1.0000 - val\_loss: 0.9255 - val\_acc: 0.7175
Epoch 9/10
570/570 [==============================] - 0s 147us/step - loss: 0.0059 - acc: 1.0000 - val\_loss: 0.9596 - val\_acc: 0.7105
Epoch 10/10
570/570 [==============================] - 0s 53us/step - loss: 0.0046 - acc: 1.0000 - val\_loss: 0.9670 - val\_acc: 0.7211
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 477us/step - loss: 2.5762 - acc: 0.3632 - val\_loss: 1.6685 - val\_acc: 0.3228
Epoch 2/10
570/570 [==============================] - 0s 46us/step - loss: 1.0014 - acc: 0.6456 - val\_loss: 1.5409 - val\_acc: 0.3842
Epoch 3/10
570/570 [==============================] - 0s 58us/step - loss: 0.6294 - acc: 0.8123 - val\_loss: 1.5791 - val\_acc: 0.4386
Epoch 4/10
570/570 [==============================] - 0s 54us/step - loss: 0.3459 - acc: 0.9035 - val\_loss: 1.4684 - val\_acc: 0.5246
Epoch 5/10
570/570 [==============================] - 0s 56us/step - loss: 0.1493 - acc: 0.9807 - val\_loss: 1.4700 - val\_acc: 0.5561
Epoch 6/10
570/570 [==============================] - 0s 55us/step - loss: 0.0600 - acc: 0.9965 - val\_loss: 1.4481 - val\_acc: 0.5825
Epoch 7/10
570/570 [==============================] - 0s 55us/step - loss: 0.0258 - acc: 1.0000 - val\_loss: 1.4447 - val\_acc: 0.6018
Epoch 8/10
570/570 [==============================] - 0s 53us/step - loss: 0.0112 - acc: 1.0000 - val\_loss: 1.4679 - val\_acc: 0.6140
Epoch 9/10
570/570 [==============================] - 0s 48us/step - loss: 0.0067 - acc: 1.0000 - val\_loss: 1.4793 - val\_acc: 0.6123
Epoch 10/10
570/570 [==============================] - 0s 50us/step - loss: 0.0042 - acc: 1.0000 - val\_loss: 1.4877 - val\_acc: 0.6140
Train on 570 samples, validate on 570 samples
Epoch 1/10
570/570 [==============================] - 0s 393us/step - loss: 2.7537 - acc: 0.3596 - val\_loss: 1.3580 - val\_acc: 0.4596
Epoch 2/10
570/570 [==============================] - 0s 48us/step - loss: 1.0996 - acc: 0.6246 - val\_loss: 1.2328 - val\_acc: 0.5386
Epoch 3/10
570/570 [==============================] - 0s 53us/step - loss: 0.6900 - acc: 0.7719 - val\_loss: 1.2491 - val\_acc: 0.5526
Epoch 4/10
570/570 [==============================] - 0s 57us/step - loss: 0.3710 - acc: 0.8930 - val\_loss: 1.2873 - val\_acc: 0.5684
Epoch 5/10
570/570 [==============================] - 0s 58us/step - loss: 0.1553 - acc: 0.9737 - val\_loss: 1.3395 - val\_acc: 0.5877
Epoch 6/10
570/570 [==============================] - 0s 54us/step - loss: 0.0640 - acc: 0.9912 - val\_loss: 1.4080 - val\_acc: 0.6053
Epoch 7/10
570/570 [==============================] - 0s 60us/step - loss: 0.0219 - acc: 1.0000 - val\_loss: 1.4398 - val\_acc: 0.6140
Epoch 8/10
570/570 [==============================] - 0s 63us/step - loss: 0.0132 - acc: 1.0000 - val\_loss: 1.5059 - val\_acc: 0.6140
Epoch 9/10
570/570 [==============================] - 0s 60us/step - loss: 0.0071 - acc: 1.0000 - val\_loss: 1.5633 - val\_acc: 0.6000
Epoch 10/10
570/570 [==============================] - 0s 57us/step - loss: 0.0046 - acc: 1.0000 - val\_loss: 1.5749 - val\_acc: 0.6070

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{} Determine the npc and nnode that provides the highest validation accuracy }
         \PY{c+c1}{\PYZsh{} TO DO }
         \PY{n}{highest\PYZus{}accuracy} \PY{o}{=} \PY{n}{result}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} 
         \PY{n}{opt\PYZus{}npc\PYZus{}index} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{n}{opt\PYZus{}nnode\PYZus{}index} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{npcs}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{nnodes}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{k}{if} \PY{n}{result}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{n}{highest\PYZus{}accuracy}\PY{p}{:}
                     \PY{n}{highest\PYZus{}accuracy} \PY{o}{=} \PY{n}{result}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}
                     \PY{n}{opt\PYZus{}npc\PYZus{}index} \PY{o}{=} \PY{n}{i}
                     \PY{n}{opt\PYZus{}nnode\PYZus{}index} \PY{o}{=} \PY{n}{j}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The best npc is }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{, and the best nnode is }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{npcs}\PY{p}{[}\PY{n}{opt\PYZus{}npc\PYZus{}index}\PY{p}{]}\PY{p}{,}\PY{n}{nnodes}\PY{p}{[}\PY{n}{opt\PYZus{}nnode\PYZus{}index}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The best validation accuracy is }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{highest\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The best npc is 50, and the best nnode is 250.
The best validation accuracy is 0.792982.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{c+c1}{\PYZsh{} Produce a contour plot of the accuracy using different nnode and npc combincations}
         \PY{c+c1}{\PYZsh{} TO DO}
         
         \PY{c+c1}{\PYZsh{} plt.contourf ...}
         \PY{n}{grid\PYZus{}x}\PY{p}{,} \PY{n}{grid\PYZus{}y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mgrid}\PY{p}{[}\PY{l+m+mi}{50}\PY{p}{:}\PY{l+m+mi}{250}\PY{p}{:}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{:}\PY{l+m+mi}{300}\PY{p}{:}\PY{l+m+mi}{50}\PY{p}{]} 
         \PY{n}{plt}\PY{o}{.}\PY{n}{contourf}\PY{p}{(}\PY{n}{grid\PYZus{}y}\PY{p}{,}\PY{n}{grid\PYZus{}x}\PY{p}{,}\PY{n}{result}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{nnodes}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{npcs}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}31}]:} Text(0,0.5,'npcs')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Now let us compare the PCA+NN with applying a CNN on the raw
image data
only.}\label{now-let-us-compare-the-pcann-with-applying-a-cnn-on-the-raw-image-data-only.}

Note that you should scale your image data to between 0 and 1. And you
should reshape your training and testing data according to image width
and height

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{c+c1}{\PYZsh{} Data preparation for input to CNN}
         \PY{c+c1}{\PYZsh{} TO DO}
         \PY{n}{Xtr\PYZus{}cnn} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{float32}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
         \PY{n}{Xts\PYZus{}cnn} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{float32}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
         \PY{n}{Xtr\PYZus{}cnn} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{Xtr\PYZus{}cnn}\PY{p}{,} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Xtr\PYZus{}cnn}\PY{p}{)}\PY{p}{,}\PY{n}{h}\PY{p}{,}\PY{n}{w}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n}{Xts\PYZus{}cnn} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{Xts\PYZus{}cnn}\PY{p}{,} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Xtr\PYZus{}cnn}\PY{p}{)}\PY{p}{,}\PY{n}{h}\PY{p}{,}\PY{n}{w}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{c+c1}{\PYZsh{} Set up a CNN model}
         \PY{c+c1}{\PYZsh{} You can use 2 conv2D layer, each with kernel size of 5x5, each followed by a pooling layer with strides of 2}
         \PY{c+c1}{\PYZsh{} For this part, let both conv2D layer generate 16 channels. }
         \PY{c+c1}{\PYZsh{} The Conv layer should be followed by a flatten layer and two dense layers. }
         \PY{c+c1}{\PYZsh{} The first dense layer should produce 200 outputs. }
         \PY{c+c1}{\PYZsh{} The last dense layer is the output layer with n\PYZus{}classes output using \PYZsq{}softmax\PYZsq{} activation.}
         \PY{c+c1}{\PYZsh{} Print model summary to verify it follows the desired structure and compile the model}
         
         \PY{c+c1}{\PYZsh{} TO DO }
         \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
                          \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{Xtr\PYZus{}cnn}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{,}
                          \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} 
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nout}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} TO DO}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv2d\_1 (Conv2D)            (None, 46, 33, 16)        416       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_1 (MaxPooling2 (None, 23, 16, 16)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_2 (Conv2D)            (None, 19, 12, 16)        6416      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_2 (MaxPooling2 (None, 9, 6, 16)          0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_1 (Flatten)          (None, 864)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_1 (Dense)              (None, 200)               173000    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_2 (Dense)              (None, 5)                 1005      
=================================================================
Total params: 180,837
Trainable params: 180,837
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
None

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{c+c1}{\PYZsh{} Fit the model using batch size=100, epochs = 40}
         \PY{c+c1}{\PYZsh{} Print the accuracy on the validation set}
         
         \PY{c+c1}{\PYZsh{} TO DO }
         \PY{n}{opt} \PY{o}{=} \PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{beta\PYZus{}1}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{,} \PY{n}{beta\PYZus{}2}\PY{o}{=}\PY{l+m+mf}{0.999}\PY{p}{,} \PY{n}{epsilon}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}08}\PY{p}{,} \PY{n}{decay}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{)} 
         \PY{c+c1}{\PYZsh{} Let\PYZsq{}s train the model using Adam}
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                \PY{n}{optimizer}\PY{o}{=}\PY{n}{opt}\PY{p}{,}\PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{hist\PYZus{}basic} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtr\PYZus{}cnn}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}\PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{40}\PY{p}{,}
                                \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{Xts\PYZus{}cnn}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,}\PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} 
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The accuracy on validation set is:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{hist\PYZus{}basic}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 570 samples, validate on 570 samples
Epoch 1/40
570/570 [==============================] - 2s 3ms/step - loss: 1.4842 - acc: 0.3877 - val\_loss: 1.3951 - val\_acc: 0.4649
Epoch 2/40
570/570 [==============================] - 1s 3ms/step - loss: 1.3941 - acc: 0.4649 - val\_loss: 1.3807 - val\_acc: 0.4649
Epoch 3/40
570/570 [==============================] - 2s 3ms/step - loss: 1.3714 - acc: 0.4649 - val\_loss: 1.3579 - val\_acc: 0.4649
Epoch 4/40
570/570 [==============================] - 1s 3ms/step - loss: 1.3443 - acc: 0.4649 - val\_loss: 1.3325 - val\_acc: 0.4649
Epoch 5/40
570/570 [==============================] - 1s 3ms/step - loss: 1.3016 - acc: 0.4912 - val\_loss: 1.2861 - val\_acc: 0.4684
Epoch 6/40
570/570 [==============================] - 2s 3ms/step - loss: 1.2390 - acc: 0.5000 - val\_loss: 1.2291 - val\_acc: 0.5404
Epoch 7/40
570/570 [==============================] - 2s 3ms/step - loss: 1.1645 - acc: 0.5579 - val\_loss: 1.1554 - val\_acc: 0.5579
Epoch 8/40
570/570 [==============================] - 1s 3ms/step - loss: 1.0714 - acc: 0.6123 - val\_loss: 1.0941 - val\_acc: 0.6105
Epoch 9/40
570/570 [==============================] - 1s 2ms/step - loss: 1.0132 - acc: 0.6404 - val\_loss: 1.0870 - val\_acc: 0.6246
Epoch 10/40
570/570 [==============================] - 1s 3ms/step - loss: 0.9418 - acc: 0.6561 - val\_loss: 0.9847 - val\_acc: 0.6491
Epoch 11/40
570/570 [==============================] - 1s 2ms/step - loss: 0.8758 - acc: 0.6842 - val\_loss: 0.9528 - val\_acc: 0.6509
Epoch 12/40
570/570 [==============================] - 1s 2ms/step - loss: 0.8227 - acc: 0.7158 - val\_loss: 0.9266 - val\_acc: 0.6526
Epoch 13/40
570/570 [==============================] - 1s 2ms/step - loss: 0.7655 - acc: 0.7386 - val\_loss: 0.8758 - val\_acc: 0.6737
Epoch 14/40
570/570 [==============================] - 1s 3ms/step - loss: 0.6990 - acc: 0.7632 - val\_loss: 0.8561 - val\_acc: 0.6912
Epoch 15/40
570/570 [==============================] - 1s 2ms/step - loss: 0.6613 - acc: 0.7860 - val\_loss: 0.8041 - val\_acc: 0.7140
Epoch 16/40
570/570 [==============================] - 1s 2ms/step - loss: 0.6094 - acc: 0.8105 - val\_loss: 0.8319 - val\_acc: 0.7105
Epoch 17/40
570/570 [==============================] - 1s 2ms/step - loss: 0.5817 - acc: 0.8263 - val\_loss: 0.7599 - val\_acc: 0.7228
Epoch 18/40
570/570 [==============================] - 1s 2ms/step - loss: 0.5340 - acc: 0.8456 - val\_loss: 0.7169 - val\_acc: 0.7614
Epoch 19/40
570/570 [==============================] - 2s 3ms/step - loss: 0.4920 - acc: 0.8544 - val\_loss: 0.6886 - val\_acc: 0.7684
Epoch 20/40
570/570 [==============================] - 1s 2ms/step - loss: 0.4560 - acc: 0.8632 - val\_loss: 0.6635 - val\_acc: 0.7825
Epoch 21/40
570/570 [==============================] - 2s 3ms/step - loss: 0.4495 - acc: 0.8579 - val\_loss: 0.7289 - val\_acc: 0.7456
Epoch 22/40
570/570 [==============================] - 1s 3ms/step - loss: 0.4474 - acc: 0.8491 - val\_loss: 0.7316 - val\_acc: 0.7456
Epoch 23/40
570/570 [==============================] - 1s 3ms/step - loss: 0.4532 - acc: 0.8544 - val\_loss: 0.6669 - val\_acc: 0.7614
Epoch 24/40
570/570 [==============================] - 1s 2ms/step - loss: 0.4010 - acc: 0.8807 - val\_loss: 0.6257 - val\_acc: 0.7930
Epoch 25/40
570/570 [==============================] - 1s 2ms/step - loss: 0.3669 - acc: 0.8965 - val\_loss: 0.6173 - val\_acc: 0.8000
Epoch 26/40
570/570 [==============================] - 1s 2ms/step - loss: 0.3661 - acc: 0.8825 - val\_loss: 0.6199 - val\_acc: 0.8018
Epoch 27/40
570/570 [==============================] - 2s 3ms/step - loss: 0.3955 - acc: 0.8719 - val\_loss: 0.7103 - val\_acc: 0.7667
Epoch 28/40
570/570 [==============================] - 1s 3ms/step - loss: 0.3784 - acc: 0.8754 - val\_loss: 0.5923 - val\_acc: 0.8211
Epoch 29/40
570/570 [==============================] - 1s 3ms/step - loss: 0.3359 - acc: 0.8965 - val\_loss: 0.6325 - val\_acc: 0.8070
Epoch 30/40
570/570 [==============================] - 1s 2ms/step - loss: 0.3253 - acc: 0.9105 - val\_loss: 0.6034 - val\_acc: 0.8158
Epoch 31/40
570/570 [==============================] - 1s 2ms/step - loss: 0.3294 - acc: 0.9000 - val\_loss: 0.7601 - val\_acc: 0.7386
Epoch 32/40
570/570 [==============================] - 1s 2ms/step - loss: 0.3716 - acc: 0.8772 - val\_loss: 0.7715 - val\_acc: 0.7421
Epoch 33/40
570/570 [==============================] - 1s 3ms/step - loss: 0.3400 - acc: 0.8912 - val\_loss: 0.6199 - val\_acc: 0.8140
Epoch 34/40
570/570 [==============================] - 1s 2ms/step - loss: 0.3221 - acc: 0.9035 - val\_loss: 0.5758 - val\_acc: 0.8211
Epoch 35/40
570/570 [==============================] - 1s 2ms/step - loss: 0.3049 - acc: 0.9018 - val\_loss: 0.6172 - val\_acc: 0.8211
Epoch 36/40
570/570 [==============================] - 1s 3ms/step - loss: 0.2915 - acc: 0.9158 - val\_loss: 0.5618 - val\_acc: 0.8228
Epoch 37/40
570/570 [==============================] - 1s 2ms/step - loss: 0.2561 - acc: 0.9316 - val\_loss: 0.5798 - val\_acc: 0.8333
Epoch 38/40
570/570 [==============================] - 1s 2ms/step - loss: 0.2444 - acc: 0.9368 - val\_loss: 0.6359 - val\_acc: 0.8088
Epoch 39/40
570/570 [==============================] - 1s 2ms/step - loss: 0.2327 - acc: 0.9421 - val\_loss: 0.6145 - val\_acc: 0.8088
Epoch 40/40
570/570 [==============================] - 1s 2ms/step - loss: 0.2227 - acc: 0.9404 - val\_loss: 0.5723 - val\_acc: 0.8351
The accuracy on validation set is:
[0.46491228070175439, 0.46491228070175439, 0.46491228070175439, 0.46491228070175439, 0.46842104928535327, 0.54035088315344693, 0.55789474855389509, 0.61052630972443966, 0.62456141124691877, 0.64912281015463047, 0.65087720176629849, 0.65263157769253377, 0.67368421951929724, 0.69122807393994246, 0.71403510633267853, 0.71052631474377814, 0.72280701419763393, 0.76140351044504262, 0.76842104761224039, 0.78245614286054643, 0.74561403613341481, 0.74561402985924163, 0.76140350521656508, 0.79298245697690728, 0.80000000460106024, 0.80175438366438212, 0.76666666959461416, 0.82105262969669546, 0.80701753863117154, 0.81578947368421051, 0.73859648746356632, 0.74210526441272939, 0.81403508207254238, 0.82105261610265368, 0.82105263178808652, 0.82280699830306203, 0.83333332705916019, 0.80877193233423061, 0.80877192292297095, 0.835087704031091]

    \end{Verbatim}

    How do the result compared with the PCA+NN method? (If you did right,
they should be similar, with PCA+NN being slightly better. If you used
more training data (e.g. 75\%) and you trained the CNN with more epochs,
CNN method may get better).

    A: PCA+NN method is slightly better with validation accuracy around
0.83, CNN method is with validation accuracy around 0.79.

    \subsection{Repeat the above using a small
dataset}\label{repeat-the-above-using-a-small-dataset}

Instead of using 50\% of the total data for training, let us assume you
have only 10\% of the total data for training. Repeat both the PCA+NN
and the CNN method, to see which one gives you better results.

Note that with only 10\% data for training, the range of the npc has to
be set to be below the total number of training samples.

For the CNN model, because you have small number of training samples,
you cannot train a network with a large number of parameters reliably.
Instead of producing 16 channels for each of the two conv2D layers,
configure the model to produce only 8 channels each.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} TO DO}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
             \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y}\PY{p}{,}\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{stratify} \PY{o}{=} \PY{n}{y}\PY{p}{,}\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{)} 
         \PY{n}{n\PYZus{}samples}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}
         \PY{n}{Xtr\PYZus{}mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{Xtr} \PY{o}{=} \PY{n}{X\PYZus{}train} \PY{o}{\PYZhy{}} \PY{n}{Xtr\PYZus{}mean}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{p}{]}
         \PY{n}{Utr}\PY{p}{,}\PY{n}{Str}\PY{p}{,}\PY{n}{Vtr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{svd}\PY{p}{(}\PY{n}{Xtr}\PY{p}{,} \PY{n}{full\PYZus{}matrices}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
         \PY{n}{nnodes} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{150}\PY{p}{,}\PY{l+m+mi}{200}\PY{p}{,}\PY{l+m+mi}{250}\PY{p}{]}
         \PY{n}{npcs} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+m+mi}{60}\PY{p}{,}\PY{l+m+mi}{70}\PY{p}{,}\PY{l+m+mi}{80}\PY{p}{,}\PY{l+m+mi}{90}\PY{p}{,}\PY{l+m+mi}{100}\PY{p}{]}
         \PY{n}{result} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{npcs}\PY{p}{)}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{nnodes}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{loss\PYZus{}hist} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{train\PYZus{}acc\PYZus{}hist} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{val\PYZus{}acc\PYZus{}hist} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{npc} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{npcs}\PY{p}{)}\PY{p}{:}
             \PY{k}{for} \PY{n}{j}\PY{p}{,}\PY{n}{nnode} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{nnodes}\PY{p}{)}\PY{p}{:}
                 \PY{n}{K}\PY{o}{.}\PY{n}{clear\PYZus{}session}\PY{p}{(}\PY{p}{)}
                 \PY{n}{eigenface} \PY{o}{=} \PY{n}{Vtr}\PY{p}{[}\PY{p}{:}\PY{n}{npc}\PY{p}{,}\PY{p}{:}\PY{p}{]}
                 \PY{n}{Xtr\PYZus{}pca} \PY{o}{=} \PY{n}{Xtr}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{eigenface}\PY{o}{.}\PY{n}{T}\PY{p}{)}
                 \PY{n}{Xtr\PYZus{}pca\PYZus{}s} \PY{o}{=} \PY{n}{Xtr\PYZus{}pca} \PY{o}{/} \PY{n}{Str}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{n}{npc}\PY{p}{]} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{p}{)} 
                 \PY{n}{Xts} \PY{o}{=} \PY{n}{X\PYZus{}test} \PY{o}{\PYZhy{}} \PY{n}{Xtr\PYZus{}mean}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{p}{]}
                 \PY{n}{Xts\PYZus{}pca} \PY{o}{=} \PY{n}{Xts}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{eigenface}\PY{o}{.}\PY{n}{T}\PY{p}{)}
                 \PY{n}{Xts\PYZus{}pca\PYZus{}s} \PY{o}{=} \PY{n}{Xts\PYZus{}pca} \PY{o}{/} \PY{n}{Str}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{n}{npc}\PY{p}{]} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{p}{)}
                 \PY{n}{nin} \PY{o}{=} \PY{n}{Xtr\PYZus{}pca}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{} dimension of input data}
                 \PY{n}{nh} \PY{o}{=} \PY{n}{nnode} \PY{c+c1}{\PYZsh{} number of hidden units}
                 \PY{n}{nout} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
                 \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nh}\PY{p}{,} \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{n}{nin}\PY{p}{,}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hidden}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} 
                 \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nout}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{output}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                 \PY{n}{opt} \PY{o}{=} \PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{beta\PYZus{}1}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{,} \PY{n}{beta\PYZus{}2}\PY{o}{=}\PY{l+m+mf}{0.999}\PY{p}{,} \PY{n}{epsilon}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}08}\PY{p}{,} \PY{n}{decay}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{)}
                 \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{n}{opt}\PY{p}{,}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                               \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
                 \PY{n}{hist} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtr\PYZus{}pca\PYZus{}s}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
                                  \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{Xts\PYZus{}pca\PYZus{}s}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
                 \PY{n}{result}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{hist}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{highest\PYZus{}accuracy} \PY{o}{=} \PY{n}{result}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} 
         \PY{n}{opt\PYZus{}npc\PYZus{}index} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{n}{opt\PYZus{}nnode\PYZus{}index} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{npcs}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{nnodes}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{k}{if} \PY{n}{result}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{n}{highest\PYZus{}accuracy}\PY{p}{:}
                     \PY{n}{highest\PYZus{}accuracy} \PY{o}{=} \PY{n}{result}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]}
                     \PY{n}{opt\PYZus{}npc\PYZus{}index} \PY{o}{=} \PY{n}{i}
                     \PY{n}{opt\PYZus{}nnode\PYZus{}index} \PY{o}{=} \PY{n}{j}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The best npc is }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{, and the best nnode is }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{npcs}\PY{p}{[}\PY{n}{opt\PYZus{}npc\PYZus{}index}\PY{p}{]}\PY{p}{,}\PY{n}{nnodes}\PY{p}{[}\PY{n}{opt\PYZus{}nnode\PYZus{}index}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The best validation accuracy is }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{highest\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.6904 - acc: 0.2982 - val\_loss: 1.4509 - val\_acc: 0.4172
Epoch 2/10
114/114 [==============================] - 0s 198us/step - loss: 1.2262 - acc: 0.5088 - val\_loss: 1.3349 - val\_acc: 0.4951
Epoch 3/10
114/114 [==============================] - 0s 205us/step - loss: 0.9618 - acc: 0.7018 - val\_loss: 1.2358 - val\_acc: 0.5585
Epoch 4/10
114/114 [==============================] - 0s 151us/step - loss: 0.7583 - acc: 0.7982 - val\_loss: 1.1481 - val\_acc: 0.5926
Epoch 5/10
114/114 [==============================] - 0s 205us/step - loss: 0.6010 - acc: 0.8684 - val\_loss: 1.0799 - val\_acc: 0.6140
Epoch 6/10
114/114 [==============================] - 0s 179us/step - loss: 0.4785 - acc: 0.9211 - val\_loss: 1.0265 - val\_acc: 0.6287
Epoch 7/10
114/114 [==============================] - 0s 158us/step - loss: 0.3843 - acc: 0.9298 - val\_loss: 0.9814 - val\_acc: 0.6472
Epoch 8/10
114/114 [==============================] - 0s 180us/step - loss: 0.3067 - acc: 0.9386 - val\_loss: 0.9418 - val\_acc: 0.6628
Epoch 9/10
114/114 [==============================] - 0s 173us/step - loss: 0.2408 - acc: 0.9737 - val\_loss: 0.9070 - val\_acc: 0.6784
Epoch 10/10
114/114 [==============================] - 0s 156us/step - loss: 0.1897 - acc: 0.9737 - val\_loss: 0.8809 - val\_acc: 0.6969
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.6233 - acc: 0.2719 - val\_loss: 1.2955 - val\_acc: 0.5244
Epoch 2/10
114/114 [==============================] - 0s 158us/step - loss: 0.9601 - acc: 0.7018 - val\_loss: 1.1209 - val\_acc: 0.6062
Epoch 3/10
114/114 [==============================] - 0s 177us/step - loss: 0.6531 - acc: 0.8070 - val\_loss: 1.0227 - val\_acc: 0.6452
Epoch 4/10
114/114 [==============================] - 0s 195us/step - loss: 0.4570 - acc: 0.9035 - val\_loss: 0.9579 - val\_acc: 0.6637
Epoch 5/10
114/114 [==============================] - 0s 156us/step - loss: 0.3270 - acc: 0.9561 - val\_loss: 0.9155 - val\_acc: 0.6715
Epoch 6/10
114/114 [==============================] - 0s 163us/step - loss: 0.2350 - acc: 0.9649 - val\_loss: 0.8814 - val\_acc: 0.6852
Epoch 7/10
114/114 [==============================] - 0s 181us/step - loss: 0.1646 - acc: 0.9912 - val\_loss: 0.8553 - val\_acc: 0.6930
Epoch 8/10
114/114 [==============================] - 0s 154us/step - loss: 0.1132 - acc: 1.0000 - val\_loss: 0.8368 - val\_acc: 0.7057
Epoch 9/10
114/114 [==============================] - 0s 185us/step - loss: 0.0778 - acc: 1.0000 - val\_loss: 0.8281 - val\_acc: 0.7154
Epoch 10/10
114/114 [==============================] - 0s 182us/step - loss: 0.0537 - acc: 1.0000 - val\_loss: 0.8269 - val\_acc: 0.7164
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.7323 - acc: 0.2368 - val\_loss: 1.2536 - val\_acc: 0.5292
Epoch 2/10
114/114 [==============================] - 0s 161us/step - loss: 0.9330 - acc: 0.6579 - val\_loss: 1.0939 - val\_acc: 0.6072
Epoch 3/10
114/114 [==============================] - 0s 205us/step - loss: 0.5822 - acc: 0.8509 - val\_loss: 1.0068 - val\_acc: 0.6491
Epoch 4/10
114/114 [==============================] - 0s 191us/step - loss: 0.3773 - acc: 0.9474 - val\_loss: 0.9522 - val\_acc: 0.6686
Epoch 5/10
114/114 [==============================] - 0s 190us/step - loss: 0.2443 - acc: 0.9912 - val\_loss: 0.9220 - val\_acc: 0.6725
Epoch 6/10
114/114 [==============================] - 0s 387us/step - loss: 0.1591 - acc: 0.9912 - val\_loss: 0.9080 - val\_acc: 0.6842
Epoch 7/10
114/114 [==============================] - 0s 321us/step - loss: 0.0999 - acc: 1.0000 - val\_loss: 0.9073 - val\_acc: 0.6862
Epoch 8/10
114/114 [==============================] - 0s 271us/step - loss: 0.0641 - acc: 1.0000 - val\_loss: 0.9189 - val\_acc: 0.6901
Epoch 9/10
114/114 [==============================] - 0s 251us/step - loss: 0.0425 - acc: 1.0000 - val\_loss: 0.9387 - val\_acc: 0.6949
Epoch 10/10
114/114 [==============================] - 0s 214us/step - loss: 0.0293 - acc: 1.0000 - val\_loss: 0.9608 - val\_acc: 0.6940
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.6380 - acc: 0.2632 - val\_loss: 1.1900 - val\_acc: 0.5604
Epoch 2/10
114/114 [==============================] - 0s 161us/step - loss: 0.7516 - acc: 0.7544 - val\_loss: 1.0559 - val\_acc: 0.6014
Epoch 3/10
114/114 [==============================] - 0s 173us/step - loss: 0.4446 - acc: 0.8860 - val\_loss: 0.9635 - val\_acc: 0.6530
Epoch 4/10
114/114 [==============================] - 0s 158us/step - loss: 0.2516 - acc: 0.9561 - val\_loss: 0.9214 - val\_acc: 0.6657
Epoch 5/10
114/114 [==============================] - 0s 186us/step - loss: 0.1564 - acc: 0.9825 - val\_loss: 0.9056 - val\_acc: 0.6832
Epoch 6/10
114/114 [==============================] - 0s 170us/step - loss: 0.1081 - acc: 0.9825 - val\_loss: 0.9017 - val\_acc: 0.6988
Epoch 7/10
114/114 [==============================] - 0s 208us/step - loss: 0.0624 - acc: 0.9825 - val\_loss: 0.9077 - val\_acc: 0.7047
Epoch 8/10
114/114 [==============================] - 0s 193us/step - loss: 0.0377 - acc: 0.9912 - val\_loss: 0.9260 - val\_acc: 0.7018
Epoch 9/10
114/114 [==============================] - 0s 294us/step - loss: 0.0252 - acc: 1.0000 - val\_loss: 0.9514 - val\_acc: 0.7027
Epoch 10/10
114/114 [==============================] - 0s 306us/step - loss: 0.0182 - acc: 1.0000 - val\_loss: 0.9800 - val\_acc: 0.7076
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.6684 - acc: 0.2018 - val\_loss: 1.1165 - val\_acc: 0.5994
Epoch 2/10
114/114 [==============================] - 0s 179us/step - loss: 0.7468 - acc: 0.7281 - val\_loss: 0.9484 - val\_acc: 0.6579
Epoch 3/10
114/114 [==============================] - 0s 222us/step - loss: 0.4239 - acc: 0.8947 - val\_loss: 0.8517 - val\_acc: 0.7164
Epoch 4/10
114/114 [==============================] - 0s 186us/step - loss: 0.2361 - acc: 0.9561 - val\_loss: 0.8161 - val\_acc: 0.7349
Epoch 5/10
114/114 [==============================] - 0s 189us/step - loss: 0.1378 - acc: 0.9737 - val\_loss: 0.8251 - val\_acc: 0.7251
Epoch 6/10
114/114 [==============================] - 0s 162us/step - loss: 0.0802 - acc: 1.0000 - val\_loss: 0.8572 - val\_acc: 0.7164
Epoch 7/10
114/114 [==============================] - 0s 186us/step - loss: 0.0545 - acc: 1.0000 - val\_loss: 0.8964 - val\_acc: 0.7105
Epoch 8/10
114/114 [==============================] - 0s 216us/step - loss: 0.0354 - acc: 1.0000 - val\_loss: 0.9315 - val\_acc: 0.7115
Epoch 9/10
114/114 [==============================] - 0s 200us/step - loss: 0.0240 - acc: 1.0000 - val\_loss: 0.9639 - val\_acc: 0.7086
Epoch 10/10
114/114 [==============================] - 0s 202us/step - loss: 0.0161 - acc: 1.0000 - val\_loss: 0.9942 - val\_acc: 0.7125
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 2.0253 - acc: 0.2456 - val\_loss: 1.6149 - val\_acc: 0.3363
Epoch 2/10
114/114 [==============================] - 0s 195us/step - loss: 1.2639 - acc: 0.4737 - val\_loss: 1.3992 - val\_acc: 0.4561
Epoch 3/10
114/114 [==============================] - 0s 249us/step - loss: 0.8926 - acc: 0.6930 - val\_loss: 1.2850 - val\_acc: 0.5127
Epoch 4/10
114/114 [==============================] - 0s 183us/step - loss: 0.6819 - acc: 0.7895 - val\_loss: 1.2100 - val\_acc: 0.5487
Epoch 5/10
114/114 [==============================] - 0s 235us/step - loss: 0.5304 - acc: 0.8596 - val\_loss: 1.1472 - val\_acc: 0.5887
Epoch 6/10
114/114 [==============================] - 0s 198us/step - loss: 0.4093 - acc: 0.8947 - val\_loss: 1.0912 - val\_acc: 0.6092
Epoch 7/10
114/114 [==============================] - 0s 248us/step - loss: 0.3062 - acc: 0.9211 - val\_loss: 1.0446 - val\_acc: 0.6335
Epoch 8/10
114/114 [==============================] - 0s 274us/step - loss: 0.2269 - acc: 0.9649 - val\_loss: 1.0078 - val\_acc: 0.6462
Epoch 9/10
114/114 [==============================] - 0s 208us/step - loss: 0.1705 - acc: 0.9912 - val\_loss: 0.9801 - val\_acc: 0.6530
Epoch 10/10
114/114 [==============================] - 0s 157us/step - loss: 0.1264 - acc: 1.0000 - val\_loss: 0.9598 - val\_acc: 0.6715
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.9153 - acc: 0.1930 - val\_loss: 1.3261 - val\_acc: 0.5136
Epoch 2/10
114/114 [==============================] - 0s 159us/step - loss: 1.0128 - acc: 0.6491 - val\_loss: 1.1451 - val\_acc: 0.5712
Epoch 3/10
114/114 [==============================] - 0s 184us/step - loss: 0.6759 - acc: 0.7632 - val\_loss: 1.0528 - val\_acc: 0.6082
Epoch 4/10
114/114 [==============================] - 0s 188us/step - loss: 0.4772 - acc: 0.8333 - val\_loss: 0.9669 - val\_acc: 0.6394
Epoch 5/10
114/114 [==============================] - 0s 205us/step - loss: 0.3117 - acc: 0.9561 - val\_loss: 0.8927 - val\_acc: 0.6920
Epoch 6/10
114/114 [==============================] - 0s 188us/step - loss: 0.1990 - acc: 0.9825 - val\_loss: 0.8485 - val\_acc: 0.6998
Epoch 7/10
114/114 [==============================] - 0s 207us/step - loss: 0.1256 - acc: 0.9912 - val\_loss: 0.8255 - val\_acc: 0.7105
Epoch 8/10
114/114 [==============================] - 0s 159us/step - loss: 0.0857 - acc: 0.9912 - val\_loss: 0.8162 - val\_acc: 0.7154
Epoch 9/10
114/114 [==============================] - 0s 160us/step - loss: 0.0591 - acc: 1.0000 - val\_loss: 0.8133 - val\_acc: 0.7173
Epoch 10/10
114/114 [==============================] - 0s 210us/step - loss: 0.0422 - acc: 1.0000 - val\_loss: 0.8144 - val\_acc: 0.7173
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 2.1994 - acc: 0.1053 - val\_loss: 1.3580 - val\_acc: 0.4873
Epoch 2/10
114/114 [==============================] - 0s 160us/step - loss: 0.9814 - acc: 0.7807 - val\_loss: 1.1429 - val\_acc: 0.5780
Epoch 3/10
114/114 [==============================] - 0s 210us/step - loss: 0.6070 - acc: 0.7895 - val\_loss: 1.0959 - val\_acc: 0.5799
Epoch 4/10
114/114 [==============================] - 0s 246us/step - loss: 0.4192 - acc: 0.8333 - val\_loss: 1.0050 - val\_acc: 0.6296
Epoch 5/10
114/114 [==============================] - 0s 224us/step - loss: 0.2615 - acc: 0.9386 - val\_loss: 0.9206 - val\_acc: 0.6657
Epoch 6/10
114/114 [==============================] - 0s 384us/step - loss: 0.1548 - acc: 0.9737 - val\_loss: 0.8641 - val\_acc: 0.6969
Epoch 7/10
114/114 [==============================] - 0s 269us/step - loss: 0.0985 - acc: 0.9912 - val\_loss: 0.8358 - val\_acc: 0.7105
Epoch 8/10
114/114 [==============================] - 0s 278us/step - loss: 0.0662 - acc: 0.9912 - val\_loss: 0.8281 - val\_acc: 0.7222
Epoch 9/10
114/114 [==============================] - 0s 271us/step - loss: 0.0482 - acc: 1.0000 - val\_loss: 0.8319 - val\_acc: 0.7222
Epoch 10/10
114/114 [==============================] - 0s 312us/step - loss: 0.0354 - acc: 1.0000 - val\_loss: 0.8424 - val\_acc: 0.7193
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.6396 - acc: 0.3070 - val\_loss: 1.2194 - val\_acc: 0.5351
Epoch 2/10
114/114 [==============================] - 0s 187us/step - loss: 0.7322 - acc: 0.7281 - val\_loss: 1.0808 - val\_acc: 0.6150
Epoch 3/10
114/114 [==============================] - 0s 199us/step - loss: 0.4317 - acc: 0.8772 - val\_loss: 0.9725 - val\_acc: 0.6618
Epoch 4/10
114/114 [==============================] - 0s 197us/step - loss: 0.2352 - acc: 0.9737 - val\_loss: 0.8935 - val\_acc: 0.6881
Epoch 5/10
114/114 [==============================] - 0s 210us/step - loss: 0.1195 - acc: 0.9912 - val\_loss: 0.8495 - val\_acc: 0.6998
Epoch 6/10
114/114 [==============================] - 0s 197us/step - loss: 0.0631 - acc: 1.0000 - val\_loss: 0.8405 - val\_acc: 0.7076
Epoch 7/10
114/114 [==============================] - 0s 191us/step - loss: 0.0370 - acc: 1.0000 - val\_loss: 0.8519 - val\_acc: 0.7300
Epoch 8/10
114/114 [==============================] - 0s 204us/step - loss: 0.0235 - acc: 1.0000 - val\_loss: 0.8692 - val\_acc: 0.7300
Epoch 9/10
114/114 [==============================] - 0s 228us/step - loss: 0.0162 - acc: 1.0000 - val\_loss: 0.8879 - val\_acc: 0.7329
Epoch 10/10
114/114 [==============================] - 0s 231us/step - loss: 0.0110 - acc: 1.0000 - val\_loss: 0.9071 - val\_acc: 0.7349
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.6465 - acc: 0.2982 - val\_loss: 1.1589 - val\_acc: 0.5517
Epoch 2/10
114/114 [==============================] - 0s 268us/step - loss: 0.6836 - acc: 0.6930 - val\_loss: 1.0296 - val\_acc: 0.6238
Epoch 3/10
114/114 [==============================] - 0s 245us/step - loss: 0.3843 - acc: 0.9035 - val\_loss: 0.9563 - val\_acc: 0.6618
Epoch 4/10
114/114 [==============================] - 0s 297us/step - loss: 0.2022 - acc: 0.9737 - val\_loss: 0.9034 - val\_acc: 0.6735
Epoch 5/10
114/114 [==============================] - 0s 253us/step - loss: 0.1072 - acc: 0.9912 - val\_loss: 0.8723 - val\_acc: 0.6930
Epoch 6/10
114/114 [==============================] - 0s 218us/step - loss: 0.0609 - acc: 0.9912 - val\_loss: 0.8594 - val\_acc: 0.7076
Epoch 7/10
114/114 [==============================] - 0s 210us/step - loss: 0.0347 - acc: 1.0000 - val\_loss: 0.8594 - val\_acc: 0.7144
Epoch 8/10
114/114 [==============================] - 0s 254us/step - loss: 0.0212 - acc: 1.0000 - val\_loss: 0.8673 - val\_acc: 0.7212
Epoch 9/10
114/114 [==============================] - 0s 260us/step - loss: 0.0135 - acc: 1.0000 - val\_loss: 0.8801 - val\_acc: 0.7261
Epoch 10/10
114/114 [==============================] - 0s 175us/step - loss: 0.0097 - acc: 1.0000 - val\_loss: 0.8953 - val\_acc: 0.7173
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.7950 - acc: 0.2544 - val\_loss: 1.4890 - val\_acc: 0.4279
Epoch 2/10
114/114 [==============================] - 0s 200us/step - loss: 1.1355 - acc: 0.5614 - val\_loss: 1.3397 - val\_acc: 0.5244
Epoch 3/10
114/114 [==============================] - 0s 166us/step - loss: 0.8220 - acc: 0.7368 - val\_loss: 1.2564 - val\_acc: 0.5565
Epoch 4/10
114/114 [==============================] - 0s 152us/step - loss: 0.6162 - acc: 0.8158 - val\_loss: 1.1838 - val\_acc: 0.5799
Epoch 5/10
114/114 [==============================] - 0s 140us/step - loss: 0.4637 - acc: 0.8860 - val\_loss: 1.1122 - val\_acc: 0.6101
Epoch 6/10
114/114 [==============================] - 0s 165us/step - loss: 0.3420 - acc: 0.9298 - val\_loss: 1.0517 - val\_acc: 0.6257
Epoch 7/10
114/114 [==============================] - 0s 182us/step - loss: 0.2490 - acc: 0.9825 - val\_loss: 1.0073 - val\_acc: 0.6472
Epoch 8/10
114/114 [==============================] - 0s 174us/step - loss: 0.1837 - acc: 0.9912 - val\_loss: 0.9736 - val\_acc: 0.6589
Epoch 9/10
114/114 [==============================] - 0s 151us/step - loss: 0.1387 - acc: 1.0000 - val\_loss: 0.9480 - val\_acc: 0.6725
Epoch 10/10
114/114 [==============================] - 0s 224us/step - loss: 0.1055 - acc: 1.0000 - val\_loss: 0.9305 - val\_acc: 0.6823
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 4ms/step - loss: 1.6452 - acc: 0.2719 - val\_loss: 1.3059 - val\_acc: 0.4971
Epoch 2/10
114/114 [==============================] - 0s 180us/step - loss: 0.8722 - acc: 0.6667 - val\_loss: 1.1665 - val\_acc: 0.5536
Epoch 3/10
114/114 [==============================] - 0s 244us/step - loss: 0.5582 - acc: 0.8158 - val\_loss: 1.0902 - val\_acc: 0.5653
Epoch 4/10
114/114 [==============================] - 0s 200us/step - loss: 0.3641 - acc: 0.8772 - val\_loss: 1.0301 - val\_acc: 0.6033
Epoch 5/10
114/114 [==============================] - 0s 191us/step - loss: 0.2249 - acc: 0.9825 - val\_loss: 0.9836 - val\_acc: 0.6218
Epoch 6/10
114/114 [==============================] - 0s 196us/step - loss: 0.1385 - acc: 1.0000 - val\_loss: 0.9520 - val\_acc: 0.6355
Epoch 7/10
114/114 [==============================] - 0s 169us/step - loss: 0.0810 - acc: 1.0000 - val\_loss: 0.9328 - val\_acc: 0.6530
Epoch 8/10
114/114 [==============================] - 0s 196us/step - loss: 0.0498 - acc: 1.0000 - val\_loss: 0.9210 - val\_acc: 0.6667
Epoch 9/10
114/114 [==============================] - 0s 143us/step - loss: 0.0324 - acc: 1.0000 - val\_loss: 0.9172 - val\_acc: 0.6764
Epoch 10/10
114/114 [==============================] - 0s 185us/step - loss: 0.0220 - acc: 1.0000 - val\_loss: 0.9207 - val\_acc: 0.6842
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.8350 - acc: 0.1930 - val\_loss: 1.2804 - val\_acc: 0.5107
Epoch 2/10
114/114 [==============================] - 0s 170us/step - loss: 0.8478 - acc: 0.7105 - val\_loss: 1.1431 - val\_acc: 0.5673
Epoch 3/10
114/114 [==============================] - 0s 179us/step - loss: 0.5193 - acc: 0.8509 - val\_loss: 1.0249 - val\_acc: 0.6238
Epoch 4/10
114/114 [==============================] - 0s 183us/step - loss: 0.3003 - acc: 0.9298 - val\_loss: 0.9452 - val\_acc: 0.6589
Epoch 5/10
114/114 [==============================] - 0s 182us/step - loss: 0.1767 - acc: 0.9825 - val\_loss: 0.8972 - val\_acc: 0.6832
Epoch 6/10
114/114 [==============================] - 0s 220us/step - loss: 0.1097 - acc: 0.9912 - val\_loss: 0.8763 - val\_acc: 0.7057
Epoch 7/10
114/114 [==============================] - 0s 205us/step - loss: 0.0717 - acc: 0.9912 - val\_loss: 0.8719 - val\_acc: 0.7057
Epoch 8/10
114/114 [==============================] - 0s 203us/step - loss: 0.0486 - acc: 1.0000 - val\_loss: 0.8771 - val\_acc: 0.7105
Epoch 9/10
114/114 [==============================] - 0s 247us/step - loss: 0.0339 - acc: 1.0000 - val\_loss: 0.8855 - val\_acc: 0.7086
Epoch 10/10
114/114 [==============================] - 0s 262us/step - loss: 0.0245 - acc: 1.0000 - val\_loss: 0.8965 - val\_acc: 0.7066
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.8324 - acc: 0.1667 - val\_loss: 1.2364 - val\_acc: 0.5253
Epoch 2/10
114/114 [==============================] - 0s 231us/step - loss: 0.7643 - acc: 0.6842 - val\_loss: 1.1102 - val\_acc: 0.5789
Epoch 3/10
114/114 [==============================] - 0s 209us/step - loss: 0.4110 - acc: 0.8772 - val\_loss: 0.9704 - val\_acc: 0.6550
Epoch 4/10
114/114 [==============================] - 0s 239us/step - loss: 0.2029 - acc: 0.9825 - val\_loss: 0.9132 - val\_acc: 0.6657
Epoch 5/10
114/114 [==============================] - 0s 236us/step - loss: 0.1121 - acc: 1.0000 - val\_loss: 0.9140 - val\_acc: 0.6637
Epoch 6/10
114/114 [==============================] - 0s 349us/step - loss: 0.0677 - acc: 1.0000 - val\_loss: 0.9335 - val\_acc: 0.6696
Epoch 7/10
114/114 [==============================] - 0s 261us/step - loss: 0.0426 - acc: 1.0000 - val\_loss: 0.9497 - val\_acc: 0.6774
Epoch 8/10
114/114 [==============================] - 0s 286us/step - loss: 0.0270 - acc: 1.0000 - val\_loss: 0.9610 - val\_acc: 0.6842
Epoch 9/10
114/114 [==============================] - 0s 285us/step - loss: 0.0174 - acc: 1.0000 - val\_loss: 0.9686 - val\_acc: 0.6862
Epoch 10/10
114/114 [==============================] - 0s 214us/step - loss: 0.0112 - acc: 1.0000 - val\_loss: 0.9758 - val\_acc: 0.6881
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.7138 - acc: 0.2982 - val\_loss: 1.1933 - val\_acc: 0.5351
Epoch 2/10
114/114 [==============================] - 0s 167us/step - loss: 0.6811 - acc: 0.7105 - val\_loss: 1.0699 - val\_acc: 0.5809
Epoch 3/10
114/114 [==============================] - 0s 194us/step - loss: 0.3587 - acc: 0.8947 - val\_loss: 0.9634 - val\_acc: 0.6462
Epoch 4/10
114/114 [==============================] - 0s 219us/step - loss: 0.1739 - acc: 0.9825 - val\_loss: 0.8989 - val\_acc: 0.6930
Epoch 5/10
114/114 [==============================] - 0s 219us/step - loss: 0.0853 - acc: 1.0000 - val\_loss: 0.8943 - val\_acc: 0.6949
Epoch 6/10
114/114 [==============================] - 0s 239us/step - loss: 0.0494 - acc: 1.0000 - val\_loss: 0.9155 - val\_acc: 0.6901
Epoch 7/10
114/114 [==============================] - 0s 193us/step - loss: 0.0303 - acc: 1.0000 - val\_loss: 0.9455 - val\_acc: 0.6871
Epoch 8/10
114/114 [==============================] - 0s 243us/step - loss: 0.0186 - acc: 1.0000 - val\_loss: 0.9802 - val\_acc: 0.6852
Epoch 9/10
114/114 [==============================] - 0s 194us/step - loss: 0.0113 - acc: 1.0000 - val\_loss: 1.0159 - val\_acc: 0.6891
Epoch 10/10
114/114 [==============================] - 0s 254us/step - loss: 0.0071 - acc: 1.0000 - val\_loss: 1.0509 - val\_acc: 0.6930
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 3ms/step - loss: 1.8343 - acc: 0.1842 - val\_loss: 1.4286 - val\_acc: 0.4103
Epoch 2/10
114/114 [==============================] - 0s 181us/step - loss: 1.1308 - acc: 0.6053 - val\_loss: 1.2563 - val\_acc: 0.5185
Epoch 3/10
114/114 [==============================] - 0s 169us/step - loss: 0.8028 - acc: 0.7632 - val\_loss: 1.1614 - val\_acc: 0.5565
Epoch 4/10
114/114 [==============================] - 0s 170us/step - loss: 0.5921 - acc: 0.8596 - val\_loss: 1.0916 - val\_acc: 0.5838
Epoch 5/10
114/114 [==============================] - 0s 186us/step - loss: 0.4357 - acc: 0.9211 - val\_loss: 1.0385 - val\_acc: 0.6179
Epoch 6/10
114/114 [==============================] - 0s 143us/step - loss: 0.3247 - acc: 0.9737 - val\_loss: 0.9959 - val\_acc: 0.6316
Epoch 7/10
114/114 [==============================] - 0s 186us/step - loss: 0.2376 - acc: 0.9825 - val\_loss: 0.9617 - val\_acc: 0.6481
Epoch 8/10
114/114 [==============================] - 0s 165us/step - loss: 0.1746 - acc: 1.0000 - val\_loss: 0.9325 - val\_acc: 0.6589
Epoch 9/10
114/114 [==============================] - 0s 194us/step - loss: 0.1266 - acc: 1.0000 - val\_loss: 0.9079 - val\_acc: 0.6715
Epoch 10/10
114/114 [==============================] - 0s 190us/step - loss: 0.0930 - acc: 1.0000 - val\_loss: 0.8898 - val\_acc: 0.6803
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 2.5671 - acc: 0.1140 - val\_loss: 1.5736 - val\_acc: 0.3285
Epoch 2/10
114/114 [==============================] - 0s 250us/step - loss: 1.1793 - acc: 0.5702 - val\_loss: 1.2532 - val\_acc: 0.5322
Epoch 3/10
114/114 [==============================] - 0s 206us/step - loss: 0.6385 - acc: 0.8333 - val\_loss: 1.1498 - val\_acc: 0.5770
Epoch 4/10
114/114 [==============================] - 0s 205us/step - loss: 0.4226 - acc: 0.9123 - val\_loss: 1.1177 - val\_acc: 0.5945
Epoch 5/10
114/114 [==============================] - 0s 237us/step - loss: 0.3066 - acc: 0.9298 - val\_loss: 1.0831 - val\_acc: 0.6150
Epoch 6/10
114/114 [==============================] - 0s 264us/step - loss: 0.2083 - acc: 0.9561 - val\_loss: 1.0436 - val\_acc: 0.6326
Epoch 7/10
114/114 [==============================] - 0s 258us/step - loss: 0.1336 - acc: 0.9912 - val\_loss: 1.0027 - val\_acc: 0.6520
Epoch 8/10
114/114 [==============================] - 0s 217us/step - loss: 0.0833 - acc: 1.0000 - val\_loss: 0.9665 - val\_acc: 0.6589
Epoch 9/10
114/114 [==============================] - 0s 268us/step - loss: 0.0501 - acc: 1.0000 - val\_loss: 0.9429 - val\_acc: 0.6686
Epoch 10/10
114/114 [==============================] - 0s 207us/step - loss: 0.0330 - acc: 1.0000 - val\_loss: 0.9287 - val\_acc: 0.6696
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.6092 - acc: 0.4298 - val\_loss: 1.1824 - val\_acc: 0.5497
Epoch 2/10
114/114 [==============================] - 0s 224us/step - loss: 0.6742 - acc: 0.8246 - val\_loss: 1.0293 - val\_acc: 0.6316
Epoch 3/10
114/114 [==============================] - 0s 181us/step - loss: 0.3472 - acc: 0.9737 - val\_loss: 0.9505 - val\_acc: 0.6745
Epoch 4/10
114/114 [==============================] - 0s 172us/step - loss: 0.1884 - acc: 1.0000 - val\_loss: 0.9156 - val\_acc: 0.6881
Epoch 5/10
114/114 [==============================] - 0s 197us/step - loss: 0.1000 - acc: 1.0000 - val\_loss: 0.9052 - val\_acc: 0.6930
Epoch 6/10
114/114 [==============================] - 0s 199us/step - loss: 0.0549 - acc: 1.0000 - val\_loss: 0.9086 - val\_acc: 0.7008
Epoch 7/10
114/114 [==============================] - 0s 208us/step - loss: 0.0310 - acc: 1.0000 - val\_loss: 0.9195 - val\_acc: 0.6979
Epoch 8/10
114/114 [==============================] - 0s 179us/step - loss: 0.0181 - acc: 1.0000 - val\_loss: 0.9351 - val\_acc: 0.7076
Epoch 9/10
114/114 [==============================] - 0s 185us/step - loss: 0.0111 - acc: 1.0000 - val\_loss: 0.9532 - val\_acc: 0.7086
Epoch 10/10
114/114 [==============================] - 0s 183us/step - loss: 0.0071 - acc: 1.0000 - val\_loss: 0.9717 - val\_acc: 0.7144
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 3ms/step - loss: 2.0338 - acc: 0.1491 - val\_loss: 1.2148 - val\_acc: 0.5497
Epoch 2/10
114/114 [==============================] - 0s 187us/step - loss: 0.7785 - acc: 0.7368 - val\_loss: 1.1358 - val\_acc: 0.5595
Epoch 3/10
114/114 [==============================] - 0s 185us/step - loss: 0.4785 - acc: 0.7895 - val\_loss: 1.0569 - val\_acc: 0.6043
Epoch 4/10
114/114 [==============================] - 0s 210us/step - loss: 0.2627 - acc: 0.9474 - val\_loss: 0.9653 - val\_acc: 0.6511
Epoch 5/10
114/114 [==============================] - 0s 177us/step - loss: 0.1296 - acc: 1.0000 - val\_loss: 0.8986 - val\_acc: 0.6823
Epoch 6/10
114/114 [==============================] - 0s 201us/step - loss: 0.0643 - acc: 1.0000 - val\_loss: 0.8630 - val\_acc: 0.6969
Epoch 7/10
114/114 [==============================] - 0s 185us/step - loss: 0.0348 - acc: 1.0000 - val\_loss: 0.8541 - val\_acc: 0.6949
Epoch 8/10
114/114 [==============================] - 0s 225us/step - loss: 0.0207 - acc: 1.0000 - val\_loss: 0.8613 - val\_acc: 0.7066
Epoch 9/10
114/114 [==============================] - 0s 211us/step - loss: 0.0134 - acc: 1.0000 - val\_loss: 0.8777 - val\_acc: 0.7144
Epoch 10/10
114/114 [==============================] - 0s 211us/step - loss: 0.0093 - acc: 1.0000 - val\_loss: 0.8984 - val\_acc: 0.7096
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.7110 - acc: 0.2281 - val\_loss: 1.1830 - val\_acc: 0.5419
Epoch 2/10
114/114 [==============================] - 0s 176us/step - loss: 0.6111 - acc: 0.7456 - val\_loss: 0.9948 - val\_acc: 0.6160
Epoch 3/10
114/114 [==============================] - 0s 167us/step - loss: 0.2616 - acc: 0.9825 - val\_loss: 0.8634 - val\_acc: 0.6979
Epoch 4/10
114/114 [==============================] - 0s 176us/step - loss: 0.1098 - acc: 1.0000 - val\_loss: 0.8188 - val\_acc: 0.7193
Epoch 5/10
114/114 [==============================] - 0s 200us/step - loss: 0.0555 - acc: 1.0000 - val\_loss: 0.8156 - val\_acc: 0.7251
Epoch 6/10
114/114 [==============================] - 0s 188us/step - loss: 0.0305 - acc: 1.0000 - val\_loss: 0.8282 - val\_acc: 0.7242
Epoch 7/10
114/114 [==============================] - 0s 210us/step - loss: 0.0180 - acc: 1.0000 - val\_loss: 0.8457 - val\_acc: 0.7261
Epoch 8/10
114/114 [==============================] - 0s 240us/step - loss: 0.0109 - acc: 1.0000 - val\_loss: 0.8645 - val\_acc: 0.7290
Epoch 9/10
114/114 [==============================] - 0s 227us/step - loss: 0.0067 - acc: 1.0000 - val\_loss: 0.8836 - val\_acc: 0.7310
Epoch 10/10
114/114 [==============================] - 0s 229us/step - loss: 0.0044 - acc: 1.0000 - val\_loss: 0.9023 - val\_acc: 0.7300
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 3ms/step - loss: 2.5833 - acc: 0.1316 - val\_loss: 1.9606 - val\_acc: 0.1784
Epoch 2/10
114/114 [==============================] - 0s 239us/step - loss: 1.6094 - acc: 0.3509 - val\_loss: 1.5727 - val\_acc: 0.3148
Epoch 3/10
114/114 [==============================] - 0s 250us/step - loss: 1.0194 - acc: 0.6842 - val\_loss: 1.3291 - val\_acc: 0.4825
Epoch 4/10
114/114 [==============================] - 0s 246us/step - loss: 0.6793 - acc: 0.8684 - val\_loss: 1.1994 - val\_acc: 0.5448
Epoch 5/10
114/114 [==============================] - 0s 332us/step - loss: 0.4856 - acc: 0.9211 - val\_loss: 1.1469 - val\_acc: 0.5721
Epoch 6/10
114/114 [==============================] - 0s 277us/step - loss: 0.3735 - acc: 0.9298 - val\_loss: 1.1264 - val\_acc: 0.5897
Epoch 7/10
114/114 [==============================] - 0s 216us/step - loss: 0.2945 - acc: 0.9298 - val\_loss: 1.1092 - val\_acc: 0.6014
Epoch 8/10
114/114 [==============================] - 0s 254us/step - loss: 0.2224 - acc: 0.9561 - val\_loss: 1.0811 - val\_acc: 0.6092
Epoch 9/10
114/114 [==============================] - 0s 247us/step - loss: 0.1610 - acc: 0.9825 - val\_loss: 1.0483 - val\_acc: 0.6257
Epoch 10/10
114/114 [==============================] - 0s 223us/step - loss: 0.1120 - acc: 0.9912 - val\_loss: 1.0170 - val\_acc: 0.6394
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 2.1399 - acc: 0.2105 - val\_loss: 1.3974 - val\_acc: 0.4522
Epoch 2/10
114/114 [==============================] - 0s 187us/step - loss: 0.9121 - acc: 0.7018 - val\_loss: 1.1930 - val\_acc: 0.5546
Epoch 3/10
114/114 [==============================] - 0s 255us/step - loss: 0.5221 - acc: 0.8246 - val\_loss: 1.0947 - val\_acc: 0.5936
Epoch 4/10
114/114 [==============================] - 0s 279us/step - loss: 0.3185 - acc: 0.9386 - val\_loss: 1.0228 - val\_acc: 0.6306
Epoch 5/10
114/114 [==============================] - 0s 243us/step - loss: 0.1886 - acc: 0.9912 - val\_loss: 0.9677 - val\_acc: 0.6647
Epoch 6/10
114/114 [==============================] - 0s 229us/step - loss: 0.1138 - acc: 1.0000 - val\_loss: 0.9347 - val\_acc: 0.6842
Epoch 7/10
114/114 [==============================] - 0s 241us/step - loss: 0.0701 - acc: 1.0000 - val\_loss: 0.9179 - val\_acc: 0.6949
Epoch 8/10
114/114 [==============================] - 0s 248us/step - loss: 0.0452 - acc: 1.0000 - val\_loss: 0.9104 - val\_acc: 0.7027
Epoch 9/10
114/114 [==============================] - 0s 165us/step - loss: 0.0299 - acc: 1.0000 - val\_loss: 0.9091 - val\_acc: 0.6979
Epoch 10/10
114/114 [==============================] - 0s 271us/step - loss: 0.0214 - acc: 1.0000 - val\_loss: 0.9113 - val\_acc: 0.6979
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.8167 - acc: 0.1842 - val\_loss: 1.2762 - val\_acc: 0.5185
Epoch 2/10
114/114 [==============================] - 0s 170us/step - loss: 0.7433 - acc: 0.7982 - val\_loss: 1.1586 - val\_acc: 0.5692
Epoch 3/10
114/114 [==============================] - 0s 181us/step - loss: 0.3647 - acc: 0.9474 - val\_loss: 1.0575 - val\_acc: 0.6199
Epoch 4/10
114/114 [==============================] - 0s 178us/step - loss: 0.1727 - acc: 0.9912 - val\_loss: 0.9952 - val\_acc: 0.6394
Epoch 5/10
114/114 [==============================] - 0s 173us/step - loss: 0.0875 - acc: 1.0000 - val\_loss: 0.9626 - val\_acc: 0.6550
Epoch 6/10
114/114 [==============================] - 0s 182us/step - loss: 0.0484 - acc: 1.0000 - val\_loss: 0.9541 - val\_acc: 0.6647
Epoch 7/10
114/114 [==============================] - 0s 177us/step - loss: 0.0295 - acc: 1.0000 - val\_loss: 0.9583 - val\_acc: 0.6715
Epoch 8/10
114/114 [==============================] - 0s 178us/step - loss: 0.0185 - acc: 1.0000 - val\_loss: 0.9685 - val\_acc: 0.6784
Epoch 9/10
114/114 [==============================] - 0s 192us/step - loss: 0.0123 - acc: 1.0000 - val\_loss: 0.9810 - val\_acc: 0.6862
Epoch 10/10
114/114 [==============================] - 0s 197us/step - loss: 0.0082 - acc: 1.0000 - val\_loss: 0.9944 - val\_acc: 0.6871
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 3ms/step - loss: 1.6185 - acc: 0.2895 - val\_loss: 1.1999 - val\_acc: 0.5614
Epoch 2/10
114/114 [==============================] - 0s 246us/step - loss: 0.5094 - acc: 0.9211 - val\_loss: 1.0243 - val\_acc: 0.6267
Epoch 3/10
114/114 [==============================] - 0s 273us/step - loss: 0.2204 - acc: 0.9649 - val\_loss: 0.9304 - val\_acc: 0.6589
Epoch 4/10
114/114 [==============================] - 0s 289us/step - loss: 0.1051 - acc: 1.0000 - val\_loss: 0.8880 - val\_acc: 0.6891
Epoch 5/10
114/114 [==============================] - 0s 355us/step - loss: 0.0529 - acc: 1.0000 - val\_loss: 0.8779 - val\_acc: 0.6969
Epoch 6/10
114/114 [==============================] - 0s 317us/step - loss: 0.0289 - acc: 1.0000 - val\_loss: 0.8821 - val\_acc: 0.6949
Epoch 7/10
114/114 [==============================] - 0s 253us/step - loss: 0.0174 - acc: 1.0000 - val\_loss: 0.8944 - val\_acc: 0.7008
Epoch 8/10
114/114 [==============================] - 0s 208us/step - loss: 0.0105 - acc: 1.0000 - val\_loss: 0.9111 - val\_acc: 0.6969
Epoch 9/10
114/114 [==============================] - 0s 263us/step - loss: 0.0070 - acc: 1.0000 - val\_loss: 0.9294 - val\_acc: 0.6979
Epoch 10/10
114/114 [==============================] - 0s 230us/step - loss: 0.0046 - acc: 1.0000 - val\_loss: 0.9475 - val\_acc: 0.6998
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.8944 - acc: 0.1404 - val\_loss: 1.1934 - val\_acc: 0.5643
Epoch 2/10
114/114 [==============================] - 0s 197us/step - loss: 0.5178 - acc: 0.8947 - val\_loss: 1.0422 - val\_acc: 0.6150
Epoch 3/10
114/114 [==============================] - 0s 257us/step - loss: 0.2081 - acc: 0.9912 - val\_loss: 0.9549 - val\_acc: 0.6579
Epoch 4/10
114/114 [==============================] - 0s 273us/step - loss: 0.0828 - acc: 1.0000 - val\_loss: 0.9164 - val\_acc: 0.6784
Epoch 5/10
114/114 [==============================] - 0s 252us/step - loss: 0.0370 - acc: 1.0000 - val\_loss: 0.9139 - val\_acc: 0.6881
Epoch 6/10
114/114 [==============================] - 0s 218us/step - loss: 0.0199 - acc: 1.0000 - val\_loss: 0.9293 - val\_acc: 0.6940
Epoch 7/10
114/114 [==============================] - 0s 190us/step - loss: 0.0118 - acc: 1.0000 - val\_loss: 0.9522 - val\_acc: 0.6998
Epoch 8/10
114/114 [==============================] - 0s 300us/step - loss: 0.0077 - acc: 1.0000 - val\_loss: 0.9771 - val\_acc: 0.7018
Epoch 9/10
114/114 [==============================] - 0s 244us/step - loss: 0.0053 - acc: 1.0000 - val\_loss: 1.0014 - val\_acc: 0.7027
Epoch 10/10
114/114 [==============================] - 0s 232us/step - loss: 0.0038 - acc: 1.0000 - val\_loss: 1.0239 - val\_acc: 0.7057
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 3ms/step - loss: 2.0177 - acc: 0.1930 - val\_loss: 1.5553 - val\_acc: 0.3470
Epoch 2/10
114/114 [==============================] - 0s 170us/step - loss: 1.1667 - acc: 0.5088 - val\_loss: 1.3504 - val\_acc: 0.4786
Epoch 3/10
114/114 [==============================] - 0s 203us/step - loss: 0.7713 - acc: 0.7807 - val\_loss: 1.2528 - val\_acc: 0.5214
Epoch 4/10
114/114 [==============================] - 0s 225us/step - loss: 0.5577 - acc: 0.8772 - val\_loss: 1.1951 - val\_acc: 0.5497
Epoch 5/10
114/114 [==============================] - 0s 224us/step - loss: 0.4066 - acc: 0.9123 - val\_loss: 1.1443 - val\_acc: 0.5770
Epoch 6/10
114/114 [==============================] - 0s 226us/step - loss: 0.2954 - acc: 0.9474 - val\_loss: 1.1073 - val\_acc: 0.6043
Epoch 7/10
114/114 [==============================] - 0s 201us/step - loss: 0.2181 - acc: 0.9649 - val\_loss: 1.0798 - val\_acc: 0.6199
Epoch 8/10
114/114 [==============================] - 0s 205us/step - loss: 0.1617 - acc: 0.9737 - val\_loss: 1.0582 - val\_acc: 0.6248
Epoch 9/10
114/114 [==============================] - 0s 202us/step - loss: 0.1192 - acc: 0.9912 - val\_loss: 1.0415 - val\_acc: 0.6296
Epoch 10/10
114/114 [==============================] - 0s 217us/step - loss: 0.0872 - acc: 1.0000 - val\_loss: 1.0294 - val\_acc: 0.6306
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 3ms/step - loss: 2.0224 - acc: 0.1316 - val\_loss: 1.3642 - val\_acc: 0.4542
Epoch 2/10
114/114 [==============================] - 0s 147us/step - loss: 0.8256 - acc: 0.7456 - val\_loss: 1.2136 - val\_acc: 0.5439
Epoch 3/10
114/114 [==============================] - 0s 182us/step - loss: 0.4746 - acc: 0.8684 - val\_loss: 1.1552 - val\_acc: 0.5604
Epoch 4/10
114/114 [==============================] - 0s 189us/step - loss: 0.2973 - acc: 0.9211 - val\_loss: 1.0992 - val\_acc: 0.5984
Epoch 5/10
114/114 [==============================] - 0s 210us/step - loss: 0.1755 - acc: 0.9737 - val\_loss: 1.0541 - val\_acc: 0.6111
Epoch 6/10
114/114 [==============================] - 0s 215us/step - loss: 0.1007 - acc: 1.0000 - val\_loss: 1.0268 - val\_acc: 0.6316
Epoch 7/10
114/114 [==============================] - 0s 184us/step - loss: 0.0576 - acc: 1.0000 - val\_loss: 1.0113 - val\_acc: 0.6404
Epoch 8/10
114/114 [==============================] - 0s 220us/step - loss: 0.0365 - acc: 1.0000 - val\_loss: 1.0032 - val\_acc: 0.6511
Epoch 9/10
114/114 [==============================] - 0s 278us/step - loss: 0.0236 - acc: 1.0000 - val\_loss: 1.0014 - val\_acc: 0.6540
Epoch 10/10
114/114 [==============================] - 0s 231us/step - loss: 0.0165 - acc: 1.0000 - val\_loss: 1.0039 - val\_acc: 0.6569
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.7516 - acc: 0.2105 - val\_loss: 1.2205 - val\_acc: 0.5595
Epoch 2/10
114/114 [==============================] - 0s 235us/step - loss: 0.6624 - acc: 0.7632 - val\_loss: 1.1005 - val\_acc: 0.6023
Epoch 3/10
114/114 [==============================] - 0s 268us/step - loss: 0.3393 - acc: 0.9211 - val\_loss: 1.0219 - val\_acc: 0.6345
Epoch 4/10
114/114 [==============================] - 0s 236us/step - loss: 0.1607 - acc: 1.0000 - val\_loss: 0.9702 - val\_acc: 0.6589
Epoch 5/10
114/114 [==============================] - 0s 184us/step - loss: 0.0775 - acc: 1.0000 - val\_loss: 0.9439 - val\_acc: 0.6589
Epoch 6/10
114/114 [==============================] - 0s 249us/step - loss: 0.0434 - acc: 1.0000 - val\_loss: 0.9420 - val\_acc: 0.6618
Epoch 7/10
114/114 [==============================] - 0s 265us/step - loss: 0.0279 - acc: 1.0000 - val\_loss: 0.9506 - val\_acc: 0.6657
Epoch 8/10
114/114 [==============================] - 0s 271us/step - loss: 0.0193 - acc: 1.0000 - val\_loss: 0.9618 - val\_acc: 0.6676
Epoch 9/10
114/114 [==============================] - 0s 216us/step - loss: 0.0135 - acc: 1.0000 - val\_loss: 0.9724 - val\_acc: 0.6754
Epoch 10/10
114/114 [==============================] - 0s 330us/step - loss: 0.0095 - acc: 1.0000 - val\_loss: 0.9817 - val\_acc: 0.6754
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.6753 - acc: 0.2368 - val\_loss: 1.2299 - val\_acc: 0.5166
Epoch 2/10
114/114 [==============================] - 0s 204us/step - loss: 0.5428 - acc: 0.7807 - val\_loss: 1.1372 - val\_acc: 0.5575
Epoch 3/10
114/114 [==============================] - 0s 213us/step - loss: 0.2207 - acc: 0.9912 - val\_loss: 1.0167 - val\_acc: 0.6462
Epoch 4/10
114/114 [==============================] - 0s 174us/step - loss: 0.0867 - acc: 1.0000 - val\_loss: 0.9589 - val\_acc: 0.6598
Epoch 5/10
114/114 [==============================] - 0s 214us/step - loss: 0.0419 - acc: 1.0000 - val\_loss: 0.9501 - val\_acc: 0.6598
Epoch 6/10
114/114 [==============================] - 0s 195us/step - loss: 0.0247 - acc: 1.0000 - val\_loss: 0.9628 - val\_acc: 0.6589
Epoch 7/10
114/114 [==============================] - 0s 197us/step - loss: 0.0160 - acc: 1.0000 - val\_loss: 0.9834 - val\_acc: 0.6647
Epoch 8/10
114/114 [==============================] - 0s 214us/step - loss: 0.0107 - acc: 1.0000 - val\_loss: 1.0057 - val\_acc: 0.6667
Epoch 9/10
114/114 [==============================] - 0s 213us/step - loss: 0.0073 - acc: 1.0000 - val\_loss: 1.0271 - val\_acc: 0.6686
Epoch 10/10
114/114 [==============================] - 0s 736us/step - loss: 0.0050 - acc: 1.0000 - val\_loss: 1.0470 - val\_acc: 0.6715
Train on 114 samples, validate on 1026 samples
Epoch 1/10
114/114 [==============================] - 0s 2ms/step - loss: 1.8982 - acc: 0.1491 - val\_loss: 1.2603 - val\_acc: 0.5039
Epoch 2/10
114/114 [==============================] - 0s 216us/step - loss: 0.6363 - acc: 0.7895 - val\_loss: 1.2042 - val\_acc: 0.5263
Epoch 3/10
114/114 [==============================] - 0s 162us/step - loss: 0.3003 - acc: 0.9123 - val\_loss: 1.0721 - val\_acc: 0.5936
Epoch 4/10
114/114 [==============================] - 0s 205us/step - loss: 0.1106 - acc: 0.9912 - val\_loss: 0.9913 - val\_acc: 0.6608
Epoch 5/10
114/114 [==============================] - 0s 161us/step - loss: 0.0424 - acc: 1.0000 - val\_loss: 0.9927 - val\_acc: 0.6667
Epoch 6/10
114/114 [==============================] - 0s 199us/step - loss: 0.0234 - acc: 1.0000 - val\_loss: 1.0277 - val\_acc: 0.6608
Epoch 7/10
114/114 [==============================] - 0s 195us/step - loss: 0.0150 - acc: 1.0000 - val\_loss: 1.0692 - val\_acc: 0.6676
Epoch 8/10
114/114 [==============================] - 0s 203us/step - loss: 0.0104 - acc: 1.0000 - val\_loss: 1.1070 - val\_acc: 0.6657
Epoch 9/10
114/114 [==============================] - 0s 239us/step - loss: 0.0073 - acc: 1.0000 - val\_loss: 1.1411 - val\_acc: 0.6725
Epoch 10/10
114/114 [==============================] - 0s 239us/step - loss: 0.0052 - acc: 1.0000 - val\_loss: 1.1711 - val\_acc: 0.6696
The best npc is 60, and the best nnode is 200.
The best validation accuracy is 0.734893.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{c+c1}{\PYZsh{}CNN}
         \PY{n}{Xtr\PYZus{}cnn} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{float32}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
         \PY{n}{Xts\PYZus{}cnn} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{float32}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
         \PY{n}{Xtr\PYZus{}cnn} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{Xtr\PYZus{}cnn}\PY{p}{,} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Xtr\PYZus{}cnn}\PY{p}{)}\PY{p}{,}\PY{n}{h}\PY{p}{,}\PY{n}{w}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} 
         \PY{n}{Xts\PYZus{}cnn} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{Xts\PYZus{}cnn}\PY{p}{,} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Xtr\PYZus{}cnn}\PY{p}{)}\PY{p}{,}\PY{n}{h}\PY{p}{,}\PY{n}{w}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n}{K}\PY{o}{.}\PY{n}{clear\PYZus{}session}\PY{p}{(}\PY{p}{)}
         \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
                          \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{Xtr\PYZus{}cnn}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{,}
                          \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{nout}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{opt} \PY{o}{=} \PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{beta\PYZus{}1}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{,} \PY{n}{beta\PYZus{}2}\PY{o}{=}\PY{l+m+mf}{0.999}\PY{p}{,} \PY{n}{epsilon}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}08}\PY{p}{,} \PY{n}{decay}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Let\PYZsq{}s train the model using Adam}
         \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sparse\PYZus{}categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{optimizer}\PY{o}{=}\PY{n}{opt}\PY{p}{,}
                       \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{hist\PYZus{}basic} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xtr\PYZus{}cnn}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}\PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{40}\PY{p}{,}
                                \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{Xts\PYZus{}cnn}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,}\PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} 
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The accuracy on validation set is:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{hist\PYZus{}basic}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 114 samples, validate on 1026 samples
Epoch 1/40
114/114 [==============================] - 1s 10ms/step - loss: 1.5895 - acc: 0.1579 - val\_loss: 1.4190 - val\_acc: 0.4649
Epoch 2/40
114/114 [==============================] - 1s 6ms/step - loss: 1.4490 - acc: 0.4649 - val\_loss: 1.4260 - val\_acc: 0.4649
Epoch 3/40
114/114 [==============================] - 1s 6ms/step - loss: 1.4300 - acc: 0.4649 - val\_loss: 1.4114 - val\_acc: 0.4649
Epoch 4/40
114/114 [==============================] - 1s 6ms/step - loss: 1.4118 - acc: 0.4649 - val\_loss: 1.4127 - val\_acc: 0.4649
Epoch 5/40
114/114 [==============================] - 1s 7ms/step - loss: 1.4072 - acc: 0.4649 - val\_loss: 1.4078 - val\_acc: 0.4649
Epoch 6/40
114/114 [==============================] - 1s 6ms/step - loss: 1.3997 - acc: 0.4649 - val\_loss: 1.4003 - val\_acc: 0.4649
Epoch 7/40
114/114 [==============================] - 1s 6ms/step - loss: 1.3937 - acc: 0.4649 - val\_loss: 1.3972 - val\_acc: 0.4649
Epoch 8/40
114/114 [==============================] - 1s 6ms/step - loss: 1.3846 - acc: 0.4649 - val\_loss: 1.3971 - val\_acc: 0.4649
Epoch 9/40
114/114 [==============================] - 1s 6ms/step - loss: 1.3803 - acc: 0.4649 - val\_loss: 1.3972 - val\_acc: 0.4649
Epoch 10/40
114/114 [==============================] - 1s 5ms/step - loss: 1.3776 - acc: 0.4649 - val\_loss: 1.4105 - val\_acc: 0.4649
Epoch 11/40
114/114 [==============================] - 1s 5ms/step - loss: 1.3877 - acc: 0.4649 - val\_loss: 1.4149 - val\_acc: 0.4649
Epoch 12/40
114/114 [==============================] - 1s 5ms/step - loss: 1.3900 - acc: 0.4649 - val\_loss: 1.3890 - val\_acc: 0.4649
Epoch 13/40
114/114 [==============================] - 1s 5ms/step - loss: 1.3607 - acc: 0.4649 - val\_loss: 1.3822 - val\_acc: 0.4649
Epoch 14/40
114/114 [==============================] - 1s 5ms/step - loss: 1.3624 - acc: 0.4649 - val\_loss: 1.3970 - val\_acc: 0.4649
Epoch 15/40
114/114 [==============================] - 1s 6ms/step - loss: 1.3707 - acc: 0.4649 - val\_loss: 1.3962 - val\_acc: 0.4649
Epoch 16/40
114/114 [==============================] - 1s 5ms/step - loss: 1.3646 - acc: 0.4737 - val\_loss: 1.3887 - val\_acc: 0.4649
Epoch 17/40
114/114 [==============================] - 1s 6ms/step - loss: 1.3531 - acc: 0.4649 - val\_loss: 1.3804 - val\_acc: 0.4649
Epoch 18/40
114/114 [==============================] - 1s 5ms/step - loss: 1.3442 - acc: 0.4649 - val\_loss: 1.3738 - val\_acc: 0.4649
Epoch 19/40
114/114 [==============================] - 1s 5ms/step - loss: 1.3362 - acc: 0.4649 - val\_loss: 1.3636 - val\_acc: 0.4649
Epoch 20/40
114/114 [==============================] - 1s 5ms/step - loss: 1.3246 - acc: 0.4649 - val\_loss: 1.3577 - val\_acc: 0.4649
Epoch 21/40
114/114 [==============================] - 1s 7ms/step - loss: 1.3189 - acc: 0.4649 - val\_loss: 1.3567 - val\_acc: 0.4649
Epoch 22/40
114/114 [==============================] - 1s 8ms/step - loss: 1.3157 - acc: 0.4649 - val\_loss: 1.3523 - val\_acc: 0.4649
Epoch 23/40
114/114 [==============================] - 1s 8ms/step - loss: 1.3067 - acc: 0.4649 - val\_loss: 1.3450 - val\_acc: 0.4649
Epoch 24/40
114/114 [==============================] - 1s 7ms/step - loss: 1.2926 - acc: 0.4649 - val\_loss: 1.3357 - val\_acc: 0.4649
Epoch 25/40
114/114 [==============================] - 1s 7ms/step - loss: 1.2768 - acc: 0.4649 - val\_loss: 1.3431 - val\_acc: 0.4698
Epoch 26/40
114/114 [==============================] - 1s 7ms/step - loss: 1.2780 - acc: 0.4912 - val\_loss: 1.3382 - val\_acc: 0.4912
Epoch 27/40
114/114 [==============================] - 1s 9ms/step - loss: 1.2625 - acc: 0.5439 - val\_loss: 1.3160 - val\_acc: 0.4669
Epoch 28/40
114/114 [==============================] - 1s 8ms/step - loss: 1.2444 - acc: 0.4825 - val\_loss: 1.3101 - val\_acc: 0.4649
Epoch 29/40
114/114 [==============================] - 1s 7ms/step - loss: 1.2335 - acc: 0.4649 - val\_loss: 1.3097 - val\_acc: 0.4649
Epoch 30/40
114/114 [==============================] - 1s 7ms/step - loss: 1.2311 - acc: 0.4649 - val\_loss: 1.3079 - val\_acc: 0.4649
Epoch 31/40
114/114 [==============================] - 1s 7ms/step - loss: 1.2195 - acc: 0.4649 - val\_loss: 1.3035 - val\_acc: 0.4649
Epoch 32/40
114/114 [==============================] - 1s 8ms/step - loss: 1.2071 - acc: 0.4649 - val\_loss: 1.2769 - val\_acc: 0.4688
Epoch 33/40
114/114 [==============================] - 1s 8ms/step - loss: 1.1675 - acc: 0.4737 - val\_loss: 1.2695 - val\_acc: 0.5088
Epoch 34/40
114/114 [==============================] - 1s 7ms/step - loss: 1.1577 - acc: 0.5789 - val\_loss: 1.2805 - val\_acc: 0.5302
Epoch 35/40
114/114 [==============================] - 1s 7ms/step - loss: 1.1555 - acc: 0.5877 - val\_loss: 1.2569 - val\_acc: 0.5331
Epoch 36/40
114/114 [==============================] - 1s 7ms/step - loss: 1.1208 - acc: 0.5877 - val\_loss: 1.2354 - val\_acc: 0.5029
Epoch 37/40
114/114 [==============================] - 1s 7ms/step - loss: 1.0941 - acc: 0.5439 - val\_loss: 1.2267 - val\_acc: 0.4854
Epoch 38/40
114/114 [==============================] - 1s 7ms/step - loss: 1.0740 - acc: 0.5439 - val\_loss: 1.2122 - val\_acc: 0.5029
Epoch 39/40
114/114 [==============================] - 1s 7ms/step - loss: 1.0460 - acc: 0.5789 - val\_loss: 1.2136 - val\_acc: 0.5536
Epoch 40/40
114/114 [==============================] - 1s 8ms/step - loss: 1.0384 - acc: 0.6404 - val\_loss: 1.2029 - val\_acc: 0.5497
The accuracy on validation set is:
[0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.469785576849653, 0.4912280727315832, 0.4668616001252775, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.4649122842455003, 0.46881091600505465, 0.508771924363707, 0.53021442227893401, 0.53313839191581769, 0.50292396801024619, 0.48538011550670945, 0.50292396801024619, 0.55360623316922852, 0.54970759722689211]

    \end{Verbatim}

    Q: How does CNN compare with PCA+NN with the small training set? Why?

    A:The validation accuracy CNN gets is much smaller than that of PCA+NN.
It probably because of its smaller training data set.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
